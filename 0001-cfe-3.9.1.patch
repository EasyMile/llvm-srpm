From 848b46abc63d6c363828b22882fd7ec0fd236426 Mon Sep 17 00:00:00 2001
From: Dan Aloni <alonid@gmail.com>
Date: Sun, 29 Jan 2017 18:30:35 +0200
Subject: [PATCH] cfe-3.9.1

---
 include/clang/AST/DeclTemplate.h                   |   10 +
 include/clang/Basic/DiagnosticDriverKinds.td       |    2 -
 include/clang/Basic/DiagnosticSemaKinds.td         |    8 +-
 include/clang/Sema/Sema.h                          |   31 +-
 lib/Basic/Targets.cpp                              |   61 +-
 lib/Basic/Version.cpp                              |    2 +-
 lib/CodeGen/CGExpr.cpp                             |   17 +-
 lib/CodeGen/CGStmt.cpp                             |   33 +-
 lib/CodeGen/CGStmtOpenMP.cpp                       |   53 +-
 lib/CodeGen/CodeGenFunction.cpp                    |   45 +-
 lib/CodeGen/CodeGenFunction.h                      |   92 ++
 lib/Driver/ToolChains.cpp                          |    4 +-
 lib/Driver/Tools.cpp                               |   32 +-
 lib/Headers/CMakeLists.txt                         |    1 +
 lib/Headers/msa.h                                  |  583 ++++++++
 lib/Sema/Sema.cpp                                  |   12 +-
 lib/Sema/SemaCXXScopeSpec.cpp                      |    2 +-
 lib/Sema/SemaChecking.cpp                          |  198 ++-
 lib/Sema/SemaDecl.cpp                              |    3 +-
 lib/Sema/SemaExpr.cpp                              |    8 +-
 lib/Sema/SemaExprCXX.cpp                           |   34 +-
 lib/Sema/SemaLambda.cpp                            |   16 +-
 lib/Sema/SemaOpenMP.cpp                            |   21 +-
 lib/Sema/SemaTemplate.cpp                          |   13 +-
 lib/Sema/SemaTemplateInstantiate.cpp               |   37 +-
 lib/Sema/SemaTemplateInstantiateDecl.cpp           |   35 +-
 lib/Serialization/ASTReaderDecl.cpp                |    2 +-
 test/CodeGen/builtins-mips-msa-error.c             |  421 ++++++
 test/CodeGen/builtins-mips-msa.c                   | 1409 ++++++++++----------
 test/CodeGenCXX/PR28523.cpp                        |   19 +
 test/CodeGenCXX/captured-statements.cpp            |    1 +
 test/CodeGenCXX/switch-case-folding-2.cpp          |   82 +-
 test/CodeGenOpenCL/kernel-arg-info-single-as.cl    |    9 +
 test/Driver/darwin-ld-lto.c                        |   23 +-
 test/Frontend/darwin-version.c                     |    4 +
 test/Modules/Inputs/static_assert/a.h              |    3 +
 test/Modules/Inputs/static_assert/module.modulemap |    1 +
 test/Modules/static_assert.cpp                     |    8 +
 test/OpenMP/atomic_write_codegen.c                 |    3 +
 test/OpenMP/cancel_codegen.cpp                     |    7 +-
 test/OpenMP/debug-info-openmp-array.cpp            |   13 +
 .../distribute_parallel_for_reduction_messages.cpp |    8 +
 ...ribute_parallel_for_simd_reduction_messages.cpp |    8 +
 test/OpenMP/distribute_simd_reduction_messages.cpp |    8 +
 test/OpenMP/for_lastprivate_codegen.cpp            |    9 +-
 test/OpenMP/for_reduction_codegen.cpp              |    4 +-
 test/OpenMP/for_reduction_codegen_UDR.cpp          |    4 +-
 test/OpenMP/for_reduction_messages.cpp             |    7 +
 test/OpenMP/for_simd_reduction_messages.cpp        |    7 +
 test/OpenMP/parallel_codegen.cpp                   |   37 +-
 test/OpenMP/parallel_for_reduction_messages.cpp    |    6 +
 .../parallel_for_simd_reduction_messages.cpp       |    6 +
 test/OpenMP/parallel_reduction_messages.cpp        |    5 +
 .../parallel_sections_reduction_messages.cpp       |    7 +
 test/OpenMP/sections_reduction_messages.cpp        |    8 +
 test/OpenMP/simd_reduction_messages.cpp            |    6 +
 test/OpenMP/target_firstprivate_codegen.cpp        |    2 +-
 test/OpenMP/target_map_codegen.cpp                 |    6 +-
 test/OpenMP/target_map_messages.cpp                |   13 +-
 test/OpenMP/target_parallel_for_map_messages.cpp   |    8 +-
 .../target_parallel_for_reduction_messages.cpp     |    6 +
 .../target_parallel_for_simd_map_messages.cpp      |    8 +-
 ...target_parallel_for_simd_reduction_messages.cpp |    6 +
 test/OpenMP/target_parallel_map_messages.cpp       |    8 +-
 test/OpenMP/target_parallel_reduction_messages.cpp |    5 +
 test/OpenMP/teams_reduction_messages.cpp           |    6 +
 test/Preprocessor/init.c                           |    5 +
 test/SemaCXX/cxx0x-defaulted-functions.cpp         |   35 +
 test/SemaCXX/cxx11-crashes.cpp                     |   12 +
 test/SemaCXX/nested-name-spec.cpp                  |   18 +
 test/SemaTemplate/instantiate-self.cpp             |  103 +-
 .../instantiation-depth-exception-spec.cpp         |   15 +-
 test/SemaTemplate/instantiation-depth.cpp          |    9 +-
 tools/libclang/CIndex.cpp                          |    5 +-
 74 files changed, 2840 insertions(+), 918 deletions(-)
 create mode 100644 lib/Headers/msa.h
 create mode 100644 test/CodeGen/builtins-mips-msa-error.c
 create mode 100644 test/CodeGenCXX/PR28523.cpp
 create mode 100644 test/CodeGenOpenCL/kernel-arg-info-single-as.cl
 create mode 100644 test/Modules/Inputs/static_assert/a.h
 create mode 100644 test/Modules/Inputs/static_assert/module.modulemap
 create mode 100644 test/Modules/static_assert.cpp
 create mode 100644 test/OpenMP/debug-info-openmp-array.cpp

diff --git a/include/clang/AST/DeclTemplate.h b/include/clang/AST/DeclTemplate.h
index 4ac8cdc9beeb..d553e739c388 100644
--- a/include/clang/AST/DeclTemplate.h
+++ b/include/clang/AST/DeclTemplate.h
@@ -44,6 +44,8 @@ class VarTemplatePartialSpecializationDecl;
 typedef llvm::PointerUnion3<TemplateTypeParmDecl*, NonTypeTemplateParmDecl*,
                             TemplateTemplateParmDecl*> TemplateParameter;
 
+NamedDecl *getAsNamedDecl(TemplateParameter P);
+
 /// \brief Stores a list of template parameters for a TemplateDecl and its
 /// derived classes.
 class TemplateParameterList final
@@ -2912,6 +2914,14 @@ public:
   friend class ASTDeclWriter;
 };
 
+inline NamedDecl *getAsNamedDecl(TemplateParameter P) {
+  if (auto *PD = P.dyn_cast<TemplateTypeParmDecl*>())
+    return PD;
+  if (auto *PD = P.dyn_cast<NonTypeTemplateParmDecl*>())
+    return PD;
+  return P.get<TemplateTemplateParmDecl*>();
+}
+
 } /* end of namespace clang */
 
 #endif
diff --git a/include/clang/Basic/DiagnosticDriverKinds.td b/include/clang/Basic/DiagnosticDriverKinds.td
index 6b8db6963daa..0a9b34827dc2 100644
--- a/include/clang/Basic/DiagnosticDriverKinds.td
+++ b/include/clang/Basic/DiagnosticDriverKinds.td
@@ -159,8 +159,6 @@ def err_drv_bitcode_unsupported_on_toolchain : Error<
   "-fembed-bitcode is not supported on versions of iOS prior to 6.0">;
 
 def warn_O4_is_O3 : Warning<"-O4 is equivalent to -O3">, InGroup<Deprecated>;
-def warn_drv_lto_libpath : Warning<"libLTO.dylib relative to clang installed dir not found; using 'ld' default search path instead">,
-  InGroup<LibLTO>;
 def warn_drv_optimization_value : Warning<"optimization level '%0' is not supported; using '%1%2' instead">,
   InGroup<InvalidCommandLineArgument>;
 def warn_ignored_gcc_optimization : Warning<"optimization flag '%0' is not supported">,
diff --git a/include/clang/Basic/DiagnosticSemaKinds.td b/include/clang/Basic/DiagnosticSemaKinds.td
index 1203fe765b52..98b687b8e821 100644
--- a/include/clang/Basic/DiagnosticSemaKinds.td
+++ b/include/clang/Basic/DiagnosticSemaKinds.td
@@ -4291,7 +4291,7 @@ def err_definition_of_implicitly_declared_member : Error<
 def err_definition_of_explicitly_defaulted_member : Error<
   "definition of explicitly defaulted %select{default constructor|copy "
   "constructor|move constructor|copy assignment operator|move assignment "
-  "operator|destructor}0">;
+  "operator|destructor|function}0">;
 def err_redefinition_extern_inline : Error<
   "redefinition of a 'extern inline' function %0 is not supported in "
   "%select{C99 mode|C++}1">;
@@ -6917,6 +6917,10 @@ def err_in_class_initializer_not_yet_parsed
 def err_in_class_initializer_not_yet_parsed_outer_class
     : Error<"cannot use defaulted default constructor of %0 within "
             "%1 outside of member functions because %2 has an initializer">;
+def err_in_class_initializer_cycle
+    : Error<"default member initializer for %0 uses itself">;
+def err_exception_spec_cycle
+    : Error<"exception specification of %0 uses itself">;
 
 def ext_in_class_initializer_non_constant : Extension<
   "in-class initializer for static data member is not a constant expression; "
@@ -7617,6 +7621,8 @@ def err_invalid_neon_type_code : Error<
   "incompatible constant for this __builtin_neon function">; 
 def err_argument_invalid_range : Error<
   "argument should be a value from %0 to %1">;
+def err_argument_not_multiple : Error<
+  "argument should be a multiple of %0">;
 def warn_neon_vector_initializer_non_portable : Warning<
   "vector initializers are not compatible with NEON intrinsics in big endian "
   "mode">, InGroup<DiagGroup<"nonportable-vector-initialization">>;
diff --git a/include/clang/Sema/Sema.h b/include/clang/Sema/Sema.h
index 10a8a5ac538a..437a44a30711 100644
--- a/include/clang/Sema/Sema.h
+++ b/include/clang/Sema/Sema.h
@@ -18,6 +18,7 @@
 #include "clang/AST/Attr.h"
 #include "clang/AST/Availability.h"
 #include "clang/AST/DeclarationName.h"
+#include "clang/AST/DeclTemplate.h"
 #include "clang/AST/Expr.h"
 #include "clang/AST/ExprObjC.h"
 #include "clang/AST/ExternalASTSource.h"
@@ -1217,8 +1218,10 @@ public:
   /// \brief Retrieve the current block, if any.
   sema::BlockScopeInfo *getCurBlock();
 
-  /// \brief Retrieve the current lambda scope info, if any.
-  sema::LambdaScopeInfo *getCurLambda();
+  /// Retrieve the current lambda scope info, if any.
+  /// \param IgnoreCapturedRegions true if should find the top-most lambda scope
+  /// info ignoring all inner captured regions scope infos.
+  sema::LambdaScopeInfo *getCurLambda(bool IgnoreCapturedRegions = false);
 
   /// \brief Retrieve the current generic lambda info, if any.
   sema::LambdaScopeInfo *getCurGenericLambda();
@@ -6613,10 +6616,10 @@ public:
       TemplateInstantiation,
 
       /// We are instantiating a default argument for a template
-      /// parameter. The Entity is the template, and
-      /// TemplateArgs/NumTemplateArguments provides the template
-      /// arguments as specified.
-      /// FIXME: Use a TemplateArgumentList
+      /// parameter. The Entity is the template parameter whose argument is
+      /// being instantiated, the Template is the template, and the
+      /// TemplateArgs/NumTemplateArguments provide the template arguments as
+      /// specified.
       DefaultTemplateArgumentInstantiation,
 
       /// We are instantiating a default argument for a function.
@@ -6731,6 +6734,9 @@ public:
   SmallVector<ActiveTemplateInstantiation, 16>
     ActiveTemplateInstantiations;
 
+  /// Specializations whose definitions are currently being instantiated.
+  llvm::DenseSet<std::pair<Decl *, unsigned>> InstantiatingSpecializations;
+
   /// \brief Extra modules inspected when performing a lookup during a template
   /// instantiation. Computed lazily.
   SmallVector<Module*, 16> ActiveTemplateInstantiationLookupModules;
@@ -6837,12 +6843,12 @@ public:
     /// \brief Note that we are instantiating a default argument in a
     /// template-id.
     InstantiatingTemplate(Sema &SemaRef, SourceLocation PointOfInstantiation,
-                          TemplateDecl *Template,
+                          TemplateParameter Param, TemplateDecl *Template,
                           ArrayRef<TemplateArgument> TemplateArgs,
                           SourceRange InstantiationRange = SourceRange());
 
-    /// \brief Note that we are instantiating a default argument in a
-    /// template-id.
+    /// \brief Note that we are substituting either explicitly-specified or
+    /// deduced template arguments during function template argument deduction.
     InstantiatingTemplate(Sema &SemaRef, SourceLocation PointOfInstantiation,
                           FunctionTemplateDecl *FunctionTemplate,
                           ArrayRef<TemplateArgument> TemplateArgs,
@@ -6909,9 +6915,14 @@ public:
     /// recursive template instantiations.
     bool isInvalid() const { return Invalid; }
 
+    /// \brief Determine whether we are already instantiating this
+    /// specialization in some surrounding active instantiation.
+    bool isAlreadyInstantiating() const { return AlreadyInstantiating; }
+
   private:
     Sema &SemaRef;
     bool Invalid;
+    bool AlreadyInstantiating;
     bool SavedInNonInstantiationSFINAEContext;
     bool CheckInstantiationDepth(SourceLocation PointOfInstantiation,
                                  SourceRange InstantiationRange);
@@ -9406,6 +9417,8 @@ private:
                               llvm::APSInt &Result);
   bool SemaBuiltinConstantArgRange(CallExpr *TheCall, int ArgNum,
                                    int Low, int High);
+  bool SemaBuiltinConstantArgMultiple(CallExpr *TheCall, int ArgNum,
+                                      unsigned Multiple);
   bool SemaBuiltinARMSpecialReg(unsigned BuiltinID, CallExpr *TheCall,
                                 int ArgNum, unsigned ExpectedFieldNum,
                                 bool AllowName);
diff --git a/lib/Basic/Targets.cpp b/lib/Basic/Targets.cpp
index 7ec06bb17e3f..be5d4ad8feda 100644
--- a/lib/Basic/Targets.cpp
+++ b/lib/Basic/Targets.cpp
@@ -158,14 +158,25 @@ static void getDarwinDefines(MacroBuilder &Builder, const LangOptions &Opts,
 
   // Set the appropriate OS version define.
   if (Triple.isiOS()) {
-    assert(Maj < 10 && Min < 100 && Rev < 100 && "Invalid version!");
-    char Str[6];
-    Str[0] = '0' + Maj;
-    Str[1] = '0' + (Min / 10);
-    Str[2] = '0' + (Min % 10);
-    Str[3] = '0' + (Rev / 10);
-    Str[4] = '0' + (Rev % 10);
-    Str[5] = '\0';
+    assert(Maj < 100 && Min < 100 && Rev < 100 && "Invalid version!");
+    char Str[7];
+    if (Maj < 10) {
+      Str[0] = '0' + Maj;
+      Str[1] = '0' + (Min / 10);
+      Str[2] = '0' + (Min % 10);
+      Str[3] = '0' + (Rev / 10);
+      Str[4] = '0' + (Rev % 10);
+      Str[5] = '\0';
+    } else {
+      // Handle versions >= 10.
+      Str[0] = '0' + (Maj / 10);
+      Str[1] = '0' + (Maj % 10);
+      Str[2] = '0' + (Min / 10);
+      Str[3] = '0' + (Min % 10);
+      Str[4] = '0' + (Rev / 10);
+      Str[5] = '0' + (Rev % 10);
+      Str[6] = '\0';
+    }
     if (Triple.isTvOS())
       Builder.defineMacro("__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__", Str);
     else
@@ -2070,21 +2081,23 @@ public:
 
   static GPUKind parseAMDGCNName(StringRef Name) {
     return llvm::StringSwitch<GPUKind>(Name)
-      .Case("tahiti",   GK_SOUTHERN_ISLANDS)
-      .Case("pitcairn", GK_SOUTHERN_ISLANDS)
-      .Case("verde",    GK_SOUTHERN_ISLANDS)
-      .Case("oland",    GK_SOUTHERN_ISLANDS)
-      .Case("hainan",   GK_SOUTHERN_ISLANDS)
-      .Case("bonaire",  GK_SEA_ISLANDS)
-      .Case("kabini",   GK_SEA_ISLANDS)
-      .Case("kaveri",   GK_SEA_ISLANDS)
-      .Case("hawaii",   GK_SEA_ISLANDS)
-      .Case("mullins",  GK_SEA_ISLANDS)
-      .Case("tonga",    GK_VOLCANIC_ISLANDS)
-      .Case("iceland",  GK_VOLCANIC_ISLANDS)
-      .Case("carrizo",  GK_VOLCANIC_ISLANDS)
-      .Case("fiji",     GK_VOLCANIC_ISLANDS)
-      .Case("stoney",   GK_VOLCANIC_ISLANDS)
+      .Case("tahiti",    GK_SOUTHERN_ISLANDS)
+      .Case("pitcairn",  GK_SOUTHERN_ISLANDS)
+      .Case("verde",     GK_SOUTHERN_ISLANDS)
+      .Case("oland",     GK_SOUTHERN_ISLANDS)
+      .Case("hainan",    GK_SOUTHERN_ISLANDS)
+      .Case("bonaire",   GK_SEA_ISLANDS)
+      .Case("kabini",    GK_SEA_ISLANDS)
+      .Case("kaveri",    GK_SEA_ISLANDS)
+      .Case("hawaii",    GK_SEA_ISLANDS)
+      .Case("mullins",   GK_SEA_ISLANDS)
+      .Case("tonga",     GK_VOLCANIC_ISLANDS)
+      .Case("iceland",   GK_VOLCANIC_ISLANDS)
+      .Case("carrizo",   GK_VOLCANIC_ISLANDS)
+      .Case("fiji",      GK_VOLCANIC_ISLANDS)
+      .Case("stoney",    GK_VOLCANIC_ISLANDS)
+      .Case("polaris10", GK_VOLCANIC_ISLANDS)
+      .Case("polaris11", GK_VOLCANIC_ISLANDS)
       .Default(GK_NONE);
   }
 
@@ -8170,6 +8183,8 @@ static TargetInfo *AllocateTarget(const llvm::Triple &Triple,
       return new DarwinARMTargetInfo(Triple, Opts);
 
     switch (os) {
+    case llvm::Triple::CloudABI:
+      return new CloudABITargetInfo<ARMleTargetInfo>(Triple, Opts);
     case llvm::Triple::Linux:
       return new LinuxTargetInfo<ARMleTargetInfo>(Triple, Opts);
     case llvm::Triple::FreeBSD:
diff --git a/lib/Basic/Version.cpp b/lib/Basic/Version.cpp
index c89e5d9b4bde..4fa52b4acce0 100644
--- a/lib/Basic/Version.cpp
+++ b/lib/Basic/Version.cpp
@@ -36,7 +36,7 @@ std::string getClangRepositoryPath() {
 
   // If the SVN_REPOSITORY is empty, try to use the SVN keyword. This helps us
   // pick up a tag in an SVN export, for example.
-  StringRef SVNRepository("$URL: https://llvm.org/svn/llvm-project/cfe/tags/RELEASE_390/final/lib/Basic/Version.cpp $");
+  StringRef SVNRepository("$URL: https://llvm.org/svn/llvm-project/cfe/tags/RELEASE_391/final/lib/Basic/Version.cpp $");
   if (URL.empty()) {
     URL = SVNRepository.slice(SVNRepository.find(':'),
                               SVNRepository.find("/lib/Basic"));
diff --git a/lib/CodeGen/CGExpr.cpp b/lib/CodeGen/CGExpr.cpp
index 3e1ae3604f94..5f3b290d8eb1 100644
--- a/lib/CodeGen/CGExpr.cpp
+++ b/lib/CodeGen/CGExpr.cpp
@@ -2105,12 +2105,11 @@ LValue CodeGenFunction::EmitDeclRefLValue(const DeclRefExpr *E) {
       if (auto *FD = LambdaCaptureFields.lookup(VD))
         return EmitCapturedFieldLValue(*this, FD, CXXABIThisValue);
       else if (CapturedStmtInfo) {
-        auto it = LocalDeclMap.find(VD);
-        if (it != LocalDeclMap.end()) {
-          if (auto RefTy = VD->getType()->getAs<ReferenceType>()) {
-            return EmitLoadOfReferenceLValue(it->second, RefTy);
-          }
-          return MakeAddrLValue(it->second, T);
+        auto I = LocalDeclMap.find(VD);
+        if (I != LocalDeclMap.end()) {
+          if (auto RefTy = VD->getType()->getAs<ReferenceType>())
+            return EmitLoadOfReferenceLValue(I->second, RefTy);
+          return MakeAddrLValue(I->second, T);
         }
         LValue CapLVal =
             EmitCapturedFieldLValue(*this, CapturedStmtInfo->lookup(VD),
@@ -2249,13 +2248,15 @@ LValue CodeGenFunction::EmitUnaryOpLValue(const UnaryOperator *E) {
       return LV;
     }
 
-    assert(E->getSubExpr()->getType()->isAnyComplexType());
+    QualType T = ExprTy->castAs<ComplexType>()->getElementType();
 
     Address Component =
       (E->getOpcode() == UO_Real
          ? emitAddrOfRealComponent(LV.getAddress(), LV.getType())
          : emitAddrOfImagComponent(LV.getAddress(), LV.getType()));
-    return MakeAddrLValue(Component, ExprTy, LV.getAlignmentSource());
+    LValue ElemLV = MakeAddrLValue(Component, T, LV.getAlignmentSource());
+    ElemLV.getQuals().addQualifiers(LV.getQuals());
+    return ElemLV;
   }
   case UO_PreInc:
   case UO_PreDec: {
diff --git a/lib/CodeGen/CGStmt.cpp b/lib/CodeGen/CGStmt.cpp
index 77879021f9af..d815863e929d 100644
--- a/lib/CodeGen/CGStmt.cpp
+++ b/lib/CodeGen/CGStmt.cpp
@@ -1323,6 +1323,10 @@ static CSFC_Result CollectStatementsForCase(const Stmt *S,
     // Handle this as two cases: we might be looking for the SwitchCase (if so
     // the skipped statements must be skippable) or we might already have it.
     CompoundStmt::const_body_iterator I = CS->body_begin(), E = CS->body_end();
+    bool StartedInLiveCode = FoundCase;
+    unsigned StartSize = ResultStmts.size();
+
+    // If we've not found the case yet, scan through looking for it.
     if (Case) {
       // Keep track of whether we see a skipped declaration.  The code could be
       // using the declaration even if it is skipped, so we can't optimize out
@@ -1332,7 +1336,7 @@ static CSFC_Result CollectStatementsForCase(const Stmt *S,
       // If we're looking for the case, just see if we can skip each of the
       // substatements.
       for (; Case && I != E; ++I) {
-        HadSkippedDecl |= isa<DeclStmt>(*I);
+        HadSkippedDecl |= CodeGenFunction::mightAddDeclToScope(*I);
 
         switch (CollectStatementsForCase(*I, Case, FoundCase, ResultStmts)) {
         case CSFC_Failure: return CSFC_Failure;
@@ -1368,11 +1372,19 @@ static CSFC_Result CollectStatementsForCase(const Stmt *S,
           break;
         }
       }
+
+      if (!FoundCase)
+        return CSFC_Success;
+
+      assert(!HadSkippedDecl && "fallthrough after skipping decl");
     }
 
     // If we have statements in our range, then we know that the statements are
     // live and need to be added to the set of statements we're tracking.
+    bool AnyDecls = false;
     for (; I != E; ++I) {
+      AnyDecls |= CodeGenFunction::mightAddDeclToScope(*I);
+
       switch (CollectStatementsForCase(*I, nullptr, FoundCase, ResultStmts)) {
       case CSFC_Failure: return CSFC_Failure;
       case CSFC_FallThrough:
@@ -1390,7 +1402,24 @@ static CSFC_Result CollectStatementsForCase(const Stmt *S,
       }
     }
 
-    return Case ? CSFC_Success : CSFC_FallThrough;
+    // If we're about to fall out of a scope without hitting a 'break;', we
+    // can't perform the optimization if there were any decls in that scope
+    // (we'd lose their end-of-lifetime).
+    if (AnyDecls) {
+      // If the entire compound statement was live, there's one more thing we
+      // can try before giving up: emit the whole thing as a single statement.
+      // We can do that unless the statement contains a 'break;'.
+      // FIXME: Such a break must be at the end of a construct within this one.
+      // We could emit this by just ignoring the BreakStmts entirely.
+      if (StartedInLiveCode && !CodeGenFunction::containsBreak(S)) {
+        ResultStmts.resize(StartSize);
+        ResultStmts.push_back(S);
+      } else {
+        return CSFC_Failure;
+      }
+    }
+
+    return CSFC_FallThrough;
   }
 
   // Okay, this is some other statement that we don't handle explicitly, like a
diff --git a/lib/CodeGen/CGStmtOpenMP.cpp b/lib/CodeGen/CGStmtOpenMP.cpp
index 26f780fdd7de..d214340bdafe 100644
--- a/lib/CodeGen/CGStmtOpenMP.cpp
+++ b/lib/CodeGen/CGStmtOpenMP.cpp
@@ -232,8 +232,15 @@ CodeGenFunction::GenerateOpenMPCapturedStmtFunction(const CapturedStmt &S) {
       assert(I->capturesVariableArrayType());
       II = &getContext().Idents.get("vla");
     }
-    if (ArgType->isVariablyModifiedType())
-      ArgType = getContext().getVariableArrayDecayedType(ArgType);
+    if (ArgType->isVariablyModifiedType()) {
+      bool IsReference = ArgType->isLValueReferenceType();
+      ArgType =
+          getContext().getCanonicalParamType(ArgType.getNonReferenceType());
+      if (IsReference && !ArgType->isPointerType()) {
+        ArgType = getContext().getLValueReferenceType(
+            ArgType, /*SpelledAsLValue=*/false);
+      }
+    }
     Args.push_back(ImplicitParamDecl::Create(getContext(), nullptr,
                                              FD->getLocation(), II, ArgType));
     ++I;
@@ -287,8 +294,14 @@ CodeGenFunction::GenerateOpenMPCapturedStmtFunction(const CapturedStmt &S) {
       QualType VarTy = Var->getType();
       Address ArgAddr = ArgLVal.getAddress();
       if (!VarTy->isReferenceType()) {
-        ArgAddr = EmitLoadOfReference(
-            ArgAddr, ArgLVal.getType()->castAs<ReferenceType>());
+        if (ArgLVal.getType()->isLValueReferenceType()) {
+          ArgAddr = EmitLoadOfReference(
+              ArgAddr, ArgLVal.getType()->castAs<ReferenceType>());
+        } else if (!VarTy->isVariablyModifiedType() || !VarTy->isPointerType()) {
+          assert(ArgLVal.getType()->isPointerType());
+          ArgAddr = EmitLoadOfPointer(
+              ArgAddr, ArgLVal.getType()->castAs<PointerType>());
+        }
       }
       setAddrOfLocalVar(
           Var, Address(ArgAddr.getPointer(), getContext().getDeclAlign(Var)));
@@ -1754,9 +1767,11 @@ void CodeGenFunction::EmitOMPOuterLoop(bool DynamicOrOrdered, bool IsMonotonic,
   EmitBlock(LoopExit.getBlock());
 
   // Tell the runtime we are done.
-  if (!DynamicOrOrdered)
-    RT.emitForStaticFinish(*this, S.getLocEnd());
-
+  auto &&CodeGen = [DynamicOrOrdered, &S](CodeGenFunction &CGF) {
+    if (!DynamicOrOrdered)
+      CGF.CGM.getOpenMPRuntime().emitForStaticFinish(CGF, S.getLocEnd());
+  };
+  OMPCancelStack.emitExit(*this, S.getDirectiveKind(), CodeGen);
 }
 
 void CodeGenFunction::EmitOMPForOuterLoop(
@@ -1872,6 +1887,8 @@ void CodeGenFunction::EmitOMPDistributeParallelForDirective(
       *this, OMPD_distribute_parallel_for,
       [&S](CodeGenFunction &CGF, PrePostActionTy &) {
         OMPLoopScope PreInitScope(CGF, S);
+        OMPCancelStackRAII CancelRegion(CGF, OMPD_distribute_parallel_for,
+                                        /*HasCancel=*/false);
         CGF.EmitStmt(
             cast<CapturedStmt>(S.getAssociatedStmt())->getCapturedStmt());
       });
@@ -2060,7 +2077,10 @@ bool CodeGenFunction::EmitOMPWorksharingLoop(const OMPLoopDirective &S) {
                          [](CodeGenFunction &) {});
         EmitBlock(LoopExit.getBlock());
         // Tell the runtime we are done.
-        RT.emitForStaticFinish(*this, S.getLocStart());
+        auto &&CodeGen = [&S](CodeGenFunction &CGF) {
+          CGF.CGM.getOpenMPRuntime().emitForStaticFinish(CGF, S.getLocEnd());
+        };
+        OMPCancelStack.emitExit(*this, S.getDirectiveKind(), CodeGen);
       } else {
         const bool IsMonotonic =
             Ordered || ScheduleKind.Schedule == OMPC_SCHEDULE_static ||
@@ -2110,6 +2130,7 @@ void CodeGenFunction::EmitOMPForDirective(const OMPForDirective &S) {
   bool HasLastprivates = false;
   auto &&CodeGen = [&S, &HasLastprivates](CodeGenFunction &CGF,
                                           PrePostActionTy &) {
+    OMPCancelStackRAII CancelRegion(CGF, OMPD_for, S.hasCancel());
     HasLastprivates = CGF.EmitOMPWorksharingLoop(S);
   };
   {
@@ -2250,7 +2271,10 @@ void CodeGenFunction::EmitSections(const OMPExecutableDirective &S) {
     CGF.EmitOMPInnerLoop(S, /*RequiresCleanup=*/false, &Cond, &Inc, BodyGen,
                          [](CodeGenFunction &) {});
     // Tell the runtime we are done.
-    CGF.CGM.getOpenMPRuntime().emitForStaticFinish(CGF, S.getLocStart());
+    auto &&CodeGen = [&S](CodeGenFunction &CGF) {
+      CGF.CGM.getOpenMPRuntime().emitForStaticFinish(CGF, S.getLocEnd());
+    };
+    CGF.OMPCancelStack.emitExit(CGF, S.getDirectiveKind(), CodeGen);
     CGF.EmitOMPReductionClauseFinal(S);
     // Emit post-update of the reduction variables if IsLastIter != 0.
     emitPostUpdateForReductionClause(
@@ -2272,6 +2296,7 @@ void CodeGenFunction::EmitSections(const OMPExecutableDirective &S) {
     HasCancel = OSD->hasCancel();
   else if (auto *OPSD = dyn_cast<OMPParallelSectionsDirective>(&S))
     HasCancel = OPSD->hasCancel();
+  OMPCancelStackRAII CancelRegion(*this, S.getDirectiveKind(), HasCancel);
   CGM.getOpenMPRuntime().emitInlinedDirective(*this, OMPD_sections, CodeGen,
                                               HasCancel);
   // Emit barrier for lastprivates only if 'sections' directive has 'nowait'
@@ -2375,6 +2400,7 @@ void CodeGenFunction::EmitOMPParallelForDirective(
   // Emit directive as a combined directive that consists of two implicit
   // directives: 'parallel' with 'for' directive.
   auto &&CodeGen = [&S](CodeGenFunction &CGF, PrePostActionTy &) {
+    OMPCancelStackRAII CancelRegion(CGF, OMPD_parallel_for, S.hasCancel());
     CGF.EmitOMPWorksharingLoop(S);
   };
   emitCommonOMPParallelDirective(*this, S, OMPD_for, CodeGen);
@@ -3374,11 +3400,14 @@ void CodeGenFunction::EmitOMPCancelDirective(const OMPCancelDirective &S) {
 
 CodeGenFunction::JumpDest
 CodeGenFunction::getOMPCancelDestination(OpenMPDirectiveKind Kind) {
-  if (Kind == OMPD_parallel || Kind == OMPD_task)
+  if (Kind == OMPD_parallel || Kind == OMPD_task ||
+      Kind == OMPD_target_parallel)
     return ReturnBlock;
   assert(Kind == OMPD_for || Kind == OMPD_section || Kind == OMPD_sections ||
-         Kind == OMPD_parallel_sections || Kind == OMPD_parallel_for);
-  return BreakContinueStack.back().BreakBlock;
+         Kind == OMPD_parallel_sections || Kind == OMPD_parallel_for ||
+         Kind == OMPD_distribute_parallel_for ||
+         Kind == OMPD_target_parallel_for);
+  return OMPCancelStack.getExitBlock();
 }
 
 // Generate the instructions for '#pragma omp target data' directive.
diff --git a/lib/CodeGen/CodeGenFunction.cpp b/lib/CodeGen/CodeGenFunction.cpp
index 183ee12ea232..11e4ad9ecefa 100644
--- a/lib/CodeGen/CodeGenFunction.cpp
+++ b/lib/CodeGen/CodeGenFunction.cpp
@@ -25,6 +25,7 @@
 #include "clang/AST/Decl.h"
 #include "clang/AST/DeclCXX.h"
 #include "clang/AST/StmtCXX.h"
+#include "clang/AST/StmtObjC.h"
 #include "clang/Basic/Builtins.h"
 #include "clang/Basic/TargetInfo.h"
 #include "clang/CodeGen/CGFunctionInfo.h"
@@ -436,6 +437,23 @@ void CodeGenFunction::EmitMCountInstrumentation() {
   EmitNounwindRuntimeCall(MCountFn);
 }
 
+// Returns the address space id that should be produced to the
+// kernel_arg_addr_space metadata. This is always fixed to the ids
+// as specified in the SPIR 2.0 specification in order to differentiate
+// for example in clGetKernelArgInfo() implementation between the address
+// spaces with targets without unique mapping to the OpenCL address spaces
+// (basically all single AS CPUs).
+static unsigned ArgInfoAddressSpace(unsigned LangAS) {
+  switch (LangAS) {
+  case LangAS::opencl_global:   return 1;
+  case LangAS::opencl_constant: return 2;
+  case LangAS::opencl_local:    return 3;
+  case LangAS::opencl_generic:  return 4; // Not in SPIR 2.0 specs.
+  default:
+    return 0; // Assume private.
+  }
+}
+
 // OpenCL v1.2 s5.6.4.6 allows the compiler to store kernel argument
 // information in the program executable. The argument information stored
 // includes the argument name, its type, the address and access qualifiers used.
@@ -476,7 +494,7 @@ static void GenOpenCLArgMetadata(const FunctionDecl *FD, llvm::Function *Fn,
 
       // Get address qualifier.
       addressQuals.push_back(llvm::ConstantAsMetadata::get(Builder.getInt32(
-          ASTCtx.getTargetAddressSpace(pointeeTy.getAddressSpace()))));
+        ArgInfoAddressSpace(pointeeTy.getAddressSpace()))));
 
       // Get argument type name.
       std::string typeName =
@@ -513,8 +531,7 @@ static void GenOpenCLArgMetadata(const FunctionDecl *FD, llvm::Function *Fn,
       uint32_t AddrSpc = 0;
       bool isPipe = ty->isPipeType();
       if (ty->isImageType() || isPipe)
-        AddrSpc =
-          CGM.getContext().getTargetAddressSpace(LangAS::opencl_global);
+        AddrSpc = ArgInfoAddressSpace(LangAS::opencl_global);
 
       addressQuals.push_back(
           llvm::ConstantAsMetadata::get(Builder.getInt32(AddrSpc)));
@@ -1143,6 +1160,28 @@ bool CodeGenFunction::containsBreak(const Stmt *S) {
   return false;
 }
 
+bool CodeGenFunction::mightAddDeclToScope(const Stmt *S) {
+  if (!S) return false;
+
+  // Some statement kinds add a scope and thus never add a decl to the current
+  // scope. Note, this list is longer than the list of statements that might
+  // have an unscoped decl nested within them, but this way is conservatively
+  // correct even if more statement kinds are added.
+  if (isa<IfStmt>(S) || isa<SwitchStmt>(S) || isa<WhileStmt>(S) ||
+      isa<DoStmt>(S) || isa<ForStmt>(S) || isa<CompoundStmt>(S) ||
+      isa<CXXForRangeStmt>(S) || isa<CXXTryStmt>(S) ||
+      isa<ObjCForCollectionStmt>(S) || isa<ObjCAtTryStmt>(S))
+    return false;
+
+  if (isa<DeclStmt>(S))
+    return true;
+
+  for (const Stmt *SubStmt : S->children())
+    if (mightAddDeclToScope(SubStmt))
+      return true;
+
+  return false;
+}
 
 /// ConstantFoldsToSimpleInteger - If the specified expression does not fold
 /// to a constant, or if it does but contains a label, return false.  If it
diff --git a/lib/CodeGen/CodeGenFunction.h b/lib/CodeGen/CodeGenFunction.h
index 45068fa55444..fb19a2657c9c 100644
--- a/lib/CodeGen/CodeGenFunction.h
+++ b/lib/CodeGen/CodeGenFunction.h
@@ -965,6 +965,94 @@ private:
   };
   SmallVector<BreakContinue, 8> BreakContinueStack;
 
+  /// Handles cancellation exit points in OpenMP-related constructs.
+  class OpenMPCancelExitStack {
+    /// Tracks cancellation exit point and join point for cancel-related exit
+    /// and normal exit.
+    struct CancelExit {
+      CancelExit() = default;
+      CancelExit(OpenMPDirectiveKind Kind, JumpDest ExitBlock,
+                 JumpDest ContBlock)
+          : Kind(Kind), ExitBlock(ExitBlock), ContBlock(ContBlock) {}
+      OpenMPDirectiveKind Kind = OMPD_unknown;
+      /// true if the exit block has been emitted already by the special
+      /// emitExit() call, false if the default codegen is used.
+      bool HasBeenEmitted = false;
+      JumpDest ExitBlock;
+      JumpDest ContBlock;
+    };
+
+    SmallVector<CancelExit, 8> Stack;
+
+  public:
+    OpenMPCancelExitStack() : Stack(1) {}
+    ~OpenMPCancelExitStack() = default;
+    /// Fetches the exit block for the current OpenMP construct.
+    JumpDest getExitBlock() const { return Stack.back().ExitBlock; }
+    /// Emits exit block with special codegen procedure specific for the related
+    /// OpenMP construct + emits code for normal construct cleanup.
+    void emitExit(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,
+                  const llvm::function_ref<void(CodeGenFunction &)> &CodeGen) {
+      if (Stack.back().Kind == Kind && getExitBlock().isValid()) {
+        assert(CGF.getOMPCancelDestination(Kind).isValid());
+        assert(CGF.HaveInsertPoint());
+        assert(!Stack.back().HasBeenEmitted);
+        auto IP = CGF.Builder.saveAndClearIP();
+        CGF.EmitBlock(Stack.back().ExitBlock.getBlock());
+        CodeGen(CGF);
+        CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);
+        CGF.Builder.restoreIP(IP);
+        Stack.back().HasBeenEmitted = true;
+      }
+      CodeGen(CGF);
+    }
+    /// Enter the cancel supporting \a Kind construct.
+    /// \param Kind OpenMP directive that supports cancel constructs.
+    /// \param HasCancel true, if the construct has inner cancel directive,
+    /// false otherwise.
+    void enter(CodeGenFunction &CGF, OpenMPDirectiveKind Kind, bool HasCancel) {
+      Stack.push_back({Kind,
+                       HasCancel ? CGF.getJumpDestInCurrentScope("cancel.exit")
+                                 : JumpDest(),
+                       HasCancel ? CGF.getJumpDestInCurrentScope("cancel.cont")
+                                 : JumpDest()});
+    }
+    /// Emits default exit point for the cancel construct (if the special one
+    /// has not be used) + join point for cancel/normal exits.
+    void exit(CodeGenFunction &CGF) {
+      if (getExitBlock().isValid()) {
+        assert(CGF.getOMPCancelDestination(Stack.back().Kind).isValid());
+        bool HaveIP = CGF.HaveInsertPoint();
+        if (!Stack.back().HasBeenEmitted) {
+          if (HaveIP)
+            CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);
+          CGF.EmitBlock(Stack.back().ExitBlock.getBlock());
+          CGF.EmitBranchThroughCleanup(Stack.back().ContBlock);
+        }
+        CGF.EmitBlock(Stack.back().ContBlock.getBlock());
+        if (!HaveIP) {
+          CGF.Builder.CreateUnreachable();
+          CGF.Builder.ClearInsertionPoint();
+        }
+      }
+      Stack.pop_back();
+    }
+  };
+  OpenMPCancelExitStack OMPCancelStack;
+
+  /// Controls insertion of cancellation exit blocks in worksharing constructs.
+  class OMPCancelStackRAII {
+    CodeGenFunction &CGF;
+
+  public:
+    OMPCancelStackRAII(CodeGenFunction &CGF, OpenMPDirectiveKind Kind,
+                       bool HasCancel)
+        : CGF(CGF) {
+      CGF.OMPCancelStack.enter(CGF, Kind, HasCancel);
+    }
+    ~OMPCancelStackRAII() { CGF.OMPCancelStack.exit(CGF); }
+  };
+
   CodeGenPGO PGO;
 
   /// Calculate branch weights appropriate for PGO data
@@ -3163,6 +3251,10 @@ public:
   /// If the statement (recursively) contains a switch or loop with a break
   /// inside of it, this is fine.
   static bool containsBreak(const Stmt *S);
+
+  /// Determine if the given statement might introduce a declaration into the
+  /// current scope, by being a (possibly-labelled) DeclStmt.
+  static bool mightAddDeclToScope(const Stmt *S);
   
   /// ConstantFoldsToSimpleInteger - If the specified expression does not fold
   /// to a constant, or if it does but contains a label, return false.  If it
diff --git a/lib/Driver/ToolChains.cpp b/lib/Driver/ToolChains.cpp
index 347aa29fde53..1b02f467c141 100644
--- a/lib/Driver/ToolChains.cpp
+++ b/lib/Driver/ToolChains.cpp
@@ -688,13 +688,13 @@ void Darwin::AddDeploymentTarget(DerivedArgList &Args) const {
     assert(iOSVersion && "Unknown target platform!");
     if (!Driver::GetReleaseVersion(iOSVersion->getValue(), Major, Minor, Micro,
                                    HadExtra) ||
-        HadExtra || Major >= 10 || Minor >= 100 || Micro >= 100)
+        HadExtra || Major >= 100 || Minor >= 100 || Micro >= 100)
       getDriver().Diag(diag::err_drv_invalid_version_number)
           << iOSVersion->getAsString(Args);
   } else if (Platform == TvOS) {
     if (!Driver::GetReleaseVersion(TvOSVersion->getValue(), Major, Minor,
                                    Micro, HadExtra) || HadExtra ||
-        Major >= 10 || Minor >= 100 || Micro >= 100)
+        Major >= 100 || Minor >= 100 || Micro >= 100)
       getDriver().Diag(diag::err_drv_invalid_version_number)
           << TvOSVersion->getAsString(Args);
   } else if (Platform == WatchOS) {
diff --git a/lib/Driver/Tools.cpp b/lib/Driver/Tools.cpp
index 31d436018450..270ed0a4e756 100644
--- a/lib/Driver/Tools.cpp
+++ b/lib/Driver/Tools.cpp
@@ -7637,23 +7637,23 @@ void darwin::Linker::AddLinkArgs(Compilation &C, const ArgList &Args,
       CmdArgs.push_back("-object_path_lto");
       CmdArgs.push_back(TmpPath);
     }
+  }
 
-    // Use -lto_library option to specify the libLTO.dylib path. Try to find
-    // it in clang installed libraries. If not found, the option is not used
-    // and 'ld' will use its default mechanism to search for libLTO.dylib.
-    if (Version[0] >= 133) {
-      // Search for libLTO in <InstalledDir>/../lib/libLTO.dylib
-      StringRef P = llvm::sys::path::parent_path(D.getInstalledDir());
-      SmallString<128> LibLTOPath(P);
-      llvm::sys::path::append(LibLTOPath, "lib");
-      llvm::sys::path::append(LibLTOPath, "libLTO.dylib");
-      if (llvm::sys::fs::exists(LibLTOPath)) {
-        CmdArgs.push_back("-lto_library");
-        CmdArgs.push_back(C.getArgs().MakeArgString(LibLTOPath));
-      } else {
-        D.Diag(diag::warn_drv_lto_libpath);
-      }
-    }
+  // Use -lto_library option to specify the libLTO.dylib path. Try to find
+  // it in clang installed libraries. ld64 will only look at this argument
+  // when it actually uses LTO, so libLTO.dylib only needs to exist at link
+  // time if ld64 decides that it needs to use LTO.
+  // Since this is passed unconditionally, ld64 will never look for libLTO.dylib
+  // next to it. That's ok since ld64 using a libLTO.dylib not matching the
+  // clang version won't work anyways.
+  if (Version[0] >= 133) {
+    // Search for libLTO in <InstalledDir>/../lib/libLTO.dylib
+    StringRef P = llvm::sys::path::parent_path(D.Dir);
+    SmallString<128> LibLTOPath(P);
+    llvm::sys::path::append(LibLTOPath, "lib");
+    llvm::sys::path::append(LibLTOPath, "libLTO.dylib");
+    CmdArgs.push_back("-lto_library");
+    CmdArgs.push_back(C.getArgs().MakeArgString(LibLTOPath));
   }
 
   // Derived from the "link" spec.
diff --git a/lib/Headers/CMakeLists.txt b/lib/Headers/CMakeLists.txt
index 657ef7e30cfa..fa2d2107781b 100644
--- a/lib/Headers/CMakeLists.txt
+++ b/lib/Headers/CMakeLists.txt
@@ -47,6 +47,7 @@ set(files
   mmintrin.h
   mm_malloc.h
   module.modulemap
+  msa.h
   mwaitxintrin.h
   nmmintrin.h
   opencl-c.h
diff --git a/lib/Headers/msa.h b/lib/Headers/msa.h
new file mode 100644
index 000000000000..da680f5ca9ee
--- /dev/null
+++ b/lib/Headers/msa.h
@@ -0,0 +1,583 @@
+/*===---- msa.h - MIPS MSA intrinsics --------------------------------------===
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this software and associated documentation files (the "Software"), to deal
+ * in the Software without restriction, including without limitation the rights
+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+ * copies of the Software, and to permit persons to whom the Software is
+ * furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+ * THE SOFTWARE.
+ *
+ *===-----------------------------------------------------------------------===
+ */
+
+#ifndef _MSA_H
+#define _MSA_H 1
+
+#if defined(__mips_msa)
+typedef signed char v16i8 __attribute__((vector_size(16), aligned(16)));
+typedef signed char v16i8_b __attribute__((vector_size(16), aligned(1)));
+typedef unsigned char v16u8 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned char v16u8_b __attribute__((vector_size(16), aligned(1)));
+typedef short v8i16 __attribute__((vector_size(16), aligned(16)));
+typedef short v8i16_h __attribute__((vector_size(16), aligned(2)));
+typedef unsigned short v8u16 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned short v8u16_h __attribute__((vector_size(16), aligned(2)));
+typedef int v4i32 __attribute__((vector_size(16), aligned(16)));
+typedef int v4i32_w __attribute__((vector_size(16), aligned(4)));
+typedef unsigned int v4u32 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned int v4u32_w __attribute__((vector_size(16), aligned(4)));
+typedef long long v2i64 __attribute__((vector_size(16), aligned(16)));
+typedef long long v2i64_d __attribute__((vector_size(16), aligned(8)));
+typedef unsigned long long v2u64 __attribute__((vector_size(16), aligned(16)));
+typedef unsigned long long v2u64_d __attribute__((vector_size(16), aligned(8)));
+typedef float v4f32 __attribute__((vector_size(16), aligned(16)));
+typedef float v4f32_w __attribute__((vector_size(16), aligned(4)));
+typedef double v2f64 __attribute__ ((vector_size(16), aligned(16)));
+typedef double v2f64_d __attribute__ ((vector_size(16), aligned(8)));
+
+#define __msa_sll_b __builtin_msa_sll_b
+#define __msa_sll_h __builtin_msa_sll_h
+#define __msa_sll_w __builtin_msa_sll_w
+#define __msa_sll_d __builtin_msa_sll_d
+#define __msa_slli_b __builtin_msa_slli_b
+#define __msa_slli_h __builtin_msa_slli_h
+#define __msa_slli_w __builtin_msa_slli_w
+#define __msa_slli_d __builtin_msa_slli_d
+#define __msa_sra_b __builtin_msa_sra_b
+#define __msa_sra_h __builtin_msa_sra_h
+#define __msa_sra_w __builtin_msa_sra_w
+#define __msa_sra_d __builtin_msa_sra_d
+#define __msa_srai_b __builtin_msa_srai_b
+#define __msa_srai_h __builtin_msa_srai_h
+#define __msa_srai_w __builtin_msa_srai_w
+#define __msa_srai_d __builtin_msa_srai_d
+#define __msa_srar_b __builtin_msa_srar_b
+#define __msa_srar_h __builtin_msa_srar_h
+#define __msa_srar_w __builtin_msa_srar_w
+#define __msa_srar_d __builtin_msa_srar_d
+#define __msa_srari_b __builtin_msa_srari_b
+#define __msa_srari_h __builtin_msa_srari_h
+#define __msa_srari_w __builtin_msa_srari_w
+#define __msa_srari_d __builtin_msa_srari_d
+#define __msa_srl_b __builtin_msa_srl_b
+#define __msa_srl_h __builtin_msa_srl_h
+#define __msa_srl_w __builtin_msa_srl_w
+#define __msa_srl_d __builtin_msa_srl_d
+#define __msa_srli_b __builtin_msa_srli_b
+#define __msa_srli_h __builtin_msa_srli_h
+#define __msa_srli_w __builtin_msa_srli_w
+#define __msa_srli_d __builtin_msa_srli_d
+#define __msa_srlr_b __builtin_msa_srlr_b
+#define __msa_srlr_h __builtin_msa_srlr_h
+#define __msa_srlr_w __builtin_msa_srlr_w
+#define __msa_srlr_d __builtin_msa_srlr_d
+#define __msa_srlri_b __builtin_msa_srlri_b
+#define __msa_srlri_h __builtin_msa_srlri_h
+#define __msa_srlri_w __builtin_msa_srlri_w
+#define __msa_srlri_d __builtin_msa_srlri_d
+#define __msa_bclr_b __builtin_msa_bclr_b
+#define __msa_bclr_h __builtin_msa_bclr_h
+#define __msa_bclr_w __builtin_msa_bclr_w
+#define __msa_bclr_d __builtin_msa_bclr_d
+#define __msa_bclri_b __builtin_msa_bclri_b
+#define __msa_bclri_h __builtin_msa_bclri_h
+#define __msa_bclri_w __builtin_msa_bclri_w
+#define __msa_bclri_d __builtin_msa_bclri_d
+#define __msa_bset_b __builtin_msa_bset_b
+#define __msa_bset_h __builtin_msa_bset_h
+#define __msa_bset_w __builtin_msa_bset_w
+#define __msa_bset_d __builtin_msa_bset_d
+#define __msa_bseti_b __builtin_msa_bseti_b
+#define __msa_bseti_h __builtin_msa_bseti_h
+#define __msa_bseti_w __builtin_msa_bseti_w
+#define __msa_bseti_d __builtin_msa_bseti_d
+#define __msa_bneg_b __builtin_msa_bneg_b
+#define __msa_bneg_h __builtin_msa_bneg_h
+#define __msa_bneg_w __builtin_msa_bneg_w
+#define __msa_bneg_d __builtin_msa_bneg_d
+#define __msa_bnegi_b __builtin_msa_bnegi_b
+#define __msa_bnegi_h __builtin_msa_bnegi_h
+#define __msa_bnegi_w __builtin_msa_bnegi_w
+#define __msa_bnegi_d __builtin_msa_bnegi_d
+#define __msa_binsl_b __builtin_msa_binsl_b
+#define __msa_binsl_h __builtin_msa_binsl_h
+#define __msa_binsl_w __builtin_msa_binsl_w
+#define __msa_binsl_d __builtin_msa_binsl_d
+#define __msa_binsli_b __builtin_msa_binsli_b
+#define __msa_binsli_h __builtin_msa_binsli_h
+#define __msa_binsli_w __builtin_msa_binsli_w
+#define __msa_binsli_d __builtin_msa_binsli_d
+#define __msa_binsr_b __builtin_msa_binsr_b
+#define __msa_binsr_h __builtin_msa_binsr_h
+#define __msa_binsr_w __builtin_msa_binsr_w
+#define __msa_binsr_d __builtin_msa_binsr_d
+#define __msa_binsri_b __builtin_msa_binsri_b
+#define __msa_binsri_h __builtin_msa_binsri_h
+#define __msa_binsri_w __builtin_msa_binsri_w
+#define __msa_binsri_d __builtin_msa_binsri_d
+#define __msa_addv_b __builtin_msa_addv_b
+#define __msa_addv_h __builtin_msa_addv_h
+#define __msa_addv_w __builtin_msa_addv_w
+#define __msa_addv_d __builtin_msa_addv_d
+#define __msa_addvi_b __builtin_msa_addvi_b
+#define __msa_addvi_h __builtin_msa_addvi_h
+#define __msa_addvi_w __builtin_msa_addvi_w
+#define __msa_addvi_d __builtin_msa_addvi_d
+#define __msa_subv_b __builtin_msa_subv_b
+#define __msa_subv_h __builtin_msa_subv_h
+#define __msa_subv_w __builtin_msa_subv_w
+#define __msa_subv_d __builtin_msa_subv_d
+#define __msa_subvi_b __builtin_msa_subvi_b
+#define __msa_subvi_h __builtin_msa_subvi_h
+#define __msa_subvi_w __builtin_msa_subvi_w
+#define __msa_subvi_d __builtin_msa_subvi_d
+#define __msa_max_s_b __builtin_msa_max_s_b
+#define __msa_max_s_h __builtin_msa_max_s_h
+#define __msa_max_s_w __builtin_msa_max_s_w
+#define __msa_max_s_d __builtin_msa_max_s_d
+#define __msa_maxi_s_b __builtin_msa_maxi_s_b
+#define __msa_maxi_s_h __builtin_msa_maxi_s_h
+#define __msa_maxi_s_w __builtin_msa_maxi_s_w
+#define __msa_maxi_s_d __builtin_msa_maxi_s_d
+#define __msa_max_u_b __builtin_msa_max_u_b
+#define __msa_max_u_h __builtin_msa_max_u_h
+#define __msa_max_u_w __builtin_msa_max_u_w
+#define __msa_max_u_d __builtin_msa_max_u_d
+#define __msa_maxi_u_b __builtin_msa_maxi_u_b
+#define __msa_maxi_u_h __builtin_msa_maxi_u_h
+#define __msa_maxi_u_w __builtin_msa_maxi_u_w
+#define __msa_maxi_u_d __builtin_msa_maxi_u_d
+#define __msa_min_s_b __builtin_msa_min_s_b
+#define __msa_min_s_h __builtin_msa_min_s_h
+#define __msa_min_s_w __builtin_msa_min_s_w
+#define __msa_min_s_d __builtin_msa_min_s_d
+#define __msa_mini_s_b __builtin_msa_mini_s_b
+#define __msa_mini_s_h __builtin_msa_mini_s_h
+#define __msa_mini_s_w __builtin_msa_mini_s_w
+#define __msa_mini_s_d __builtin_msa_mini_s_d
+#define __msa_min_u_b __builtin_msa_min_u_b
+#define __msa_min_u_h __builtin_msa_min_u_h
+#define __msa_min_u_w __builtin_msa_min_u_w
+#define __msa_min_u_d __builtin_msa_min_u_d
+#define __msa_mini_u_b __builtin_msa_mini_u_b
+#define __msa_mini_u_h __builtin_msa_mini_u_h
+#define __msa_mini_u_w __builtin_msa_mini_u_w
+#define __msa_mini_u_d __builtin_msa_mini_u_d
+#define __msa_max_a_b __builtin_msa_max_a_b
+#define __msa_max_a_h __builtin_msa_max_a_h
+#define __msa_max_a_w __builtin_msa_max_a_w
+#define __msa_max_a_d __builtin_msa_max_a_d
+#define __msa_min_a_b __builtin_msa_min_a_b
+#define __msa_min_a_h __builtin_msa_min_a_h
+#define __msa_min_a_w __builtin_msa_min_a_w
+#define __msa_min_a_d __builtin_msa_min_a_d
+#define __msa_ceq_b __builtin_msa_ceq_b
+#define __msa_ceq_h __builtin_msa_ceq_h
+#define __msa_ceq_w __builtin_msa_ceq_w
+#define __msa_ceq_d __builtin_msa_ceq_d
+#define __msa_ceqi_b __builtin_msa_ceqi_b
+#define __msa_ceqi_h __builtin_msa_ceqi_h
+#define __msa_ceqi_w __builtin_msa_ceqi_w
+#define __msa_ceqi_d __builtin_msa_ceqi_d
+#define __msa_clt_s_b __builtin_msa_clt_s_b
+#define __msa_clt_s_h __builtin_msa_clt_s_h
+#define __msa_clt_s_w __builtin_msa_clt_s_w
+#define __msa_clt_s_d __builtin_msa_clt_s_d
+#define __msa_clti_s_b __builtin_msa_clti_s_b
+#define __msa_clti_s_h __builtin_msa_clti_s_h
+#define __msa_clti_s_w __builtin_msa_clti_s_w
+#define __msa_clti_s_d __builtin_msa_clti_s_d
+#define __msa_clt_u_b __builtin_msa_clt_u_b
+#define __msa_clt_u_h __builtin_msa_clt_u_h
+#define __msa_clt_u_w __builtin_msa_clt_u_w
+#define __msa_clt_u_d __builtin_msa_clt_u_d
+#define __msa_clti_u_b __builtin_msa_clti_u_b
+#define __msa_clti_u_h __builtin_msa_clti_u_h
+#define __msa_clti_u_w __builtin_msa_clti_u_w
+#define __msa_clti_u_d __builtin_msa_clti_u_d
+#define __msa_cle_s_b __builtin_msa_cle_s_b
+#define __msa_cle_s_h __builtin_msa_cle_s_h
+#define __msa_cle_s_w __builtin_msa_cle_s_w
+#define __msa_cle_s_d __builtin_msa_cle_s_d
+#define __msa_clei_s_b __builtin_msa_clei_s_b
+#define __msa_clei_s_h __builtin_msa_clei_s_h
+#define __msa_clei_s_w __builtin_msa_clei_s_w
+#define __msa_clei_s_d __builtin_msa_clei_s_d
+#define __msa_cle_u_b __builtin_msa_cle_u_b
+#define __msa_cle_u_h __builtin_msa_cle_u_h
+#define __msa_cle_u_w __builtin_msa_cle_u_w
+#define __msa_cle_u_d __builtin_msa_cle_u_d
+#define __msa_clei_u_b __builtin_msa_clei_u_b
+#define __msa_clei_u_h __builtin_msa_clei_u_h
+#define __msa_clei_u_w __builtin_msa_clei_u_w
+#define __msa_clei_u_d __builtin_msa_clei_u_d
+#define __msa_ld_b __builtin_msa_ld_b
+#define __msa_ld_h __builtin_msa_ld_h
+#define __msa_ld_w __builtin_msa_ld_w
+#define __msa_ld_d __builtin_msa_ld_d
+#define __msa_st_b __builtin_msa_st_b
+#define __msa_st_h __builtin_msa_st_h
+#define __msa_st_w __builtin_msa_st_w
+#define __msa_st_d __builtin_msa_st_d
+#define __msa_sat_s_b __builtin_msa_sat_s_b
+#define __msa_sat_s_h __builtin_msa_sat_s_h
+#define __msa_sat_s_w __builtin_msa_sat_s_w
+#define __msa_sat_s_d __builtin_msa_sat_s_d
+#define __msa_sat_u_b __builtin_msa_sat_u_b
+#define __msa_sat_u_h __builtin_msa_sat_u_h
+#define __msa_sat_u_w __builtin_msa_sat_u_w
+#define __msa_sat_u_d __builtin_msa_sat_u_d
+#define __msa_add_a_b __builtin_msa_add_a_b
+#define __msa_add_a_h __builtin_msa_add_a_h
+#define __msa_add_a_w __builtin_msa_add_a_w
+#define __msa_add_a_d __builtin_msa_add_a_d
+#define __msa_adds_a_b __builtin_msa_adds_a_b
+#define __msa_adds_a_h __builtin_msa_adds_a_h
+#define __msa_adds_a_w __builtin_msa_adds_a_w
+#define __msa_adds_a_d __builtin_msa_adds_a_d
+#define __msa_adds_s_b __builtin_msa_adds_s_b
+#define __msa_adds_s_h __builtin_msa_adds_s_h
+#define __msa_adds_s_w __builtin_msa_adds_s_w
+#define __msa_adds_s_d __builtin_msa_adds_s_d
+#define __msa_adds_u_b __builtin_msa_adds_u_b
+#define __msa_adds_u_h __builtin_msa_adds_u_h
+#define __msa_adds_u_w __builtin_msa_adds_u_w
+#define __msa_adds_u_d __builtin_msa_adds_u_d
+#define __msa_ave_s_b __builtin_msa_ave_s_b
+#define __msa_ave_s_h __builtin_msa_ave_s_h
+#define __msa_ave_s_w __builtin_msa_ave_s_w
+#define __msa_ave_s_d __builtin_msa_ave_s_d
+#define __msa_ave_u_b __builtin_msa_ave_u_b
+#define __msa_ave_u_h __builtin_msa_ave_u_h
+#define __msa_ave_u_w __builtin_msa_ave_u_w
+#define __msa_ave_u_d __builtin_msa_ave_u_d
+#define __msa_aver_s_b __builtin_msa_aver_s_b
+#define __msa_aver_s_h __builtin_msa_aver_s_h
+#define __msa_aver_s_w __builtin_msa_aver_s_w
+#define __msa_aver_s_d __builtin_msa_aver_s_d
+#define __msa_aver_u_b __builtin_msa_aver_u_b
+#define __msa_aver_u_h __builtin_msa_aver_u_h
+#define __msa_aver_u_w __builtin_msa_aver_u_w
+#define __msa_aver_u_d __builtin_msa_aver_u_d
+#define __msa_subs_s_b __builtin_msa_subs_s_b
+#define __msa_subs_s_h __builtin_msa_subs_s_h
+#define __msa_subs_s_w __builtin_msa_subs_s_w
+#define __msa_subs_s_d __builtin_msa_subs_s_d
+#define __msa_subs_u_b __builtin_msa_subs_u_b
+#define __msa_subs_u_h __builtin_msa_subs_u_h
+#define __msa_subs_u_w __builtin_msa_subs_u_w
+#define __msa_subs_u_d __builtin_msa_subs_u_d
+#define __msa_subsuu_s_b __builtin_msa_subsuu_s_b
+#define __msa_subsuu_s_h __builtin_msa_subsuu_s_h
+#define __msa_subsuu_s_w __builtin_msa_subsuu_s_w
+#define __msa_subsuu_s_d __builtin_msa_subsuu_s_d
+#define __msa_subsus_u_b __builtin_msa_subsus_u_b
+#define __msa_subsus_u_h __builtin_msa_subsus_u_h
+#define __msa_subsus_u_w __builtin_msa_subsus_u_w
+#define __msa_subsus_u_d __builtin_msa_subsus_u_d
+#define __msa_asub_s_b __builtin_msa_asub_s_b
+#define __msa_asub_s_h __builtin_msa_asub_s_h
+#define __msa_asub_s_w __builtin_msa_asub_s_w
+#define __msa_asub_s_d __builtin_msa_asub_s_d
+#define __msa_asub_u_b __builtin_msa_asub_u_b
+#define __msa_asub_u_h __builtin_msa_asub_u_h
+#define __msa_asub_u_w __builtin_msa_asub_u_w
+#define __msa_asub_u_d __builtin_msa_asub_u_d
+#define __msa_mulv_b __builtin_msa_mulv_b
+#define __msa_mulv_h __builtin_msa_mulv_h
+#define __msa_mulv_w __builtin_msa_mulv_w
+#define __msa_mulv_d __builtin_msa_mulv_d
+#define __msa_maddv_b __builtin_msa_maddv_b
+#define __msa_maddv_h __builtin_msa_maddv_h
+#define __msa_maddv_w __builtin_msa_maddv_w
+#define __msa_maddv_d __builtin_msa_maddv_d
+#define __msa_msubv_b __builtin_msa_msubv_b
+#define __msa_msubv_h __builtin_msa_msubv_h
+#define __msa_msubv_w __builtin_msa_msubv_w
+#define __msa_msubv_d __builtin_msa_msubv_d
+#define __msa_div_s_b __builtin_msa_div_s_b
+#define __msa_div_s_h __builtin_msa_div_s_h
+#define __msa_div_s_w __builtin_msa_div_s_w
+#define __msa_div_s_d __builtin_msa_div_s_d
+#define __msa_div_u_b __builtin_msa_div_u_b
+#define __msa_div_u_h __builtin_msa_div_u_h
+#define __msa_div_u_w __builtin_msa_div_u_w
+#define __msa_div_u_d __builtin_msa_div_u_d
+#define __msa_hadd_s_h __builtin_msa_hadd_s_h
+#define __msa_hadd_s_w __builtin_msa_hadd_s_w
+#define __msa_hadd_s_d __builtin_msa_hadd_s_d
+#define __msa_hadd_u_h __builtin_msa_hadd_u_h
+#define __msa_hadd_u_w __builtin_msa_hadd_u_w
+#define __msa_hadd_u_d __builtin_msa_hadd_u_d
+#define __msa_hsub_s_h __builtin_msa_hsub_s_h
+#define __msa_hsub_s_w __builtin_msa_hsub_s_w
+#define __msa_hsub_s_d __builtin_msa_hsub_s_d
+#define __msa_hsub_u_h __builtin_msa_hsub_u_h
+#define __msa_hsub_u_w __builtin_msa_hsub_u_w
+#define __msa_hsub_u_d __builtin_msa_hsub_u_d
+#define __msa_mod_s_b __builtin_msa_mod_s_b
+#define __msa_mod_s_h __builtin_msa_mod_s_h
+#define __msa_mod_s_w __builtin_msa_mod_s_w
+#define __msa_mod_s_d __builtin_msa_mod_s_d
+#define __msa_mod_u_b __builtin_msa_mod_u_b
+#define __msa_mod_u_h __builtin_msa_mod_u_h
+#define __msa_mod_u_w __builtin_msa_mod_u_w
+#define __msa_mod_u_d __builtin_msa_mod_u_d
+#define __msa_dotp_s_h __builtin_msa_dotp_s_h
+#define __msa_dotp_s_w __builtin_msa_dotp_s_w
+#define __msa_dotp_s_d __builtin_msa_dotp_s_d
+#define __msa_dotp_u_h __builtin_msa_dotp_u_h
+#define __msa_dotp_u_w __builtin_msa_dotp_u_w
+#define __msa_dotp_u_d __builtin_msa_dotp_u_d
+#define __msa_dpadd_s_h __builtin_msa_dpadd_s_h
+#define __msa_dpadd_s_w __builtin_msa_dpadd_s_w
+#define __msa_dpadd_s_d __builtin_msa_dpadd_s_d
+#define __msa_dpadd_u_h __builtin_msa_dpadd_u_h
+#define __msa_dpadd_u_w __builtin_msa_dpadd_u_w
+#define __msa_dpadd_u_d __builtin_msa_dpadd_u_d
+#define __msa_dpsub_s_h __builtin_msa_dpsub_s_h
+#define __msa_dpsub_s_w __builtin_msa_dpsub_s_w
+#define __msa_dpsub_s_d __builtin_msa_dpsub_s_d
+#define __msa_dpsub_u_h __builtin_msa_dpsub_u_h
+#define __msa_dpsub_u_w __builtin_msa_dpsub_u_w
+#define __msa_dpsub_u_d __builtin_msa_dpsub_u_d
+#define __msa_sld_b __builtin_msa_sld_b
+#define __msa_sld_h __builtin_msa_sld_h
+#define __msa_sld_w __builtin_msa_sld_w
+#define __msa_sld_d __builtin_msa_sld_d
+#define __msa_sldi_b __builtin_msa_sldi_b
+#define __msa_sldi_h __builtin_msa_sldi_h
+#define __msa_sldi_w __builtin_msa_sldi_w
+#define __msa_sldi_d __builtin_msa_sldi_d
+#define __msa_splat_b __builtin_msa_splat_b
+#define __msa_splat_h __builtin_msa_splat_h
+#define __msa_splat_w __builtin_msa_splat_w
+#define __msa_splat_d __builtin_msa_splat_d
+#define __msa_splati_b __builtin_msa_splati_b
+#define __msa_splati_h __builtin_msa_splati_h
+#define __msa_splati_w __builtin_msa_splati_w
+#define __msa_splati_d __builtin_msa_splati_d
+#define __msa_pckev_b __builtin_msa_pckev_b
+#define __msa_pckev_h __builtin_msa_pckev_h
+#define __msa_pckev_w __builtin_msa_pckev_w
+#define __msa_pckev_d __builtin_msa_pckev_d
+#define __msa_pckod_b __builtin_msa_pckod_b
+#define __msa_pckod_h __builtin_msa_pckod_h
+#define __msa_pckod_w __builtin_msa_pckod_w
+#define __msa_pckod_d __builtin_msa_pckod_d
+#define __msa_ilvl_b __builtin_msa_ilvl_b
+#define __msa_ilvl_h __builtin_msa_ilvl_h
+#define __msa_ilvl_w __builtin_msa_ilvl_w
+#define __msa_ilvl_d __builtin_msa_ilvl_d
+#define __msa_ilvr_b __builtin_msa_ilvr_b
+#define __msa_ilvr_h __builtin_msa_ilvr_h
+#define __msa_ilvr_w __builtin_msa_ilvr_w
+#define __msa_ilvr_d __builtin_msa_ilvr_d
+#define __msa_ilvev_b __builtin_msa_ilvev_b
+#define __msa_ilvev_h __builtin_msa_ilvev_h
+#define __msa_ilvev_w __builtin_msa_ilvev_w
+#define __msa_ilvev_d __builtin_msa_ilvev_d
+#define __msa_ilvod_b __builtin_msa_ilvod_b
+#define __msa_ilvod_h __builtin_msa_ilvod_h
+#define __msa_ilvod_w __builtin_msa_ilvod_w
+#define __msa_ilvod_d __builtin_msa_ilvod_d
+#define __msa_vshf_b __builtin_msa_vshf_b
+#define __msa_vshf_h __builtin_msa_vshf_h
+#define __msa_vshf_w __builtin_msa_vshf_w
+#define __msa_vshf_d __builtin_msa_vshf_d
+#define __msa_and_v __builtin_msa_and_v
+#define __msa_andi_b __builtin_msa_andi_b
+#define __msa_or_v __builtin_msa_or_v
+#define __msa_ori_b __builtin_msa_ori_b
+#define __msa_nor_v __builtin_msa_nor_v
+#define __msa_nori_b __builtin_msa_nori_b
+#define __msa_xor_v __builtin_msa_xor_v
+#define __msa_xori_b __builtin_msa_xori_b
+#define __msa_bmnz_v __builtin_msa_bmnz_v
+#define __msa_bmnzi_b __builtin_msa_bmnzi_b
+#define __msa_bmz_v __builtin_msa_bmz_v
+#define __msa_bmzi_b __builtin_msa_bmzi_b
+#define __msa_bsel_v __builtin_msa_bsel_v
+#define __msa_bseli_b __builtin_msa_bseli_b
+#define __msa_shf_b __builtin_msa_shf_b
+#define __msa_shf_h __builtin_msa_shf_h
+#define __msa_shf_w __builtin_msa_shf_w
+#define __msa_test_bnz_v __builtin_msa_bnz_v
+#define __msa_test_bz_v __builtin_msa_bz_v
+#define __msa_fill_b __builtin_msa_fill_b
+#define __msa_fill_h __builtin_msa_fill_h
+#define __msa_fill_w __builtin_msa_fill_w
+#define __msa_fill_d __builtin_msa_fill_d
+#define __msa_pcnt_b __builtin_msa_pcnt_b
+#define __msa_pcnt_h __builtin_msa_pcnt_h
+#define __msa_pcnt_w __builtin_msa_pcnt_w
+#define __msa_pcnt_d __builtin_msa_pcnt_d
+#define __msa_nloc_b __builtin_msa_nloc_b
+#define __msa_nloc_h __builtin_msa_nloc_h
+#define __msa_nloc_w __builtin_msa_nloc_w
+#define __msa_nloc_d __builtin_msa_nloc_d
+#define __msa_nlzc_b __builtin_msa_nlzc_b
+#define __msa_nlzc_h __builtin_msa_nlzc_h
+#define __msa_nlzc_w __builtin_msa_nlzc_w
+#define __msa_nlzc_d __builtin_msa_nlzc_d
+#define __msa_copy_s_b __builtin_msa_copy_s_b
+#define __msa_copy_s_h __builtin_msa_copy_s_h
+#define __msa_copy_s_w __builtin_msa_copy_s_w
+#define __msa_copy_s_d __builtin_msa_copy_s_d
+#define __msa_copy_u_b __builtin_msa_copy_u_b
+#define __msa_copy_u_h __builtin_msa_copy_u_h
+#define __msa_copy_u_w __builtin_msa_copy_u_w
+#define __msa_copy_u_d __builtin_msa_copy_u_d
+#define __msa_insert_b __builtin_msa_insert_b
+#define __msa_insert_h __builtin_msa_insert_h
+#define __msa_insert_w __builtin_msa_insert_w
+#define __msa_insert_d __builtin_msa_insert_d
+#define __msa_insve_b __builtin_msa_insve_b
+#define __msa_insve_h __builtin_msa_insve_h
+#define __msa_insve_w __builtin_msa_insve_w
+#define __msa_insve_d __builtin_msa_insve_d
+#define __msa_test_bnz_b __builtin_msa_bnz_b
+#define __msa_test_bnz_h __builtin_msa_bnz_h
+#define __msa_test_bnz_w __builtin_msa_bnz_w
+#define __msa_test_bnz_d __builtin_msa_bnz_d
+#define __msa_test_bz_b __builtin_msa_bz_b
+#define __msa_test_bz_h __builtin_msa_bz_h
+#define __msa_test_bz_w __builtin_msa_bz_w
+#define __msa_test_bz_d __builtin_msa_bz_d
+#define __msa_ldi_b __builtin_msa_ldi_b
+#define __msa_ldi_h __builtin_msa_ldi_h
+#define __msa_ldi_w __builtin_msa_ldi_w
+#define __msa_ldi_d __builtin_msa_ldi_d
+#define __msa_fcaf_w __builtin_msa_fcaf_w
+#define __msa_fcaf_d __builtin_msa_fcaf_d
+#define __msa_fcor_w __builtin_msa_fcor_w
+#define __msa_fcor_d __builtin_msa_fcor_d
+#define __msa_fcun_w __builtin_msa_fcun_w
+#define __msa_fcun_d __builtin_msa_fcun_d
+#define __msa_fcune_w __builtin_msa_fcune_w
+#define __msa_fcune_d __builtin_msa_fcune_d
+#define __msa_fcueq_w __builtin_msa_fcueq_w
+#define __msa_fcueq_d __builtin_msa_fcueq_d
+#define __msa_fceq_w __builtin_msa_fceq_w
+#define __msa_fceq_d __builtin_msa_fceq_d
+#define __msa_fcne_w __builtin_msa_fcne_w
+#define __msa_fcne_d __builtin_msa_fcne_d
+#define __msa_fclt_w __builtin_msa_fclt_w
+#define __msa_fclt_d __builtin_msa_fclt_d
+#define __msa_fcult_w __builtin_msa_fcult_w
+#define __msa_fcult_d __builtin_msa_fcult_d
+#define __msa_fcle_w __builtin_msa_fcle_w
+#define __msa_fcle_d __builtin_msa_fcle_d
+#define __msa_fcule_w __builtin_msa_fcule_w
+#define __msa_fcule_d __builtin_msa_fcule_d
+#define __msa_fsaf_w __builtin_msa_fsaf_w
+#define __msa_fsaf_d __builtin_msa_fsaf_d
+#define __msa_fsor_w __builtin_msa_fsor_w
+#define __msa_fsor_d __builtin_msa_fsor_d
+#define __msa_fsun_w __builtin_msa_fsun_w
+#define __msa_fsun_d __builtin_msa_fsun_d
+#define __msa_fsune_w __builtin_msa_fsune_w
+#define __msa_fsune_d __builtin_msa_fsune_d
+#define __msa_fsueq_w __builtin_msa_fsueq_w
+#define __msa_fsueq_d __builtin_msa_fsueq_d
+#define __msa_fseq_w __builtin_msa_fseq_w
+#define __msa_fseq_d __builtin_msa_fseq_d
+#define __msa_fsne_w __builtin_msa_fsne_w
+#define __msa_fsne_d __builtin_msa_fsne_d
+#define __msa_fslt_w __builtin_msa_fslt_w
+#define __msa_fslt_d __builtin_msa_fslt_d
+#define __msa_fsult_w __builtin_msa_fsult_w
+#define __msa_fsult_d __builtin_msa_fsult_d
+#define __msa_fsle_w __builtin_msa_fsle_w
+#define __msa_fsle_d __builtin_msa_fsle_d
+#define __msa_fsule_w __builtin_msa_fsule_w
+#define __msa_fsule_d __builtin_msa_fsule_d
+#define __msa_fadd_w __builtin_msa_fadd_w
+#define __msa_fadd_d __builtin_msa_fadd_d
+#define __msa_fsub_w __builtin_msa_fsub_w
+#define __msa_fsub_d __builtin_msa_fsub_d
+#define __msa_fmul_w __builtin_msa_fmul_w
+#define __msa_fmul_d __builtin_msa_fmul_d
+#define __msa_fdiv_w __builtin_msa_fdiv_w
+#define __msa_fdiv_d __builtin_msa_fdiv_d
+#define __msa_fmadd_w __builtin_msa_fmadd_w
+#define __msa_fmadd_d __builtin_msa_fmadd_d
+#define __msa_fmsub_w __builtin_msa_fmsub_w
+#define __msa_fmsub_d __builtin_msa_fmsub_d
+#define __msa_fexp2_w __builtin_msa_fexp2_w
+#define __msa_fexp2_d __builtin_msa_fexp2_d
+#define __msa_fexdo_h __builtin_msa_fexdo_h
+#define __msa_fexdo_w __builtin_msa_fexdo_w
+#define __msa_ftq_h __builtin_msa_ftq_h
+#define __msa_ftq_w __builtin_msa_ftq_w
+#define __msa_fmin_w __builtin_msa_fmin_w
+#define __msa_fmin_d __builtin_msa_fmin_d
+#define __msa_fmin_a_w __builtin_msa_fmin_a_w
+#define __msa_fmin_a_d __builtin_msa_fmin_a_d
+#define __msa_fmax_w __builtin_msa_fmax_w
+#define __msa_fmax_d __builtin_msa_fmax_d
+#define __msa_fmax_a_w __builtin_msa_fmax_a_w
+#define __msa_fmax_a_d __builtin_msa_fmax_a_d
+#define __msa_mul_q_h __builtin_msa_mul_q_h
+#define __msa_mul_q_w __builtin_msa_mul_q_w
+#define __msa_mulr_q_h __builtin_msa_mulr_q_h
+#define __msa_mulr_q_w __builtin_msa_mulr_q_w
+#define __msa_madd_q_h __builtin_msa_madd_q_h
+#define __msa_madd_q_w __builtin_msa_madd_q_w
+#define __msa_maddr_q_h __builtin_msa_maddr_q_h
+#define __msa_maddr_q_w __builtin_msa_maddr_q_w
+#define __msa_msub_q_h __builtin_msa_msub_q_h
+#define __msa_msub_q_w __builtin_msa_msub_q_w
+#define __msa_msubr_q_h __builtin_msa_msubr_q_h
+#define __msa_msubr_q_w __builtin_msa_msubr_q_w
+#define __msa_fclass_w __builtin_msa_fclass_w
+#define __msa_fclass_d __builtin_msa_fclass_d
+#define __msa_fsqrt_w __builtin_msa_fsqrt_w
+#define __msa_fsqrt_d __builtin_msa_fsqrt_d
+#define __msa_frcp_w __builtin_msa_frcp_w
+#define __msa_frcp_d __builtin_msa_frcp_d
+#define __msa_frint_w __builtin_msa_frint_w
+#define __msa_frint_d __builtin_msa_frint_d
+#define __msa_frsqrt_w __builtin_msa_frsqrt_w
+#define __msa_frsqrt_d __builtin_msa_frsqrt_d
+#define __msa_flog2_w __builtin_msa_flog2_w
+#define __msa_flog2_d __builtin_msa_flog2_d
+#define __msa_fexupl_w __builtin_msa_fexupl_w
+#define __msa_fexupl_d __builtin_msa_fexupl_d
+#define __msa_fexupr_w __builtin_msa_fexupr_w
+#define __msa_fexupr_d __builtin_msa_fexupr_d
+#define __msa_ffql_w __builtin_msa_ffql_w
+#define __msa_ffql_d __builtin_msa_ffql_d
+#define __msa_ffqr_w __builtin_msa_ffqr_w
+#define __msa_ffqr_d __builtin_msa_ffqr_d
+#define __msa_ftint_s_w __builtin_msa_ftint_s_w
+#define __msa_ftint_s_d __builtin_msa_ftint_s_d
+#define __msa_ftint_u_w __builtin_msa_ftint_u_w
+#define __msa_ftint_u_d __builtin_msa_ftint_u_d
+#define __msa_ftrunc_s_w __builtin_msa_ftrunc_s_w
+#define __msa_ftrunc_s_d __builtin_msa_ftrunc_s_d
+#define __msa_ftrunc_u_w __builtin_msa_ftrunc_u_w
+#define __msa_ftrunc_u_d __builtin_msa_ftrunc_u_d
+#define __msa_ffint_s_w __builtin_msa_ffint_s_w
+#define __msa_ffint_s_d __builtin_msa_ffint_s_d
+#define __msa_ffint_u_w __builtin_msa_ffint_u_w
+#define __msa_ffint_u_d __builtin_msa_ffint_u_d
+#define __msa_cfcmsa __builtin_msa_cfcmsa
+#define __msa_move_v __builtin_msa_move_v
+#define __msa_cast_to_vector_float __builtin_msa_cast_to_vector_float
+#define __msa_cast_to_vector_double __builtin_msa_cast_to_vector_double
+#define __msa_cast_to_scalar_float __builtin_msa_cast_to_scalar_float
+#define __msa_cast_to_scalar_double __builtin_msa_cast_to_scalar_double
+#endif /* defined(__mips_msa) */
+#endif /* _MSA_H */
diff --git a/lib/Sema/Sema.cpp b/lib/Sema/Sema.cpp
index a242ace9f64e..777747606304 100644
--- a/lib/Sema/Sema.cpp
+++ b/lib/Sema/Sema.cpp
@@ -1197,11 +1197,19 @@ BlockScopeInfo *Sema::getCurBlock() {
   return CurBSI;
 }
 
-LambdaScopeInfo *Sema::getCurLambda() {
+LambdaScopeInfo *Sema::getCurLambda(bool IgnoreCapturedRegions) {
   if (FunctionScopes.empty())
     return nullptr;
 
-  auto CurLSI = dyn_cast<LambdaScopeInfo>(FunctionScopes.back());
+  auto I = FunctionScopes.rbegin();
+  if (IgnoreCapturedRegions) {
+    auto E = FunctionScopes.rend();
+    while (I != E && isa<CapturedRegionScopeInfo>(*I))
+      ++I;
+    if (I == E)
+      return nullptr;
+  }
+  auto *CurLSI = dyn_cast<LambdaScopeInfo>(*I);
   if (CurLSI && CurLSI->Lambda &&
       !CurLSI->Lambda->Encloses(CurContext)) {
     // We have switched contexts due to template instantiation.
diff --git a/lib/Sema/SemaCXXScopeSpec.cpp b/lib/Sema/SemaCXXScopeSpec.cpp
index ab0e709a97e7..949263d24897 100644
--- a/lib/Sema/SemaCXXScopeSpec.cpp
+++ b/lib/Sema/SemaCXXScopeSpec.cpp
@@ -806,7 +806,7 @@ bool Sema::BuildCXXNestedNameSpecifier(Scope *S,
   if (!Found.empty()) {
     if (TypeDecl *TD = Found.getAsSingle<TypeDecl>())
       Diag(IdentifierLoc, diag::err_expected_class_or_namespace)
-          << QualType(TD->getTypeForDecl(), 0) << getLangOpts().CPlusPlus;
+          << Context.getTypeDeclType(TD) << getLangOpts().CPlusPlus;
     else {
       Diag(IdentifierLoc, diag::err_expected_class_or_namespace)
           << &Identifier << getLangOpts().CPlusPlus;
diff --git a/lib/Sema/SemaChecking.cpp b/lib/Sema/SemaChecking.cpp
index b8e7ede2716c..7f7dbe8873d4 100644
--- a/lib/Sema/SemaChecking.cpp
+++ b/lib/Sema/SemaChecking.cpp
@@ -1454,8 +1454,17 @@ bool Sema::CheckAArch64BuiltinFunctionCall(unsigned BuiltinID,
   return SemaBuiltinConstantArgRange(TheCall, i, l, u + l);
 }
 
+// CheckMipsBuiltinFunctionCall - Checks the constant value passed to the
+// intrinsic is correct. The switch statement is ordered by DSP, MSA. The
+// ordering for DSP is unspecified. MSA is ordered by the data format used
+// by the underlying instruction i.e., df/m, df/n and then by size.
+//
+// FIXME: The size tests here should instead be tablegen'd along with the
+//        definitions from include/clang/Basic/BuiltinsMips.def.
+// FIXME: GCC is strict on signedness for some of these intrinsics, we should
+//        be too.
 bool Sema::CheckMipsBuiltinFunctionCall(unsigned BuiltinID, CallExpr *TheCall) {
-  unsigned i = 0, l = 0, u = 0;
+  unsigned i = 0, l = 0, u = 0, m = 0;
   switch (BuiltinID) {
   default: return false;
   case Mips::BI__builtin_mips_wrdsp: i = 1; l = 0; u = 63; break;
@@ -1465,9 +1474,168 @@ bool Sema::CheckMipsBuiltinFunctionCall(unsigned BuiltinID, CallExpr *TheCall) {
   case Mips::BI__builtin_mips_precr_sra_ph_w: i = 2; l = 0; u = 31; break;
   case Mips::BI__builtin_mips_precr_sra_r_ph_w: i = 2; l = 0; u = 31; break;
   case Mips::BI__builtin_mips_prepend: i = 2; l = 0; u = 31; break;
-  }
-
-  return SemaBuiltinConstantArgRange(TheCall, i, l, u);
+  // MSA instrinsics. Instructions (which the intrinsics maps to) which use the
+  // df/m field.
+  // These intrinsics take an unsigned 3 bit immediate.
+  case Mips::BI__builtin_msa_bclri_b:
+  case Mips::BI__builtin_msa_bnegi_b:
+  case Mips::BI__builtin_msa_bseti_b:
+  case Mips::BI__builtin_msa_sat_s_b:
+  case Mips::BI__builtin_msa_sat_u_b:
+  case Mips::BI__builtin_msa_slli_b:
+  case Mips::BI__builtin_msa_srai_b:
+  case Mips::BI__builtin_msa_srari_b:
+  case Mips::BI__builtin_msa_srli_b:
+  case Mips::BI__builtin_msa_srlri_b: i = 1; l = 0; u = 7; break;
+  case Mips::BI__builtin_msa_binsli_b:
+  case Mips::BI__builtin_msa_binsri_b: i = 2; l = 0; u = 7; break;
+  // These intrinsics take an unsigned 4 bit immediate.
+  case Mips::BI__builtin_msa_bclri_h:
+  case Mips::BI__builtin_msa_bnegi_h:
+  case Mips::BI__builtin_msa_bseti_h:
+  case Mips::BI__builtin_msa_sat_s_h:
+  case Mips::BI__builtin_msa_sat_u_h:
+  case Mips::BI__builtin_msa_slli_h:
+  case Mips::BI__builtin_msa_srai_h:
+  case Mips::BI__builtin_msa_srari_h:
+  case Mips::BI__builtin_msa_srli_h:
+  case Mips::BI__builtin_msa_srlri_h: i = 1; l = 0; u = 15; break;
+  case Mips::BI__builtin_msa_binsli_h:
+  case Mips::BI__builtin_msa_binsri_h: i = 2; l = 0; u = 15; break;
+  // These intrinsics take an unsigned 5 bit immedate.
+  // The first block of intrinsics actually have an unsigned 5 bit field,
+  // not a df/n field.
+  case Mips::BI__builtin_msa_clei_u_b:
+  case Mips::BI__builtin_msa_clei_u_h:
+  case Mips::BI__builtin_msa_clei_u_w:
+  case Mips::BI__builtin_msa_clei_u_d:
+  case Mips::BI__builtin_msa_clti_u_b:
+  case Mips::BI__builtin_msa_clti_u_h:
+  case Mips::BI__builtin_msa_clti_u_w:
+  case Mips::BI__builtin_msa_clti_u_d:
+  case Mips::BI__builtin_msa_maxi_u_b:
+  case Mips::BI__builtin_msa_maxi_u_h:
+  case Mips::BI__builtin_msa_maxi_u_w:
+  case Mips::BI__builtin_msa_maxi_u_d:
+  case Mips::BI__builtin_msa_mini_u_b:
+  case Mips::BI__builtin_msa_mini_u_h:
+  case Mips::BI__builtin_msa_mini_u_w:
+  case Mips::BI__builtin_msa_mini_u_d:
+  case Mips::BI__builtin_msa_addvi_b:
+  case Mips::BI__builtin_msa_addvi_h:
+  case Mips::BI__builtin_msa_addvi_w:
+  case Mips::BI__builtin_msa_addvi_d:
+  case Mips::BI__builtin_msa_bclri_w:
+  case Mips::BI__builtin_msa_bnegi_w:
+  case Mips::BI__builtin_msa_bseti_w:
+  case Mips::BI__builtin_msa_sat_s_w:
+  case Mips::BI__builtin_msa_sat_u_w:
+  case Mips::BI__builtin_msa_slli_w:
+  case Mips::BI__builtin_msa_srai_w:
+  case Mips::BI__builtin_msa_srari_w:
+  case Mips::BI__builtin_msa_srli_w:
+  case Mips::BI__builtin_msa_srlri_w:
+  case Mips::BI__builtin_msa_subvi_b:
+  case Mips::BI__builtin_msa_subvi_h:
+  case Mips::BI__builtin_msa_subvi_w:
+  case Mips::BI__builtin_msa_subvi_d: i = 1; l = 0; u = 31; break;
+  case Mips::BI__builtin_msa_binsli_w:
+  case Mips::BI__builtin_msa_binsri_w: i = 2; l = 0; u = 31; break;
+  // These intrinsics take an unsigned 6 bit immediate.
+  case Mips::BI__builtin_msa_bclri_d:
+  case Mips::BI__builtin_msa_bnegi_d:
+  case Mips::BI__builtin_msa_bseti_d:
+  case Mips::BI__builtin_msa_sat_s_d:
+  case Mips::BI__builtin_msa_sat_u_d:
+  case Mips::BI__builtin_msa_slli_d:
+  case Mips::BI__builtin_msa_srai_d:
+  case Mips::BI__builtin_msa_srari_d:
+  case Mips::BI__builtin_msa_srli_d:
+  case Mips::BI__builtin_msa_srlri_d: i = 1; l = 0; u = 63; break;
+  case Mips::BI__builtin_msa_binsli_d:
+  case Mips::BI__builtin_msa_binsri_d: i = 2; l = 0; u = 63; break;
+  // These intrinsics take a signed 5 bit immediate.
+  case Mips::BI__builtin_msa_ceqi_b:
+  case Mips::BI__builtin_msa_ceqi_h:
+  case Mips::BI__builtin_msa_ceqi_w:
+  case Mips::BI__builtin_msa_ceqi_d:
+  case Mips::BI__builtin_msa_clti_s_b:
+  case Mips::BI__builtin_msa_clti_s_h:
+  case Mips::BI__builtin_msa_clti_s_w:
+  case Mips::BI__builtin_msa_clti_s_d:
+  case Mips::BI__builtin_msa_clei_s_b:
+  case Mips::BI__builtin_msa_clei_s_h:
+  case Mips::BI__builtin_msa_clei_s_w:
+  case Mips::BI__builtin_msa_clei_s_d:
+  case Mips::BI__builtin_msa_maxi_s_b:
+  case Mips::BI__builtin_msa_maxi_s_h:
+  case Mips::BI__builtin_msa_maxi_s_w:
+  case Mips::BI__builtin_msa_maxi_s_d:
+  case Mips::BI__builtin_msa_mini_s_b:
+  case Mips::BI__builtin_msa_mini_s_h:
+  case Mips::BI__builtin_msa_mini_s_w:
+  case Mips::BI__builtin_msa_mini_s_d: i = 1; l = -16; u = 15; break;
+  // These intrinsics take an unsigned 8 bit immediate.
+  case Mips::BI__builtin_msa_andi_b:
+  case Mips::BI__builtin_msa_nori_b:
+  case Mips::BI__builtin_msa_ori_b:
+  case Mips::BI__builtin_msa_shf_b:
+  case Mips::BI__builtin_msa_shf_h:
+  case Mips::BI__builtin_msa_shf_w:
+  case Mips::BI__builtin_msa_xori_b: i = 1; l = 0; u = 255; break;
+  case Mips::BI__builtin_msa_bseli_b:
+  case Mips::BI__builtin_msa_bmnzi_b:
+  case Mips::BI__builtin_msa_bmzi_b: i = 2; l = 0; u = 255; break;
+  // df/n format
+  // These intrinsics take an unsigned 4 bit immediate.
+  case Mips::BI__builtin_msa_copy_s_b:
+  case Mips::BI__builtin_msa_copy_u_b:
+  case Mips::BI__builtin_msa_insve_b:
+  case Mips::BI__builtin_msa_splati_b: i = 1; l = 0; u = 15; break;
+  case Mips::BI__builtin_msa_sld_b:
+  case Mips::BI__builtin_msa_sldi_b: i = 2; l = 0; u = 15; break;
+  // These intrinsics take an unsigned 3 bit immediate.
+  case Mips::BI__builtin_msa_copy_s_h:
+  case Mips::BI__builtin_msa_copy_u_h:
+  case Mips::BI__builtin_msa_insve_h:
+  case Mips::BI__builtin_msa_splati_h: i = 1; l = 0; u = 7; break;
+  case Mips::BI__builtin_msa_sld_h:
+  case Mips::BI__builtin_msa_sldi_h: i = 2; l = 0; u = 7; break;
+  // These intrinsics take an unsigned 2 bit immediate.
+  case Mips::BI__builtin_msa_copy_s_w:
+  case Mips::BI__builtin_msa_copy_u_w:
+  case Mips::BI__builtin_msa_insve_w:
+  case Mips::BI__builtin_msa_splati_w: i = 1; l = 0; u = 3; break;
+  case Mips::BI__builtin_msa_sld_w:
+  case Mips::BI__builtin_msa_sldi_w: i = 2; l = 0; u = 3; break;
+  // These intrinsics take an unsigned 1 bit immediate.
+  case Mips::BI__builtin_msa_copy_s_d:
+  case Mips::BI__builtin_msa_copy_u_d:
+  case Mips::BI__builtin_msa_insve_d:
+  case Mips::BI__builtin_msa_splati_d: i = 1; l = 0; u = 1; break;
+  case Mips::BI__builtin_msa_sld_d:
+  case Mips::BI__builtin_msa_sldi_d: i = 2; l = 0; u = 1; break;
+  // Memory offsets and immediate loads.
+  // These intrinsics take a signed 10 bit immediate.
+  case Mips::BI__builtin_msa_ldi_b: i = 0; l = -128; u = 127; break;
+  case Mips::BI__builtin_msa_ldi_h:
+  case Mips::BI__builtin_msa_ldi_w:
+  case Mips::BI__builtin_msa_ldi_d: i = 0; l = -512; u = 511; break;
+  case Mips::BI__builtin_msa_ld_b: i = 1; l = -512; u = 511; m = 16; break;
+  case Mips::BI__builtin_msa_ld_h: i = 1; l = -1024; u = 1022; m = 16; break;
+  case Mips::BI__builtin_msa_ld_w: i = 1; l = -2048; u = 2044; m = 16; break;
+  case Mips::BI__builtin_msa_ld_d: i = 1; l = -4096; u = 4088; m = 16; break;
+  case Mips::BI__builtin_msa_st_b: i = 2; l = -512; u = 511; m = 16; break;
+  case Mips::BI__builtin_msa_st_h: i = 2; l = -1024; u = 1022; m = 16; break;
+  case Mips::BI__builtin_msa_st_w: i = 2; l = -2048; u = 2044; m = 16; break;
+  case Mips::BI__builtin_msa_st_d: i = 2; l = -4096; u = 4088; m = 16; break;
+  }
+
+  if (!m)
+    return SemaBuiltinConstantArgRange(TheCall, i, l, u);
+
+  return SemaBuiltinConstantArgRange(TheCall, i, l, u) ||
+         SemaBuiltinConstantArgMultiple(TheCall, i, m);
 }
 
 bool Sema::CheckPPCBuiltinFunctionCall(unsigned BuiltinID, CallExpr *TheCall) {
@@ -3605,6 +3773,28 @@ bool Sema::SemaBuiltinConstantArgRange(CallExpr *TheCall, int ArgNum,
   return false;
 }
 
+/// SemaBuiltinConstantArgMultiple - Handle a check if argument ArgNum of CallExpr
+/// TheCall is a constant expression is a multiple of Num..
+bool Sema::SemaBuiltinConstantArgMultiple(CallExpr *TheCall, int ArgNum,
+                                          unsigned Num) {
+  llvm::APSInt Result;
+
+  // We can't check the value of a dependent argument.
+  Expr *Arg = TheCall->getArg(ArgNum);
+  if (Arg->isTypeDependent() || Arg->isValueDependent())
+    return false;
+
+  // Check constant-ness first.
+  if (SemaBuiltinConstantArg(TheCall, ArgNum, Result))
+    return true;
+
+  if (Result.getSExtValue() % Num != 0)
+    return Diag(TheCall->getLocStart(), diag::err_argument_not_multiple)
+      << Num << Arg->getSourceRange();
+
+  return false;
+}
+
 /// SemaBuiltinARMSpecialReg - Handle a check if argument ArgNum of CallExpr
 /// TheCall is an ARM/AArch64 special register string literal.
 bool Sema::SemaBuiltinARMSpecialReg(unsigned BuiltinID, CallExpr *TheCall,
diff --git a/lib/Sema/SemaDecl.cpp b/lib/Sema/SemaDecl.cpp
index ea276a995d81..41719d4e7b08 100644
--- a/lib/Sema/SemaDecl.cpp
+++ b/lib/Sema/SemaDecl.cpp
@@ -9615,7 +9615,8 @@ void Sema::AddInitializerToDecl(Decl *RealDecl, Expr *Init,
   }
 
   VarDecl *Def;
-  if ((Def = VDecl->getDefinition()) && Def != VDecl) {
+  if ((Def = VDecl->getDefinition()) && Def != VDecl &&
+      (!VDecl->isStaticDataMember() || VDecl->isOutOfLine())) {
     NamedDecl *Hidden = nullptr;
     if (!hasVisibleDefinition(Def, &Hidden) &&
         (VDecl->getFormalLinkage() == InternalLinkage ||
diff --git a/lib/Sema/SemaExpr.cpp b/lib/Sema/SemaExpr.cpp
index adcc38cbc4fd..719e1e3502ca 100644
--- a/lib/Sema/SemaExpr.cpp
+++ b/lib/Sema/SemaExpr.cpp
@@ -4522,6 +4522,11 @@ ExprResult Sema::BuildCXXDefaultArgExpr(SourceLocation CallLoc,
                                MutiLevelArgList.getInnermost());
     if (Inst.isInvalid())
       return ExprError();
+    if (Inst.isAlreadyInstantiating()) {
+      Diag(Param->getLocStart(), diag::err_recursive_default_argument) << FD;
+      Param->setInvalidDecl();
+      return ExprError();
+    }
 
     ExprResult Result;
     {
@@ -13880,7 +13885,8 @@ static void DoMarkVarDeclReferenced(Sema &SemaRef, SourceLocation Loc,
         (SemaRef.CurContext != Var->getDeclContext() &&
          Var->getDeclContext()->isFunctionOrMethod() && Var->hasLocalStorage());
     if (RefersToEnclosingScope) {
-      if (LambdaScopeInfo *const LSI = SemaRef.getCurLambda()) {
+      if (LambdaScopeInfo *const LSI =
+              SemaRef.getCurLambda(/*IgnoreCapturedRegions=*/true)) {
         // If a variable could potentially be odr-used, defer marking it so
         // until we finish analyzing the full expression for any
         // lvalue-to-rvalue
diff --git a/lib/Sema/SemaExprCXX.cpp b/lib/Sema/SemaExprCXX.cpp
index b7a968e09d4d..dfdd36752bf6 100644
--- a/lib/Sema/SemaExprCXX.cpp
+++ b/lib/Sema/SemaExprCXX.cpp
@@ -4221,9 +4221,12 @@ static bool EvaluateUnaryTypeTrait(Sema &Self, TypeTrait UTT,
         // A template constructor is never a copy constructor.
         // FIXME: However, it may actually be selected at the actual overload
         // resolution point.
-        if (isa<FunctionTemplateDecl>(ND))
+        if (isa<FunctionTemplateDecl>(ND->getUnderlyingDecl()))
           continue;
-        const CXXConstructorDecl *Constructor = cast<CXXConstructorDecl>(ND);
+        // UsingDecl itself is not a constructor
+        if (isa<UsingDecl>(ND))
+          continue;
+        auto *Constructor = cast<CXXConstructorDecl>(ND->getUnderlyingDecl());
         if (Constructor->isCopyConstructor(FoundTQs)) {
           FoundConstructor = true;
           const FunctionProtoType *CPT
@@ -4257,9 +4260,12 @@ static bool EvaluateUnaryTypeTrait(Sema &Self, TypeTrait UTT,
       bool FoundConstructor = false;
       for (const auto *ND : Self.LookupConstructors(RD)) {
         // FIXME: In C++0x, a constructor template can be a default constructor.
-        if (isa<FunctionTemplateDecl>(ND))
+        if (isa<FunctionTemplateDecl>(ND->getUnderlyingDecl()))
+          continue;
+        // UsingDecl itself is not a constructor
+        if (isa<UsingDecl>(ND))
           continue;
-        const CXXConstructorDecl *Constructor = cast<CXXConstructorDecl>(ND);
+        auto *Constructor = cast<CXXConstructorDecl>(ND->getUnderlyingDecl());
         if (Constructor->isDefaultConstructor()) {
           FoundConstructor = true;
           const FunctionProtoType *CPT
@@ -6582,10 +6588,16 @@ static inline bool VariableCanNeverBeAConstantExpression(VarDecl *Var,
 static void CheckIfAnyEnclosingLambdasMustCaptureAnyPotentialCaptures(
     Expr *const FE, LambdaScopeInfo *const CurrentLSI, Sema &S) {
 
-  assert(!S.isUnevaluatedContext());  
-  assert(S.CurContext->isDependentContext()); 
-  assert(CurrentLSI->CallOperator == S.CurContext && 
+  assert(!S.isUnevaluatedContext());
+  assert(S.CurContext->isDependentContext());
+#ifndef NDEBUG
+  DeclContext *DC = S.CurContext;
+  while (DC && isa<CapturedDecl>(DC))
+    DC = DC->getParent();
+  assert(
+      CurrentLSI->CallOperator == DC &&
       "The current call operator must be synchronized with Sema's CurContext");
+#endif // NDEBUG
 
   const bool IsFullExprInstantiationDependent = FE->isInstantiationDependent();
 
@@ -7051,7 +7063,8 @@ ExprResult Sema::ActOnFinishFullExpr(Expr *FE, SourceLocation CC,
   // and then the full-expression +n + ({ 0; }); ends, but it's too late 
   // for us to see that we need to capture n after all.
 
-  LambdaScopeInfo *const CurrentLSI = getCurLambda();
+  LambdaScopeInfo *const CurrentLSI =
+      getCurLambda(/*IgnoreCapturedRegions=*/true);
   // FIXME: PR 17877 showed that getCurLambda() can return a valid pointer 
   // even if CurContext is not a lambda call operator. Refer to that Bug Report
   // for an example of the code that might cause this asynchrony.  
@@ -7066,7 +7079,10 @@ ExprResult Sema::ActOnFinishFullExpr(Expr *FE, SourceLocation CC,
   //     constructor/destructor.
   //  - Teach the handful of places that iterate over FunctionScopes to 
   //    stop at the outermost enclosing lexical scope."
-  const bool IsInLambdaDeclContext = isLambdaCallOperator(CurContext);
+  DeclContext *DC = CurContext;
+  while (DC && isa<CapturedDecl>(DC))
+    DC = DC->getParent();
+  const bool IsInLambdaDeclContext = isLambdaCallOperator(DC);
   if (IsInLambdaDeclContext && CurrentLSI &&
       CurrentLSI->hasPotentialCaptures() && !FullExpr.isInvalid())
     CheckIfAnyEnclosingLambdasMustCaptureAnyPotentialCaptures(FE, CurrentLSI,
diff --git a/lib/Sema/SemaLambda.cpp b/lib/Sema/SemaLambda.cpp
index 8a2bf929dfc0..0b3af262cd61 100644
--- a/lib/Sema/SemaLambda.cpp
+++ b/lib/Sema/SemaLambda.cpp
@@ -66,17 +66,20 @@ getStackIndexOfNearestEnclosingCaptureReadyLambda(
   // Label failure to capture.
   const Optional<unsigned> NoLambdaIsCaptureReady;
 
+  // Ignore all inner captured regions.
+  unsigned CurScopeIndex = FunctionScopes.size() - 1;
+  while (CurScopeIndex > 0 && isa<clang::sema::CapturedRegionScopeInfo>(
+                                  FunctionScopes[CurScopeIndex]))
+    --CurScopeIndex;
   assert(
-      isa<clang::sema::LambdaScopeInfo>(
-          FunctionScopes[FunctionScopes.size() - 1]) &&
+      isa<clang::sema::LambdaScopeInfo>(FunctionScopes[CurScopeIndex]) &&
       "The function on the top of sema's function-info stack must be a lambda");
-  
+
   // If VarToCapture is null, we are attempting to capture 'this'.
   const bool IsCapturingThis = !VarToCapture;
   const bool IsCapturingVariable = !IsCapturingThis;
 
   // Start with the current lambda at the top of the stack (highest index).
-  unsigned CurScopeIndex = FunctionScopes.size() - 1;
   DeclContext *EnclosingDC =
       cast<sema::LambdaScopeInfo>(FunctionScopes[CurScopeIndex])->CallOperator;
 
@@ -311,18 +314,21 @@ Sema::getCurrentMangleNumberContext(const DeclContext *DC,
   bool IsInNonspecializedTemplate =
     !ActiveTemplateInstantiations.empty() || CurContext->isDependentContext();
   switch (Kind) {
-  case Normal:
+  case Normal: {
     //  -- the bodies of non-exported nonspecialized template functions
     //  -- the bodies of inline functions
     if ((IsInNonspecializedTemplate &&
          !(ManglingContextDecl && isa<ParmVarDecl>(ManglingContextDecl))) ||
         isInInlineFunction(CurContext)) {
       ManglingContextDecl = nullptr;
+      while (auto *CD = dyn_cast<CapturedDecl>(DC))
+        DC = CD->getParent();
       return &Context.getManglingNumberContext(DC);
     }
 
     ManglingContextDecl = nullptr;
     return nullptr;
+  }
 
   case StaticDataMember:
     //  -- the initializers of nonspecialized static members of template classes
diff --git a/lib/Sema/SemaOpenMP.cpp b/lib/Sema/SemaOpenMP.cpp
index 3c8554893b43..b7ac48583e1a 100644
--- a/lib/Sema/SemaOpenMP.cpp
+++ b/lib/Sema/SemaOpenMP.cpp
@@ -9133,7 +9133,7 @@ OMPClause *Sema::ActOnOpenMPReductionClause(
     //  for all threads of the team.
     if (!ASE && !OASE && VD) {
       VarDecl *VDDef = VD->getDefinition();
-      if (VD->getType()->isReferenceType() && VDDef) {
+      if (VD->getType()->isReferenceType() && VDDef && VDDef->hasInit()) {
         DSARefChecker Check(DSAStack);
         if (Check.Visit(VDDef->getInit())) {
           Diag(ELoc, diag::err_omp_reduction_ref_type_arg) << ERange;
@@ -10680,6 +10680,25 @@ static bool CheckMapConflicts(
           if (CI->getAssociatedDeclaration() != SI->getAssociatedDeclaration())
             break;
         }
+        // Check if the extra components of the expressions in the enclosing
+        // data environment are redundant for the current base declaration.
+        // If they are, the maps completely overlap, which is legal.
+        for (; SI != SE; ++SI) {
+          QualType Type;
+          if (auto *ASE =
+              dyn_cast<ArraySubscriptExpr>(SI->getAssociatedExpression())) {
+            Type = ASE->getBase()->IgnoreParenImpCasts()->getType();
+          } else if (auto *OASE =
+              dyn_cast<OMPArraySectionExpr>(SI->getAssociatedExpression())) {
+            auto *E = OASE->getBase()->IgnoreParenImpCasts();
+            Type =
+                OMPArraySectionExpr::getBaseOriginalType(E).getCanonicalType();
+          }
+          if (Type.isNull() || Type->isAnyPointerType() ||
+              CheckArrayExpressionDoesNotReferToWholeSize(
+                  SemaRef, SI->getAssociatedExpression(), Type))
+            break;
+        }
 
         // OpenMP 4.5 [2.15.5.1, map Clause, Restrictions, p.4]
         //  List items of map clauses in the same construct must not share
diff --git a/lib/Sema/SemaTemplate.cpp b/lib/Sema/SemaTemplate.cpp
index 7fc5db82d32c..72e499342f8f 100644
--- a/lib/Sema/SemaTemplate.cpp
+++ b/lib/Sema/SemaTemplate.cpp
@@ -3256,7 +3256,7 @@ SubstDefaultTemplateArgument(Sema &SemaRef,
   // on the previously-computed template arguments.
   if (ArgType->getType()->isDependentType()) {
     Sema::InstantiatingTemplate Inst(SemaRef, TemplateLoc,
-                                     Template, Converted,
+                                     Param, Template, Converted,
                                      SourceRange(TemplateLoc, RAngleLoc));
     if (Inst.isInvalid())
       return nullptr;
@@ -3308,7 +3308,7 @@ SubstDefaultTemplateArgument(Sema &SemaRef,
                              NonTypeTemplateParmDecl *Param,
                         SmallVectorImpl<TemplateArgument> &Converted) {
   Sema::InstantiatingTemplate Inst(SemaRef, TemplateLoc,
-                                   Template, Converted,
+                                   Param, Template, Converted,
                                    SourceRange(TemplateLoc, RAngleLoc));
   if (Inst.isInvalid())
     return ExprError();
@@ -3359,8 +3359,9 @@ SubstDefaultTemplateArgument(Sema &SemaRef,
                              TemplateTemplateParmDecl *Param,
                        SmallVectorImpl<TemplateArgument> &Converted,
                              NestedNameSpecifierLoc &QualifierLoc) {
-  Sema::InstantiatingTemplate Inst(SemaRef, TemplateLoc, Template, Converted,
-                                   SourceRange(TemplateLoc, RAngleLoc));
+  Sema::InstantiatingTemplate Inst(
+      SemaRef, TemplateLoc, TemplateParameter(Param), Template, Converted,
+      SourceRange(TemplateLoc, RAngleLoc));
   if (Inst.isInvalid())
     return TemplateName();
 
@@ -3981,7 +3982,9 @@ bool Sema::CheckTemplateArgumentList(TemplateDecl *Template,
     }
 
     // Introduce an instantiation record that describes where we are using
-    // the default template argument.
+    // the default template argument. We're not actually instantiating a
+    // template here, we just create this object to put a note into the
+    // context stack.
     InstantiatingTemplate Inst(*this, RAngleLoc, Template, *Param, Converted,
                                SourceRange(TemplateLoc, RAngleLoc));
     if (Inst.isInvalid())
diff --git a/lib/Sema/SemaTemplateInstantiate.cpp b/lib/Sema/SemaTemplateInstantiate.cpp
index 48c6a506ee36..65a5633bf0d5 100644
--- a/lib/Sema/SemaTemplateInstantiate.cpp
+++ b/lib/Sema/SemaTemplateInstantiate.cpp
@@ -225,6 +225,10 @@ Sema::InstantiatingTemplate::InstantiatingTemplate(
     Inst.NumTemplateArgs = TemplateArgs.size();
     Inst.DeductionInfo = DeductionInfo;
     Inst.InstantiationRange = InstantiationRange;
+    AlreadyInstantiating =
+        !SemaRef.InstantiatingSpecializations
+             .insert(std::make_pair(Inst.Entity->getCanonicalDecl(), Inst.Kind))
+             .second;
     SemaRef.InNonInstantiationSFINAEContext = false;
     SemaRef.ActiveTemplateInstantiations.push_back(Inst);
     if (!Inst.isInstantiationRecord())
@@ -247,13 +251,14 @@ Sema::InstantiatingTemplate::InstantiatingTemplate(
           PointOfInstantiation, InstantiationRange, Entity) {}
 
 Sema::InstantiatingTemplate::InstantiatingTemplate(
-    Sema &SemaRef, SourceLocation PointOfInstantiation, TemplateDecl *Template,
-    ArrayRef<TemplateArgument> TemplateArgs, SourceRange InstantiationRange)
+    Sema &SemaRef, SourceLocation PointOfInstantiation, TemplateParameter Param,
+    TemplateDecl *Template, ArrayRef<TemplateArgument> TemplateArgs,
+    SourceRange InstantiationRange)
     : InstantiatingTemplate(
           SemaRef,
           ActiveTemplateInstantiation::DefaultTemplateArgumentInstantiation,
-          PointOfInstantiation, InstantiationRange, Template, nullptr,
-          TemplateArgs) {}
+          PointOfInstantiation, InstantiationRange, getAsNamedDecl(Param),
+          Template, TemplateArgs) {}
 
 Sema::InstantiatingTemplate::InstantiatingTemplate(
     Sema &SemaRef, SourceLocation PointOfInstantiation,
@@ -263,7 +268,11 @@ Sema::InstantiatingTemplate::InstantiatingTemplate(
     sema::TemplateDeductionInfo &DeductionInfo, SourceRange InstantiationRange)
     : InstantiatingTemplate(SemaRef, Kind, PointOfInstantiation,
                             InstantiationRange, FunctionTemplate, nullptr,
-                            TemplateArgs, &DeductionInfo) {}
+                            TemplateArgs, &DeductionInfo) {
+  assert(
+    Kind == ActiveTemplateInstantiation::ExplicitTemplateArgumentSubstitution ||
+    Kind == ActiveTemplateInstantiation::DeducedTemplateArgumentSubstitution);
+}
 
 Sema::InstantiatingTemplate::InstantiatingTemplate(
     Sema &SemaRef, SourceLocation PointOfInstantiation,
@@ -327,7 +336,8 @@ Sema::InstantiatingTemplate::InstantiatingTemplate(
 
 void Sema::InstantiatingTemplate::Clear() {
   if (!Invalid) {
-    if (!SemaRef.ActiveTemplateInstantiations.back().isInstantiationRecord()) {
+    auto &Active = SemaRef.ActiveTemplateInstantiations.back();
+    if (!Active.isInstantiationRecord()) {
       assert(SemaRef.NonInstantiationEntries > 0);
       --SemaRef.NonInstantiationEntries;
     }
@@ -345,6 +355,10 @@ void Sema::InstantiatingTemplate::Clear() {
       SemaRef.ActiveTemplateInstantiationLookupModules.pop_back();
     }
 
+    if (!AlreadyInstantiating)
+      SemaRef.InstantiatingSpecializations.erase(
+          std::make_pair(Active.Entity, Active.Kind));
+
     SemaRef.ActiveTemplateInstantiations.pop_back();
     Invalid = true;
   }
@@ -443,7 +457,7 @@ void Sema::PrintInstantiationStack() {
     }
 
     case ActiveTemplateInstantiation::DefaultTemplateArgumentInstantiation: {
-      TemplateDecl *Template = cast<TemplateDecl>(Active->Entity);
+      TemplateDecl *Template = cast<TemplateDecl>(Active->Template);
       SmallVector<char, 128> TemplateArgsStr;
       llvm::raw_svector_ostream OS(TemplateArgsStr);
       Template->printName(OS);
@@ -1950,6 +1964,7 @@ Sema::InstantiateClass(SourceLocation PointOfInstantiation,
   InstantiatingTemplate Inst(*this, PointOfInstantiation, Instantiation);
   if (Inst.isInvalid())
     return true;
+  assert(!Inst.isAlreadyInstantiating() && "should have been caught by caller");
   PrettyDeclStackTraceEntry CrashInfo(*this, Instantiation, SourceLocation(),
                                       "instantiating class definition");
 
@@ -2175,6 +2190,8 @@ bool Sema::InstantiateEnum(SourceLocation PointOfInstantiation,
   InstantiatingTemplate Inst(*this, PointOfInstantiation, Instantiation);
   if (Inst.isInvalid())
     return true;
+  if (Inst.isAlreadyInstantiating())
+    return false;
   PrettyDeclStackTraceEntry CrashInfo(*this, Instantiation, SourceLocation(),
                                       "instantiating enum definition");
 
@@ -2249,6 +2266,12 @@ bool Sema::InstantiateInClassInitializer(
   InstantiatingTemplate Inst(*this, PointOfInstantiation, Instantiation);
   if (Inst.isInvalid())
     return true;
+  if (Inst.isAlreadyInstantiating()) {
+    // Error out if we hit an instantiation cycle for this initializer.
+    Diag(PointOfInstantiation, diag::err_in_class_initializer_cycle)
+      << Instantiation;
+    return true;
+  }
   PrettyDeclStackTraceEntry CrashInfo(*this, Instantiation, SourceLocation(),
                                       "instantiating default member init");
 
diff --git a/lib/Sema/SemaTemplateInstantiateDecl.cpp b/lib/Sema/SemaTemplateInstantiateDecl.cpp
index 6a213953ec9a..dd3748fb5337 100644
--- a/lib/Sema/SemaTemplateInstantiateDecl.cpp
+++ b/lib/Sema/SemaTemplateInstantiateDecl.cpp
@@ -3360,6 +3360,13 @@ void Sema::InstantiateExceptionSpec(SourceLocation PointOfInstantiation,
     UpdateExceptionSpec(Decl, EST_None);
     return;
   }
+  if (Inst.isAlreadyInstantiating()) {
+    // This exception specification indirectly depends on itself. Reject.
+    // FIXME: Corresponding rule in the standard?
+    Diag(PointOfInstantiation, diag::err_exception_spec_cycle) << Decl;
+    UpdateExceptionSpec(Decl, EST_None);
+    return;
+  }
 
   // Enter the scope of this instantiation. We don't use
   // PushDeclContext because we don't have a scope.
@@ -3619,7 +3626,7 @@ void Sema::InstantiateFunctionDefinition(SourceLocation PointOfInstantiation,
   }
 
   InstantiatingTemplate Inst(*this, PointOfInstantiation, Function);
-  if (Inst.isInvalid())
+  if (Inst.isInvalid() || Inst.isAlreadyInstantiating())
     return;
   PrettyDeclStackTraceEntry CrashInfo(*this, Function, SourceLocation(),
                                       "instantiating function definition");
@@ -3882,10 +3889,6 @@ void Sema::InstantiateVariableInitializer(
   else if (OldVar->isInline())
     Var->setImplicitlyInline();
 
-  if (Var->getAnyInitializer())
-    // We already have an initializer in the class.
-    return;
-
   if (OldVar->getInit()) {
     if (Var->isStaticDataMember() && !OldVar->isOutOfLine())
       PushExpressionEvaluationContext(Sema::ConstantEvaluated, OldVar);
@@ -3921,9 +3924,23 @@ void Sema::InstantiateVariableInitializer(
     }
 
     PopExpressionEvaluationContext();
-  } else if ((!Var->isStaticDataMember() || Var->isOutOfLine()) &&
-             !Var->isCXXForRangeDecl())
+  } else {
+    if (Var->isStaticDataMember()) {
+      if (!Var->isOutOfLine())
+        return;
+
+      // If the declaration inside the class had an initializer, don't add
+      // another one to the out-of-line definition.
+      if (OldVar->getFirstDecl()->hasInit())
+        return;
+    }
+
+    // We'll add an initializer to a for-range declaration later.
+    if (Var->isCXXForRangeDecl())
+      return;
+
     ActOnUninitializedDecl(Var, false);
+  }
 }
 
 /// \brief Instantiate the definition of the given variable from its
@@ -4013,7 +4030,7 @@ void Sema::InstantiateVariableDefinition(SourceLocation PointOfInstantiation,
       // FIXME: Factor out the duplicated instantiation context setup/tear down
       // code here.
       InstantiatingTemplate Inst(*this, PointOfInstantiation, Var);
-      if (Inst.isInvalid())
+      if (Inst.isInvalid() || Inst.isAlreadyInstantiating())
         return;
       PrettyDeclStackTraceEntry CrashInfo(*this, Var, SourceLocation(),
                                           "instantiating variable initializer");
@@ -4142,7 +4159,7 @@ void Sema::InstantiateVariableDefinition(SourceLocation PointOfInstantiation,
   }
 
   InstantiatingTemplate Inst(*this, PointOfInstantiation, Var);
-  if (Inst.isInvalid())
+  if (Inst.isInvalid() || Inst.isAlreadyInstantiating())
     return;
   PrettyDeclStackTraceEntry CrashInfo(*this, Var, SourceLocation(),
                                       "instantiating variable definition");
diff --git a/lib/Serialization/ASTReaderDecl.cpp b/lib/Serialization/ASTReaderDecl.cpp
index d38a701c04b5..35da8f3ebcfe 100644
--- a/lib/Serialization/ASTReaderDecl.cpp
+++ b/lib/Serialization/ASTReaderDecl.cpp
@@ -2220,7 +2220,7 @@ void ASTDeclReader::VisitStaticAssertDecl(StaticAssertDecl *D) {
   VisitDecl(D);
   D->AssertExprAndFailed.setPointer(Reader.ReadExpr(F));
   D->AssertExprAndFailed.setInt(Record[Idx++]);
-  D->Message = cast<StringLiteral>(Reader.ReadExpr(F));
+  D->Message = cast_or_null<StringLiteral>(Reader.ReadExpr(F));
   D->RParenLoc = ReadSourceLocation(Record, Idx);
 }
 
diff --git a/test/CodeGen/builtins-mips-msa-error.c b/test/CodeGen/builtins-mips-msa-error.c
new file mode 100644
index 000000000000..fcdf6f0c48c8
--- /dev/null
+++ b/test/CodeGen/builtins-mips-msa-error.c
@@ -0,0 +1,421 @@
+// REQUIRES: mips-registered-target
+// RUN: not %clang_cc1 -triple mips-unknown-linux-gnu -fsyntax-only %s \
+// RUN:            -target-feature +msa -target-feature +fp64 \
+// RUN:            -mfloat-abi hard -o - 2>&1 | FileCheck %s
+
+#include <msa.h>
+
+void test(void) {
+  v16i8 v16i8_a = (v16i8) {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16i8 v16i8_r;
+  v8i16 v8i16_a = (v8i16) {0, 1, 2, 3, 4, 5, 6, 7};
+  v8i16 v8i16_r;
+  v4i32 v4i32_a = (v4i32) {0, 1, 2, 3};
+  v4i32 v4i32_r;
+  v2i64 v2i64_a = (v2i64) {0, 1};
+  v2i64 v2i64_r;
+
+  v16u8 v16u8_a = (v16u8) {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
+  v16u8 v16u8_r;
+  v8u16 v8u16_a = (v8u16) {0, 1, 2, 3, 4, 5, 6, 7};
+  v8u16 v8u16_r;
+  v4u32 v4u32_a = (v4u32) {0, 1, 2, 3};
+  v4u32 v4u32_r;
+  v2u64 v2u64_a = (v2u64) {0, 1};
+  v2u64 v2u64_r;
+
+
+  int int_r;
+  long long ll_r;
+
+  v16u8_r = __msa_addvi_b(v16u8_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_addvi_h(v8u16_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_addvi_w(v4u32_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_addvi_d(v2u64_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_andi_b(v16i8_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v8i16_r = __msa_andi_b(v8i16_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v4i32_r = __msa_andi_b(v4i32_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v2i64_r = __msa_andi_b(v2i64_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bclri_b(v16i8_a, 8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_bclri_h(v8i16_a, 16);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_bclri_w(v4i32_a, 33);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_bclri_d(v2i64_a, 64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_binsli_b(v16i8_r, v16i8_a, 8);     // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_binsli_h(v8i16_r, v8i16_a, 16);    // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_binsli_w(v4i32_r, v4i32_a, 32);    // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_binsli_d(v2i64_r, v2i64_a, 64);    // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_binsri_b(v16i8_r, v16i8_a, 8);     // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_binsri_h(v8i16_r, v8i16_a, 16);    // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_binsri_w(v4i32_r, v4i32_a, 32);    // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_binsri_d(v2i64_r, v2i64_a, 64);    // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_bmnzi_b(v16i8_r, v16i8_a, 256);    // expected-error {{argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bmzi_b(v16i8_r, v16i8_a, 256);     // expected-error {{argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bnegi_b(v16i8_a, 8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_bnegi_h(v8i16_a, 16);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_bnegi_w(v4i32_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_bnegi_d(v2i64_a, 64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_bseli_b(v16i8_r, v16i8_a, 256);    // expected-error {{argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bseti_b(v16i8_a, 8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_bseti_h(v8i16_a, 16);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_bseti_w(v4i32_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_bseti_d(v2i64_a, 64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_ceqi_b(v16i8_a, 16);               // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_ceqi_h(v8i16_a, 16);               // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_ceqi_w(v4i32_a, 16);               // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_ceqi_d(v2i64_a, 16);               // expected-error {{argument should be a value from -16 to 15}}
+
+  v16i8_r = __msa_clei_s_b(v16i8_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_clei_s_h(v8i16_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_clei_s_w(v4i32_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_clei_s_d(v2i64_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_clei_u_b(v16u8_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_clei_u_h(v8u16_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_clei_u_w(v4u32_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_clei_u_d(v2u64_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_clti_s_b(v16i8_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_clti_s_h(v8i16_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_clti_s_w(v4i32_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_clti_s_d(v2i64_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_clti_u_b(v16u8_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_clti_u_h(v8u16_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_clti_u_w(v4u32_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_clti_u_d(v2u64_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+
+  int_r = __msa_copy_s_b(v16i8_a, 16);               // expected-error {{argument should be a value from 0 to 15}}
+  int_r = __msa_copy_s_h(v8i16_a, 8);                // expected-error {{argument should be a value from 0 to 7}}
+  int_r = __msa_copy_s_w(v4i32_a, 4);                // expected-error {{argument should be a value from 0 to 3}}
+  ll_r  = __msa_copy_s_d(v2i64_a, 2);                // expected-error {{argument should be a value from 0 to 1}}
+
+  int_r = __msa_copy_u_b(v16u8_a, 16);               // expected-error {{argument should be a value from 0 to 15}}
+  int_r = __msa_copy_u_h(v8u16_a, 8);                // expected-error {{argument should be a value from 0 to 7}}
+  int_r = __msa_copy_u_w(v4u32_a, 4);                // expected-error {{argument should be a value from 0 to 3}}
+  ll_r  = __msa_copy_u_d(v2i64_a, 2);                // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_insve_b(v16i8_r, 16, v16i8_a);     // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_insve_h(v8i16_r, 8, v8i16_a);      // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_insve_w(v4i32_r, 4, v4i32_a);      // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_insve_d(v2i64_r, 2, v2i64_a);      // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_ld_b(&v16i8_a, 23);               // expected-error {{argument should be a multiple of 16}}
+  v8i16_r = __msa_ld_h(&v8i16_a, 77);               // expected-error {{argument should be a multiple of 16}}
+  v4i32_r = __msa_ld_w(&v4i32_a, 14);               // expected-error {{argument should be a multiple of 16}}
+  v2i64_r = __msa_ld_d(&v2i64_a, 23);               // expected-error {{argument should be a multiple of 16}}
+
+  v16i8_r = __msa_ld_b(&v16i8_a, 512);               // expected-error {{argument should be a value from -512 to 511}}
+  v8i16_r = __msa_ld_h(&v8i16_a, 512);               // expected-error {{argument should be a value from -512 to 511}}
+  v4i32_r = __msa_ld_w(&v4i32_a, 512);               // expected-error {{argument should be a value from -512 to 511}}
+  v2i64_r = __msa_ld_d(&v2i64_a, 512);               // expected-error {{argument should be a value from -512 to 511}}
+
+  v16i8_r = __msa_ldi_b(512);                        // expected-error {{argument should be a value from -512 to 511}}
+  v8i16_r = __msa_ldi_h(512);                        // expected-error {{argument should be a value from -512 to 511}}
+  v4i32_r = __msa_ldi_w(512);                        // expected-error {{argument should be a value from -512 to 511}}
+  v2i64_r = __msa_ldi_d(512);                        // expected-error {{argument should be a value from -512 to 511}}
+
+  v16i8_r = __msa_maxi_s_b(v16i8_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_maxi_s_h(v8i16_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_maxi_s_w(v4i32_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_maxi_s_d(v2i64_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_maxi_u_b(v16u8_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_maxi_u_h(v8u16_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_maxi_u_w(v4u32_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_maxi_u_d(v2u64_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_mini_s_b(v16i8_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_mini_s_h(v8i16_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_mini_s_w(v4i32_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_mini_s_d(v2i64_a, 16);             // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_mini_u_b(v16u8_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_mini_u_h(v8u16_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_mini_u_w(v4u32_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_mini_u_d(v2u64_a, 32);             // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_nori_b(v16i8_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_ori_b(v16i8_a, 256);               // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_sat_s_b(v16i8_a, 8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_sat_s_h(v8i16_a, 16);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_sat_s_w(v4i32_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_sat_s_d(v2i64_a, 64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_sat_u_b(v16i8_a, 8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_sat_u_h(v8i16_a, 16);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_sat_u_w(v4i32_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_sat_u_d(v2i64_a, 64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_shf_b(v16i8_a, 256);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v8i16_r = __msa_shf_h(v8i16_a, 256);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v4i32_r = __msa_shf_w(v4i32_a, 256);               // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_sld_b(v16i8_r, v16i8_a, 16);      // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_sld_h(v8i16_r, v8i16_a, 8);       // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_sld_w(v4i32_r, v4i32_a, 4);       // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_sld_d(v2i64_r, v2i64_a, 2);       // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_sldi_b(v16i8_r, v16i8_a, 16);      // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_sldi_h(v8i16_r, v8i16_a, 8);       // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_sldi_w(v4i32_r, v4i32_a, 4);       // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_sldi_d(v2i64_r, v2i64_a, 2);       // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_slli_b(v16i8_a, 8);                // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_slli_h(v8i16_a, 16);               // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_slli_w(v4i32_a, 32);               // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_slli_d(v2i64_a, 64);               // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_splati_b(v16i8_a, 16);             // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_splati_h(v8i16_a, 8);              // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_splati_w(v4i32_a, 4);              // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_splati_d(v2i64_a, 2);              // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_srai_b(v16i8_a, 8);                // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srai_h(v8i16_a, 16);               // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srai_w(v4i32_a, 32);               // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srai_d(v2i64_a, 64);               // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_srari_b(v16i8_a, 8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srari_h(v8i16_a, 16);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srari_w(v4i32_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srari_d(v2i64_a, 64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_srli_b(v16i8_a, 8);                // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srli_h(v8i16_a, 16);               // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srli_w(v4i32_a, 32);               // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srli_d(v2i64_a, 64);               // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_srlri_b(v16i8_a, 8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srlri_h(v8i16_a, 16);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srlri_w(v4i32_a, 32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srlri_d(v2i64_a, 64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  __msa_st_b(v16i8_b, &v16i8_a, 52);                // expected-error {{argument should be a multiple of 16}}
+  __msa_st_h(v8i16_b, &v8i16_a, 51);                // expected-error {{argument should be a multiple of 16}}
+  __msa_st_w(v4i32_b, &v4i32_a, 51);                // expected-error {{argument should be a multiple of 16}}
+  __msa_st_d(v2i64_b, &v2i64_a, 12);                // expected-error {{argument should be a multiple of 16}}
+
+  __msa_st_b(v16i8_b, &v16i8_a, 512);                // expected-error {{argument should be a value from -512 to 511}}
+  __msa_st_h(v8i16_b, &v8i16_a, 512);                // expected-error {{argument should be a value from -512 to 511}}
+  __msa_st_w(v4i32_b, &v4i32_a, 512);                // expected-error {{argument should be a value from -512 to 511}}
+  __msa_st_d(v2i64_b, &v2i64_a, 512);                // expected-error {{argument should be a value from -512 to 511}}
+
+  v16i8_r = __msa_subvi_b(v16i8_a, 256);             // expected-error {{argument should be a value from 0 to 31}}
+  v8i16_r = __msa_subvi_h(v8i16_a, 256);             // expected-error {{argument should be a value from 0 to 31}}
+  v4i32_r = __msa_subvi_w(v4i32_a, 256);             // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_subvi_d(v2i64_a, 256);             // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_xori_b(v16i8_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v8i16_r = __msa_xori_b(v8i16_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v4i32_r = __msa_xori_b(v4i32_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v2i64_r = __msa_xori_b(v2i64_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16u8_r = __msa_xori_b(v16u8_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v8u16_r = __msa_xori_b(v8u16_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v4u32_r = __msa_xori_b(v4u32_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+  v2u64_r = __msa_xori_b(v2u64_a, 256);              // CHECK: warning: argument should be a value from 0 to 255}}
+
+  // Test the lower bounds
+
+  v16u8_r = __msa_addvi_b(v16u8_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_addvi_h(v8u16_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_addvi_w(v4u32_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_addvi_d(v2u64_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_andi_b(v16i8_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v8i16_r = __msa_andi_b(v8i16_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v4i32_r = __msa_andi_b(v4i32_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v2i64_r = __msa_andi_b(v2i64_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bclri_b(v16i8_a, -1);              // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_bclri_h(v8i16_a, -1);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_bclri_w(v4i32_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_bclri_d(v2i64_a, -1);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_binsli_b(v16i8_r, v16i8_a, -8);    // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_binsli_h(v8i16_r, v8i16_a, -1);    // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_binsli_w(v4i32_r, v4i32_a, -1);    // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_binsli_d(v2i64_r, v2i64_a, -1);    // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_binsri_b(v16i8_r, v16i8_a, -1);    // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_binsri_h(v8i16_r, v8i16_a, -1);    // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_binsri_w(v4i32_r, v4i32_a, -1);    // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_binsri_d(v2i64_r, v2i64_a, -1);    // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_bmnzi_b(v16i8_r, v16i8_a, -1);     // expected-error {{argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bmzi_b(v16i8_r, v16i8_a, -1);      // expected-error {{argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bnegi_b(v16i8_a, -1);              // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_bnegi_h(v8i16_a, -1);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_bnegi_w(v4i32_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_bnegi_d(v2i64_a, -1);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_bseli_b(v16i8_r, v16i8_a, -1);     // expected-error {{argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_bseti_b(v16i8_a, -8);              // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_bseti_h(v8i16_a, -1);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_bseti_w(v4i32_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_bseti_d(v2i64_a, -1);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_ceqi_b(v16i8_a, -17);              // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_ceqi_h(v8i16_a, -17);              // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_ceqi_w(v4i32_a, -17);              // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_ceqi_d(v2i64_a, -17);              // expected-error {{argument should be a value from -16 to 15}}
+
+  v16i8_r = __msa_clei_s_b(v16i8_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_clei_s_h(v8i16_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_clei_s_w(v4i32_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_clei_s_d(v2i64_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_clei_u_b(v16u8_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_clei_u_h(v8u16_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_clei_u_w(v4u32_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_clei_u_d(v2u64_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_clti_s_b(v16i8_a, -17);             // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_clti_s_h(v8i16_a, -17);             // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_clti_s_w(v4i32_a, -17);             // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_clti_s_d(v2i64_a, -17);             // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_clti_u_b(v16u8_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_clti_u_h(v8u16_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_clti_u_w(v4u32_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_clti_u_d(v2u64_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+
+  int_r = __msa_copy_s_b(v16i8_a, -1);               // expected-error {{argument should be a value from 0 to 15}}
+  int_r = __msa_copy_s_h(v8i16_a, -1);               // expected-error {{argument should be a value from 0 to 7}}
+  int_r = __msa_copy_s_w(v4i32_a, -1);               // expected-error {{argument should be a value from 0 to 3}}
+  ll_r  = __msa_copy_s_d(v2i64_a, -1);               // expected-error {{argument should be a value from 0 to 1}}
+
+  int_r = __msa_copy_u_b(v16u8_a, -17);              // expected-error {{argument should be a value from 0 to 15}}
+  int_r = __msa_copy_u_h(v8u16_a, -8);               // expected-error {{argument should be a value from 0 to 7}}
+  int_r = __msa_copy_u_w(v4u32_a, -4);               // expected-error {{argument should be a value from 0 to 3}}
+  ll_r  = __msa_copy_u_d(v2i64_a, -2);               // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_insve_b(v16i8_r, 16, v16i8_a);     // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_insve_h(v8i16_r, 8, v8i16_a);      // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_insve_w(v4i32_r, 4, v4i32_a);      // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_insve_d(v2i64_r, 2, v2i64_a);      // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_ld_b(&v16i8_a, -513);              // expected-error {{argument should be a value from -512 to 511}}
+  v8i16_r = __msa_ld_h(&v8i16_a, -513);              // expected-error {{argument should be a value from -512 to 511}}
+  v4i32_r = __msa_ld_w(&v4i32_a, -513);              // expected-error {{argument should be a value from -512 to 511}}
+  v2i64_r = __msa_ld_d(&v2i64_a, -513);              // expected-error {{argument should be a value from -512 to 511}}
+
+  v16i8_r = __msa_ldi_b(-513);                       // expected-error {{argument should be a value from -512 to 511}}
+  v8i16_r = __msa_ldi_h(-513);                       // expected-error {{argument should be a value from -512 to 511}}
+  v4i32_r = __msa_ldi_w(-513);                       // expected-error {{argument should be a value from -512 to 511}}
+  v2i64_r = __msa_ldi_d(-513);                       // expected-error {{argument should be a value from -512 to 511}}
+
+  v16i8_r = __msa_maxi_s_b(v16i8_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_maxi_s_h(v8i16_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_maxi_s_w(v4i32_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_maxi_s_d(v2i64_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_maxi_u_b(v16u8_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_maxi_u_h(v8u16_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_maxi_u_w(v4u32_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_maxi_u_d(v2u64_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_mini_s_b(v16i8_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v8i16_r = __msa_mini_s_h(v8i16_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v4i32_r = __msa_mini_s_w(v4i32_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+  v2i64_r = __msa_mini_s_d(v2i64_a, -17);            // expected-error {{argument should be a value from -16 to 15}}
+
+  v16u8_r = __msa_mini_u_b(v16u8_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v8u16_r = __msa_mini_u_h(v8u16_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v4u32_r = __msa_mini_u_w(v4u32_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+  v2u64_r = __msa_mini_u_d(v2u64_a, -1);             // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_nori_b(v16i8_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_ori_b(v16i8_a, -1);                // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_sat_s_b(v16i8_a, -1);              // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_sat_s_h(v8i16_a, -1);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_sat_s_w(v4i32_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_sat_s_d(v2i64_a, -1);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_sat_u_b(v16i8_a, -8);              // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_sat_u_h(v8i16_a, -17);             // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_sat_u_w(v4i32_a, -32);             // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_sat_u_d(v2i64_a, -64);             // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_shf_b(v16i8_a, -1);                // CHECK: warning: argument should be a value from 0 to 255}}
+  v8i16_r = __msa_shf_h(v8i16_a, -1);                // CHECK: warning: argument should be a value from 0 to 255}}
+  v4i32_r = __msa_shf_w(v4i32_a, -1);                // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16i8_r = __msa_sld_b(v16i8_r, v16i8_a, -17);      // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_sld_h(v8i16_r, v8i16_a, -8);       // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_sld_w(v4i32_r, v4i32_a, -4);       // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_sld_d(v2i64_r, v2i64_a, -2);       // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_sldi_b(v16i8_r, v16i8_a, -17);     // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_sldi_h(v8i16_r, v8i16_a, -8);      // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_sldi_w(v4i32_r, v4i32_a, -4);      // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_sldi_d(v2i64_r, v2i64_a, -2);      // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_slli_b(v16i8_a, -8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_slli_h(v8i16_a, -17);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_slli_w(v4i32_a, -32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_slli_d(v2i64_a, -64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_splati_b(v16i8_a, -17);            // expected-error {{argument should be a value from 0 to 15}}
+  v8i16_r = __msa_splati_h(v8i16_a, -8);             // expected-error {{argument should be a value from 0 to 7}}
+  v4i32_r = __msa_splati_w(v4i32_a, -4);             // expected-error {{argument should be a value from 0 to 3}}
+  v2i64_r = __msa_splati_d(v2i64_a, -2);             // expected-error {{argument should be a value from 0 to 1}}
+
+  v16i8_r = __msa_srai_b(v16i8_a, -8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srai_h(v8i16_a, -17);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srai_w(v4i32_a, -32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srai_d(v2i64_a, -64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_srari_b(v16i8_a, -8);              // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srari_h(v8i16_a, -17);             // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srari_w(v4i32_a, -32);             // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srari_d(v2i64_a, -64);             // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_srli_b(v16i8_a, -8);               // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srli_h(v8i16_a, -17);              // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srli_w(v4i32_a, -32);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srli_d(v2i64_a, -64);              // expected-error {{argument should be a value from 0 to 63}}
+
+  v16i8_r = __msa_srlri_b(v16i8_a, -8);              // expected-error {{argument should be a value from 0 to 7}}
+  v8i16_r = __msa_srlri_h(v8i16_a, -17);             // expected-error {{argument should be a value from 0 to 15}}
+  v4i32_r = __msa_srlri_w(v4i32_a, -32);             // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_srlri_d(v2i64_a, -64);             // expected-error {{argument should be a value from 0 to 63}}
+
+  __msa_st_b(v16i8_b, &v16i8_a, -513);               // expected-error {{argument should be a value from -512 to 511}}
+  __msa_st_h(v8i16_b, &v8i16_a, -513);               // expected-error {{argument should be a value from -512 to 511}}
+  __msa_st_w(v4i32_b, &v4i32_a, -513);               // expected-error {{argument should be a value from -512 to 511}}
+  __msa_st_d(v2i64_b, &v2i64_a, -513);               // expected-error {{argument should be a value from -512 to 511}}
+
+  v16i8_r = __msa_subvi_b(v16i8_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v8i16_r = __msa_subvi_h(v8i16_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v4i32_r = __msa_subvi_w(v4i32_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+  v2i64_r = __msa_subvi_d(v2i64_a, -1);              // expected-error {{argument should be a value from 0 to 31}}
+
+  v16i8_r = __msa_xori_b(v16i8_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v8i16_r = __msa_xori_b(v8i16_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v4i32_r = __msa_xori_b(v4i32_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v2i64_r = __msa_xori_b(v2i64_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+
+  v16u8_r = __msa_xori_b(v16u8_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v8u16_r = __msa_xori_b(v8u16_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v4u32_r = __msa_xori_b(v4u32_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+  v2u64_r = __msa_xori_b(v2u64_a, -1);               // CHECK: warning: argument should be a value from 0 to 255}}
+
+}
diff --git a/test/CodeGen/builtins-mips-msa.c b/test/CodeGen/builtins-mips-msa.c
index 38aea04d9c36..125679545601 100644
--- a/test/CodeGen/builtins-mips-msa.c
+++ b/test/CodeGen/builtins-mips-msa.c
@@ -1,18 +1,11 @@
 // REQUIRES: mips-registered-target
-// RUN: %clang_cc1 -triple mips-unknown-linux-gnu -emit-llvm %s -o - \
-// RUN:   | FileCheck %s
-
-typedef signed char      v16i8 __attribute__ ((vector_size(16)));
-typedef signed short     v8i16 __attribute__ ((vector_size(16)));
-typedef signed int       v4i32 __attribute__ ((vector_size(16)));
-typedef signed long long v2i64 __attribute__ ((vector_size(16)));
-typedef unsigned char      v16u8 __attribute__ ((vector_size(16)));
-typedef unsigned short     v8u16 __attribute__ ((vector_size(16)));
-typedef unsigned int       v4u32 __attribute__ ((vector_size(16)));
-typedef unsigned long long v2u64 __attribute__ ((vector_size(16)));
+// RUN: %clang_cc1 -triple mips-unknown-linux-gnu -emit-llvm %s \
+// RUN:            -target-feature +msa -target-feature +fp64 \
+// RUN:            -mfloat-abi hard -o - | FileCheck %s
+
+#include <msa.h>
+
 typedef __fp16 v8f16 __attribute__ ((vector_size(16)));
-typedef float  v4f32 __attribute__ ((vector_size(16)));
-typedef double v2f64 __attribute__ ((vector_size(16)));
 
 void test(void) {
   v16i8 v16i8_a = (v16i8) {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15};
@@ -55,775 +48,775 @@ void test(void) {
   long long ll_r;
   int int_a = 0;
 
-  v16i8_r = __builtin_msa_add_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.add.a.b(
-  v8i16_r = __builtin_msa_add_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.add.a.h(
-  v4i32_r = __builtin_msa_add_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.add.a.w(
-  v2i64_r = __builtin_msa_add_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.add.a.d(
-
-  v16i8_r = __builtin_msa_adds_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.adds.a.b(
-  v8i16_r = __builtin_msa_adds_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.adds.a.h(
-  v4i32_r = __builtin_msa_adds_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.adds.a.w(
-  v2i64_r = __builtin_msa_adds_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.adds.a.d(
-
-  v16i8_r = __builtin_msa_adds_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.adds.s.b(
-  v8i16_r = __builtin_msa_adds_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.adds.s.h(
-  v4i32_r = __builtin_msa_adds_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.adds.s.w(
-  v2i64_r = __builtin_msa_adds_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.adds.s.d(
-
-  v16u8_r = __builtin_msa_adds_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.adds.u.b(
-  v8u16_r = __builtin_msa_adds_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.adds.u.h(
-  v4u32_r = __builtin_msa_adds_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.adds.u.w(
-  v2u64_r = __builtin_msa_adds_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.adds.u.d(
-
-  v16i8_r = __builtin_msa_addv_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.addv.b(
-  v8i16_r = __builtin_msa_addv_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.addv.h(
-  v4i32_r = __builtin_msa_addv_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.addv.w(
-  v2i64_r = __builtin_msa_addv_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.addv.d(
-
-  v16u8_r = __builtin_msa_addv_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.addv.b(
-  v8u16_r = __builtin_msa_addv_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.addv.h(
-  v4u32_r = __builtin_msa_addv_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.addv.w(
-  v2u64_r = __builtin_msa_addv_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.addv.d(
-
-  v16i8_r = __builtin_msa_addvi_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.addvi.b(
-  v8i16_r = __builtin_msa_addvi_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.addvi.h(
-  v4i32_r = __builtin_msa_addvi_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.addvi.w(
-  v2i64_r = __builtin_msa_addvi_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.addvi.d(
-
-  v16u8_r = __builtin_msa_addvi_b(v16u8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.addvi.b(
-  v8u16_r = __builtin_msa_addvi_h(v8u16_a, 25); // CHECK: call <8  x i16> @llvm.mips.addvi.h(
-  v4u32_r = __builtin_msa_addvi_w(v4u32_a, 25); // CHECK: call <4  x i32> @llvm.mips.addvi.w(
-  v2u64_r = __builtin_msa_addvi_d(v2u64_a, 25); // CHECK: call <2  x i64> @llvm.mips.addvi.d(
-
-  v16i8_r = __builtin_msa_and_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
-  v8i16_r = __builtin_msa_and_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
-  v4i32_r = __builtin_msa_and_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
-  v2i64_r = __builtin_msa_and_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
-
-  v16i8_r = __builtin_msa_andi_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-  v8i16_r = __builtin_msa_andi_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-  v4i32_r = __builtin_msa_andi_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-  v2i64_r = __builtin_msa_andi_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-
-  v16u8_r = __builtin_msa_andi_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-  v8u16_r = __builtin_msa_andi_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-  v4u32_r = __builtin_msa_andi_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-  v2u64_r = __builtin_msa_andi_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
-
-  v16i8_r = __builtin_msa_asub_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.asub.s.b(
-  v8i16_r = __builtin_msa_asub_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.asub.s.h(
-  v4i32_r = __builtin_msa_asub_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.asub.s.w(
-  v2i64_r = __builtin_msa_asub_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.asub.s.d(
-
-  v16u8_r = __builtin_msa_asub_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.asub.u.b(
-  v8u16_r = __builtin_msa_asub_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.asub.u.h(
-  v4u32_r = __builtin_msa_asub_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.asub.u.w(
-  v2u64_r = __builtin_msa_asub_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.asub.u.d(
-
-  v16i8_r = __builtin_msa_ave_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ave.s.b(
-  v8i16_r = __builtin_msa_ave_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ave.s.h(
-  v4i32_r = __builtin_msa_ave_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ave.s.w(
-  v2i64_r = __builtin_msa_ave_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ave.s.d(
-
-  v16u8_r = __builtin_msa_ave_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.ave.u.b(
-  v8u16_r = __builtin_msa_ave_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.ave.u.h(
-  v4u32_r = __builtin_msa_ave_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.ave.u.w(
-  v2u64_r = __builtin_msa_ave_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.ave.u.d(
-
-  v16i8_r = __builtin_msa_aver_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.aver.s.b(
-  v8i16_r = __builtin_msa_aver_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.aver.s.h(
-  v4i32_r = __builtin_msa_aver_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.aver.s.w(
-  v2i64_r = __builtin_msa_aver_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.aver.s.d(
-
-  v16u8_r = __builtin_msa_aver_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.aver.u.b(
-  v8u16_r = __builtin_msa_aver_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.aver.u.h(
-  v4u32_r = __builtin_msa_aver_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.aver.u.w(
-  v2u64_r = __builtin_msa_aver_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.aver.u.d(
-
-  v16i8_r = __builtin_msa_bclr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.bclr.b(
-  v8i16_r = __builtin_msa_bclr_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.bclr.h(
-  v4i32_r = __builtin_msa_bclr_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.bclr.w(
-  v2i64_r = __builtin_msa_bclr_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.bclr.d(
-
-  v16i8_r = __builtin_msa_bclri_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bclri.b(
-  v8i16_r = __builtin_msa_bclri_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.bclri.h(
-  v4i32_r = __builtin_msa_bclri_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.bclri.w(
-  v2i64_r = __builtin_msa_bclri_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.bclri.d(
-
-  v16i8_r = __builtin_msa_binsl_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.binsl.b(
-  v8i16_r = __builtin_msa_binsl_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.binsl.h(
-  v4i32_r = __builtin_msa_binsl_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.binsl.w(
-  v2i64_r = __builtin_msa_binsl_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.binsl.d(
-
-  v16i8_r = __builtin_msa_binsli_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.binsli.b(
-  v8i16_r = __builtin_msa_binsli_h(v8i16_r, v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.binsli.h(
-  v4i32_r = __builtin_msa_binsli_w(v4i32_r, v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.binsli.w(
-  v2i64_r = __builtin_msa_binsli_d(v2i64_r, v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.binsli.d(
-
-  v16i8_r = __builtin_msa_binsr_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.binsr.b(
-  v8i16_r = __builtin_msa_binsr_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.binsr.h(
-  v4i32_r = __builtin_msa_binsr_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.binsr.w(
-  v2i64_r = __builtin_msa_binsr_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.binsr.d(
-
-  v16i8_r = __builtin_msa_binsri_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.binsri.b(
-  v8i16_r = __builtin_msa_binsri_h(v8i16_r, v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.binsri.h(
-  v4i32_r = __builtin_msa_binsri_w(v4i32_r, v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.binsri.w(
-  v2i64_r = __builtin_msa_binsri_d(v2i64_r, v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.binsri.d(
-
-  v16i8_r = __builtin_msa_bmnz_v(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
-  v8i16_r = __builtin_msa_bmnz_v(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
-  v4i32_r = __builtin_msa_bmnz_v(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
-  v2i64_r = __builtin_msa_bmnz_v(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
-
-  v16i8_r = __builtin_msa_bmnzi_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bmnzi.b(
-
-  v16i8_r = __builtin_msa_bmz_v(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
-  v8i16_r = __builtin_msa_bmz_v(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
-  v4i32_r = __builtin_msa_bmz_v(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
-  v2i64_r = __builtin_msa_bmz_v(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
-
-  v16i8_r = __builtin_msa_bmzi_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bmzi.b(
-
-  v16i8_r = __builtin_msa_bneg_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.bneg.b(
-  v8i16_r = __builtin_msa_bneg_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.bneg.h(
-  v4i32_r = __builtin_msa_bneg_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.bneg.w(
-  v2i64_r = __builtin_msa_bneg_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.bneg.d(
-
-  v16i8_r = __builtin_msa_bnegi_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bnegi.b(
-  v8i16_r = __builtin_msa_bnegi_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.bnegi.h(
-  v4i32_r = __builtin_msa_bnegi_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.bnegi.w(
-  v2i64_r = __builtin_msa_bnegi_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.bnegi.d(
-
-  int_r = __builtin_msa_bnz_b(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.b(
-  int_r = __builtin_msa_bnz_h(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.h(
-  int_r = __builtin_msa_bnz_w(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.w(
-  int_r = __builtin_msa_bnz_d(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.d(
-
-  int_r = __builtin_msa_bnz_v(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.v(
-
-  v16i8_r = __builtin_msa_bsel_v(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
-  v8i16_r = __builtin_msa_bsel_v(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
-  v4i32_r = __builtin_msa_bsel_v(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
-  v2i64_r = __builtin_msa_bsel_v(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
-
-  v16i8_r = __builtin_msa_bseli_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bseli.b(
-
-  v16i8_r = __builtin_msa_bset_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.bset.b(
-  v8i16_r = __builtin_msa_bset_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.bset.h(
-  v4i32_r = __builtin_msa_bset_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.bset.w(
-  v2i64_r = __builtin_msa_bset_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.bset.d(
-
-  v16i8_r = __builtin_msa_bseti_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bseti.b(
-  v8i16_r = __builtin_msa_bseti_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.bseti.h(
-  v4i32_r = __builtin_msa_bseti_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.bseti.w(
-  v2i64_r = __builtin_msa_bseti_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.bseti.d(
-
-  int_r = __builtin_msa_bz_b(v16i8_a); // CHECK: call i32 @llvm.mips.bz.b(
-  int_r = __builtin_msa_bz_h(v16i8_a); // CHECK: call i32 @llvm.mips.bz.h(
-  int_r = __builtin_msa_bz_w(v16i8_a); // CHECK: call i32 @llvm.mips.bz.w(
-  int_r = __builtin_msa_bz_d(v16i8_a); // CHECK: call i32 @llvm.mips.bz.d(
-
-  int_r = __builtin_msa_bz_v(v16i8_a); // CHECK: call i32 @llvm.mips.bz.v(
-
-  v16i8_r = __builtin_msa_ceq_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ceq.b(
-  v8i16_r = __builtin_msa_ceq_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ceq.h(
-  v4i32_r = __builtin_msa_ceq_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ceq.w(
-  v2i64_r = __builtin_msa_ceq_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ceq.d(
-
-  v16i8_r = __builtin_msa_ceqi_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.ceqi.b(
-  v8i16_r = __builtin_msa_ceqi_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.ceqi.h(
-  v4i32_r = __builtin_msa_ceqi_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.ceqi.w(
-  v2i64_r = __builtin_msa_ceqi_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.ceqi.d(
-
-  int_r = __builtin_msa_cfcmsa(1); // CHECK: call i32 @llvm.mips.cfcmsa(
-
-  v16i8_r = __builtin_msa_cle_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.cle.s.b(
-  v8i16_r = __builtin_msa_cle_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.cle.s.h(
-  v4i32_r = __builtin_msa_cle_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.cle.s.w(
-  v2i64_r = __builtin_msa_cle_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.cle.s.d(
-
-  v16u8_r = __builtin_msa_cle_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.cle.u.b(
-  v8u16_r = __builtin_msa_cle_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.cle.u.h(
-  v4u32_r = __builtin_msa_cle_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.cle.u.w(
-  v2u64_r = __builtin_msa_cle_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.cle.u.d(
-
-  v16i8_r = __builtin_msa_clei_s_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.clei.s.b(
-  v8i16_r = __builtin_msa_clei_s_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.clei.s.h(
-  v4i32_r = __builtin_msa_clei_s_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.clei.s.w(
-  v2i64_r = __builtin_msa_clei_s_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.clei.s.d(
-
-  v16u8_r = __builtin_msa_clei_u_b(v16u8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.clei.u.b(
-  v8u16_r = __builtin_msa_clei_u_h(v8u16_a, 25); // CHECK: call <8  x i16> @llvm.mips.clei.u.h(
-  v4u32_r = __builtin_msa_clei_u_w(v4u32_a, 25); // CHECK: call <4  x i32> @llvm.mips.clei.u.w(
-  v2u64_r = __builtin_msa_clei_u_d(v2u64_a, 25); // CHECK: call <2  x i64> @llvm.mips.clei.u.d(
-
-  v16i8_r = __builtin_msa_clt_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.clt.s.b(
-  v8i16_r = __builtin_msa_clt_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.clt.s.h(
-  v4i32_r = __builtin_msa_clt_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.clt.s.w(
-  v2i64_r = __builtin_msa_clt_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.clt.s.d(
-
-  v16u8_r = __builtin_msa_clt_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.clt.u.b(
-  v8u16_r = __builtin_msa_clt_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.clt.u.h(
-  v4u32_r = __builtin_msa_clt_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.clt.u.w(
-  v2u64_r = __builtin_msa_clt_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.clt.u.d(
-
-  v16i8_r = __builtin_msa_clti_s_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.clti.s.b(
-  v8i16_r = __builtin_msa_clti_s_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.clti.s.h(
-  v4i32_r = __builtin_msa_clti_s_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.clti.s.w(
-  v2i64_r = __builtin_msa_clti_s_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.clti.s.d(
-
-  v16u8_r = __builtin_msa_clti_u_b(v16u8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.clti.u.b(
-  v8u16_r = __builtin_msa_clti_u_h(v8u16_a, 25); // CHECK: call <8  x i16> @llvm.mips.clti.u.h(
-  v4u32_r = __builtin_msa_clti_u_w(v4u32_a, 25); // CHECK: call <4  x i32> @llvm.mips.clti.u.w(
-  v2u64_r = __builtin_msa_clti_u_d(v2u64_a, 25); // CHECK: call <2  x i64> @llvm.mips.clti.u.d(
-
-  int_r = __builtin_msa_copy_s_b(v16i8_a, 1); // CHECK: call i32 @llvm.mips.copy.s.b(
-  int_r = __builtin_msa_copy_s_h(v8i16_a, 1); // CHECK: call i32 @llvm.mips.copy.s.h(
-  int_r = __builtin_msa_copy_s_w(v4i32_a, 1); // CHECK: call i32 @llvm.mips.copy.s.w(
-  ll_r  = __builtin_msa_copy_s_d(v2i64_a, 1); // CHECK: call i64 @llvm.mips.copy.s.d(
-
-  int_r = __builtin_msa_copy_u_b(v16u8_a, 1); // CHECK: call i32 @llvm.mips.copy.u.b(
-  int_r = __builtin_msa_copy_u_h(v8u16_a, 1); // CHECK: call i32 @llvm.mips.copy.u.h(
-  int_r = __builtin_msa_copy_u_w(v4u32_a, 1); // CHECK: call i32 @llvm.mips.copy.u.w(
-  ll_r  = __builtin_msa_copy_u_d(v2i64_a, 1); // CHECK: call i64 @llvm.mips.copy.u.d(
+  v16i8_r = __msa_add_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.add.a.b(
+  v8i16_r = __msa_add_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.add.a.h(
+  v4i32_r = __msa_add_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.add.a.w(
+  v2i64_r = __msa_add_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.add.a.d(
+
+  v16i8_r = __msa_adds_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.adds.a.b(
+  v8i16_r = __msa_adds_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.adds.a.h(
+  v4i32_r = __msa_adds_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.adds.a.w(
+  v2i64_r = __msa_adds_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.adds.a.d(
+
+  v16i8_r = __msa_adds_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.adds.s.b(
+  v8i16_r = __msa_adds_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.adds.s.h(
+  v4i32_r = __msa_adds_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.adds.s.w(
+  v2i64_r = __msa_adds_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.adds.s.d(
+
+  v16u8_r = __msa_adds_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.adds.u.b(
+  v8u16_r = __msa_adds_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.adds.u.h(
+  v4u32_r = __msa_adds_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.adds.u.w(
+  v2u64_r = __msa_adds_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.adds.u.d(
+
+  v16i8_r = __msa_addv_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.addv.b(
+  v8i16_r = __msa_addv_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.addv.h(
+  v4i32_r = __msa_addv_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.addv.w(
+  v2i64_r = __msa_addv_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.addv.d(
+
+  v16u8_r = __msa_addv_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.addv.b(
+  v8u16_r = __msa_addv_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.addv.h(
+  v4u32_r = __msa_addv_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.addv.w(
+  v2u64_r = __msa_addv_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.addv.d(
+
+  v16i8_r = __msa_addvi_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.addvi.b(
+  v8i16_r = __msa_addvi_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.addvi.h(
+  v4i32_r = __msa_addvi_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.addvi.w(
+  v2i64_r = __msa_addvi_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.addvi.d(
+
+  v16u8_r = __msa_addvi_b(v16u8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.addvi.b(
+  v8u16_r = __msa_addvi_h(v8u16_a, 25); // CHECK: call <8  x i16> @llvm.mips.addvi.h(
+  v4u32_r = __msa_addvi_w(v4u32_a, 25); // CHECK: call <4  x i32> @llvm.mips.addvi.w(
+  v2u64_r = __msa_addvi_d(v2u64_a, 25); // CHECK: call <2  x i64> @llvm.mips.addvi.d(
+
+  v16i8_r = __msa_and_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
+  v8i16_r = __msa_and_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
+  v4i32_r = __msa_and_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
+  v2i64_r = __msa_and_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.and.v(
+
+  v16i8_r = __msa_andi_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+  v8i16_r = __msa_andi_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+  v4i32_r = __msa_andi_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+  v2i64_r = __msa_andi_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+
+  v16u8_r = __msa_andi_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+  v8u16_r = __msa_andi_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+  v4u32_r = __msa_andi_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+  v2u64_r = __msa_andi_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.andi.b(
+
+  v16i8_r = __msa_asub_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.asub.s.b(
+  v8i16_r = __msa_asub_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.asub.s.h(
+  v4i32_r = __msa_asub_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.asub.s.w(
+  v2i64_r = __msa_asub_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.asub.s.d(
+
+  v16u8_r = __msa_asub_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.asub.u.b(
+  v8u16_r = __msa_asub_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.asub.u.h(
+  v4u32_r = __msa_asub_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.asub.u.w(
+  v2u64_r = __msa_asub_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.asub.u.d(
+
+  v16i8_r = __msa_ave_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ave.s.b(
+  v8i16_r = __msa_ave_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ave.s.h(
+  v4i32_r = __msa_ave_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ave.s.w(
+  v2i64_r = __msa_ave_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ave.s.d(
+
+  v16u8_r = __msa_ave_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.ave.u.b(
+  v8u16_r = __msa_ave_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.ave.u.h(
+  v4u32_r = __msa_ave_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.ave.u.w(
+  v2u64_r = __msa_ave_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.ave.u.d(
+
+  v16i8_r = __msa_aver_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.aver.s.b(
+  v8i16_r = __msa_aver_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.aver.s.h(
+  v4i32_r = __msa_aver_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.aver.s.w(
+  v2i64_r = __msa_aver_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.aver.s.d(
+
+  v16u8_r = __msa_aver_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.aver.u.b(
+  v8u16_r = __msa_aver_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.aver.u.h(
+  v4u32_r = __msa_aver_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.aver.u.w(
+  v2u64_r = __msa_aver_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.aver.u.d(
+
+  v16i8_r = __msa_bclr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.bclr.b(
+  v8i16_r = __msa_bclr_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.bclr.h(
+  v4i32_r = __msa_bclr_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.bclr.w(
+  v2i64_r = __msa_bclr_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.bclr.d(
+
+  v16i8_r = __msa_bclri_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.bclri.b(
+  v8i16_r = __msa_bclri_h(v8i16_a, 8); // CHECK: call <8  x i16> @llvm.mips.bclri.h(
+  v4i32_r = __msa_bclri_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.bclri.w(
+  v2i64_r = __msa_bclri_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.bclri.d(
+
+  v16i8_r = __msa_binsl_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.binsl.b(
+  v8i16_r = __msa_binsl_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.binsl.h(
+  v4i32_r = __msa_binsl_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.binsl.w(
+  v2i64_r = __msa_binsl_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.binsl.d(
+
+  v16i8_r = __msa_binsli_b(v16i8_r, v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.binsli.b(
+  v8i16_r = __msa_binsli_h(v8i16_r, v8i16_a, 8); // CHECK: call <8  x i16> @llvm.mips.binsli.h(
+  v4i32_r = __msa_binsli_w(v4i32_r, v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.binsli.w(
+  v2i64_r = __msa_binsli_d(v2i64_r, v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.binsli.d(
+
+  v16i8_r = __msa_binsr_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.binsr.b(
+  v8i16_r = __msa_binsr_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.binsr.h(
+  v4i32_r = __msa_binsr_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.binsr.w(
+  v2i64_r = __msa_binsr_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.binsr.d(
+
+  v16i8_r = __msa_binsri_b(v16i8_r, v16i8_a, 5); // CHECK: call <16 x i8>  @llvm.mips.binsri.b(
+  v8i16_r = __msa_binsri_h(v8i16_r, v8i16_a, 15); // CHECK: call <8  x i16> @llvm.mips.binsri.h(
+  v4i32_r = __msa_binsri_w(v4i32_r, v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.binsri.w(
+  v2i64_r = __msa_binsri_d(v2i64_r, v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.binsri.d(
+
+  v16i8_r = __msa_bmnz_v(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
+  v8i16_r = __msa_bmnz_v(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
+  v4i32_r = __msa_bmnz_v(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
+  v2i64_r = __msa_bmnz_v(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.bmnz.v(
+
+  v16i8_r = __msa_bmnzi_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bmnzi.b(
+
+  v16i8_r = __msa_bmz_v(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
+  v8i16_r = __msa_bmz_v(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
+  v4i32_r = __msa_bmz_v(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
+  v2i64_r = __msa_bmz_v(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.bmz.v(
+
+  v16i8_r = __msa_bmzi_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bmzi.b(
+
+  v16i8_r = __msa_bneg_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.bneg.b(
+  v8i16_r = __msa_bneg_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.bneg.h(
+  v4i32_r = __msa_bneg_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.bneg.w(
+  v2i64_r = __msa_bneg_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.bneg.d(
+
+  v16i8_r = __msa_bnegi_b(v16i8_a, 6); // CHECK: call <16 x i8>  @llvm.mips.bnegi.b(
+  v8i16_r = __msa_bnegi_h(v8i16_a, 14); // CHECK: call <8  x i16> @llvm.mips.bnegi.h(
+  v4i32_r = __msa_bnegi_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.bnegi.w(
+  v2i64_r = __msa_bnegi_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.bnegi.d(
+
+  int_r = __msa_test_bnz_b(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.b(
+  int_r = __msa_test_bnz_h(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.h(
+  int_r = __msa_test_bnz_w(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.w(
+  int_r = __msa_test_bnz_d(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.d(
+
+  int_r = __msa_test_bnz_v(v16i8_a); // CHECK: call i32 @llvm.mips.bnz.v(
+
+  v16i8_r = __msa_bsel_v(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
+  v8i16_r = __msa_bsel_v(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
+  v4i32_r = __msa_bsel_v(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
+  v2i64_r = __msa_bsel_v(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.bsel.v(
+
+  v16i8_r = __msa_bseli_b(v16i8_r, v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.bseli.b(
+
+  v16i8_r = __msa_bset_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.bset.b(
+  v8i16_r = __msa_bset_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.bset.h(
+  v4i32_r = __msa_bset_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.bset.w(
+  v2i64_r = __msa_bset_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.bset.d(
+
+  v16i8_r = __msa_bseti_b(v16i8_a, 5); // CHECK: call <16 x i8>  @llvm.mips.bseti.b(
+  v8i16_r = __msa_bseti_h(v8i16_a, 15); // CHECK: call <8  x i16> @llvm.mips.bseti.h(
+  v4i32_r = __msa_bseti_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.bseti.w(
+  v2i64_r = __msa_bseti_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.bseti.d(
+
+  int_r = __msa_test_bz_b(v16i8_a); // CHECK: call i32 @llvm.mips.bz.b(
+  int_r = __msa_test_bz_h(v16i8_a); // CHECK: call i32 @llvm.mips.bz.h(
+  int_r = __msa_test_bz_w(v16i8_a); // CHECK: call i32 @llvm.mips.bz.w(
+  int_r = __msa_test_bz_d(v16i8_a); // CHECK: call i32 @llvm.mips.bz.d(
+
+  int_r = __msa_test_bz_v(v16i8_a); // CHECK: call i32 @llvm.mips.bz.v(
+
+  v16i8_r = __msa_ceq_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ceq.b(
+  v8i16_r = __msa_ceq_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ceq.h(
+  v4i32_r = __msa_ceq_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ceq.w(
+  v2i64_r = __msa_ceq_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ceq.d(
+
+  v16i8_r = __msa_ceqi_b(v16i8_a, -3); // CHECK: call <16 x i8>  @llvm.mips.ceqi.b(
+  v8i16_r = __msa_ceqi_h(v8i16_a, -12); // CHECK: call <8  x i16> @llvm.mips.ceqi.h(
+  v4i32_r = __msa_ceqi_w(v4i32_a, 14); // CHECK: call <4  x i32> @llvm.mips.ceqi.w(
+  v2i64_r = __msa_ceqi_d(v2i64_a, 15); // CHECK: call <2  x i64> @llvm.mips.ceqi.d(
+
+  int_r = __msa_cfcmsa(1); // CHECK: call i32 @llvm.mips.cfcmsa(
+
+  v16i8_r = __msa_cle_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.cle.s.b(
+  v8i16_r = __msa_cle_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.cle.s.h(
+  v4i32_r = __msa_cle_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.cle.s.w(
+  v2i64_r = __msa_cle_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.cle.s.d(
+
+  v16u8_r = __msa_cle_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.cle.u.b(
+  v8u16_r = __msa_cle_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.cle.u.h(
+  v4u32_r = __msa_cle_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.cle.u.w(
+  v2u64_r = __msa_cle_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.cle.u.d(
+
+  v16i8_r = __msa_clei_s_b(v16i8_a, 12); // CHECK: call <16 x i8>  @llvm.mips.clei.s.b(
+  v8i16_r = __msa_clei_s_h(v8i16_a, 13); // CHECK: call <8  x i16> @llvm.mips.clei.s.h(
+  v4i32_r = __msa_clei_s_w(v4i32_a, 14); // CHECK: call <4  x i32> @llvm.mips.clei.s.w(
+  v2i64_r = __msa_clei_s_d(v2i64_a, 15); // CHECK: call <2  x i64> @llvm.mips.clei.s.d(
+
+  v16u8_r = __msa_clei_u_b(v16u8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.clei.u.b(
+  v8u16_r = __msa_clei_u_h(v8u16_a, 25); // CHECK: call <8  x i16> @llvm.mips.clei.u.h(
+  v4u32_r = __msa_clei_u_w(v4u32_a, 25); // CHECK: call <4  x i32> @llvm.mips.clei.u.w(
+  v2u64_r = __msa_clei_u_d(v2u64_a, 25); // CHECK: call <2  x i64> @llvm.mips.clei.u.d(
+
+  v16i8_r = __msa_clt_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.clt.s.b(
+  v8i16_r = __msa_clt_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.clt.s.h(
+  v4i32_r = __msa_clt_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.clt.s.w(
+  v2i64_r = __msa_clt_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.clt.s.d(
+
+  v16u8_r = __msa_clt_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.clt.u.b(
+  v8u16_r = __msa_clt_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.clt.u.h(
+  v4u32_r = __msa_clt_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.clt.u.w(
+  v2u64_r = __msa_clt_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.clt.u.d(
+
+  v16i8_r = __msa_clti_s_b(v16i8_a, 15); // CHECK: call <16 x i8>  @llvm.mips.clti.s.b(
+  v8i16_r = __msa_clti_s_h(v8i16_a, 15); // CHECK: call <8  x i16> @llvm.mips.clti.s.h(
+  v4i32_r = __msa_clti_s_w(v4i32_a, 15); // CHECK: call <4  x i32> @llvm.mips.clti.s.w(
+  v2i64_r = __msa_clti_s_d(v2i64_a, 15); // CHECK: call <2  x i64> @llvm.mips.clti.s.d(
+
+  v16u8_r = __msa_clti_u_b(v16u8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.clti.u.b(
+  v8u16_r = __msa_clti_u_h(v8u16_a, 25); // CHECK: call <8  x i16> @llvm.mips.clti.u.h(
+  v4u32_r = __msa_clti_u_w(v4u32_a, 25); // CHECK: call <4  x i32> @llvm.mips.clti.u.w(
+  v2u64_r = __msa_clti_u_d(v2u64_a, 25); // CHECK: call <2  x i64> @llvm.mips.clti.u.d(
+
+  int_r = __msa_copy_s_b(v16i8_a, 1); // CHECK: call i32 @llvm.mips.copy.s.b(
+  int_r = __msa_copy_s_h(v8i16_a, 1); // CHECK: call i32 @llvm.mips.copy.s.h(
+  int_r = __msa_copy_s_w(v4i32_a, 1); // CHECK: call i32 @llvm.mips.copy.s.w(
+  ll_r  = __msa_copy_s_d(v2i64_a, 1); // CHECK: call i64 @llvm.mips.copy.s.d(
+
+  int_r = __msa_copy_u_b(v16u8_a, 1); // CHECK: call i32 @llvm.mips.copy.u.b(
+  int_r = __msa_copy_u_h(v8u16_a, 1); // CHECK: call i32 @llvm.mips.copy.u.h(
+  int_r = __msa_copy_u_w(v4u32_a, 1); // CHECK: call i32 @llvm.mips.copy.u.w(
+  ll_r  = __msa_copy_u_d(v2i64_a, 1); // CHECK: call i64 @llvm.mips.copy.u.d(
 
   __builtin_msa_ctcmsa(1, int_a); // CHECK: call void @llvm.mips.ctcmsa(
 
-  v16i8_r = __builtin_msa_div_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.div.s.b(
-  v8i16_r = __builtin_msa_div_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.div.s.h(
-  v4i32_r = __builtin_msa_div_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.div.s.w(
-  v2i64_r = __builtin_msa_div_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.div.s.d(
+  v16i8_r = __msa_div_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.div.s.b(
+  v8i16_r = __msa_div_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.div.s.h(
+  v4i32_r = __msa_div_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.div.s.w(
+  v2i64_r = __msa_div_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.div.s.d(
 
-  v16u8_r = __builtin_msa_div_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.div.u.b(
-  v8u16_r = __builtin_msa_div_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.div.u.h(
-  v4u32_r = __builtin_msa_div_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.div.u.w(
-  v2u64_r = __builtin_msa_div_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.div.u.d(
+  v16u8_r = __msa_div_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.div.u.b(
+  v8u16_r = __msa_div_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.div.u.h(
+  v4u32_r = __msa_div_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.div.u.w(
+  v2u64_r = __msa_div_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.div.u.d(
 
-  v8i16_r = __builtin_msa_dotp_s_h(v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.dotp.s.h(
-  v4i32_r = __builtin_msa_dotp_s_w(v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.dotp.s.w(
-  v2i64_r = __builtin_msa_dotp_s_d(v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.dotp.s.d(
+  v8i16_r = __msa_dotp_s_h(v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.dotp.s.h(
+  v4i32_r = __msa_dotp_s_w(v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.dotp.s.w(
+  v2i64_r = __msa_dotp_s_d(v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.dotp.s.d(
 
-  v8u16_r = __builtin_msa_dotp_u_h(v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.dotp.u.h(
-  v4u32_r = __builtin_msa_dotp_u_w(v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.dotp.u.w(
-  v2u64_r = __builtin_msa_dotp_u_d(v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.dotp.u.d(
+  v8u16_r = __msa_dotp_u_h(v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.dotp.u.h(
+  v4u32_r = __msa_dotp_u_w(v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.dotp.u.w(
+  v2u64_r = __msa_dotp_u_d(v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.dotp.u.d(
 
-  v8i16_r = __builtin_msa_dpadd_s_h(v8i16_r, v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.dpadd.s.h(
-  v4i32_r = __builtin_msa_dpadd_s_w(v4i32_r, v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.dpadd.s.w(
-  v2i64_r = __builtin_msa_dpadd_s_d(v2i64_r, v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.dpadd.s.d(
+  v8i16_r = __msa_dpadd_s_h(v8i16_r, v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.dpadd.s.h(
+  v4i32_r = __msa_dpadd_s_w(v4i32_r, v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.dpadd.s.w(
+  v2i64_r = __msa_dpadd_s_d(v2i64_r, v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.dpadd.s.d(
 
-  v8u16_r = __builtin_msa_dpadd_u_h(v8u16_r, v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.dpadd.u.h(
-  v4u32_r = __builtin_msa_dpadd_u_w(v4u32_r, v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.dpadd.u.w(
-  v2u64_r = __builtin_msa_dpadd_u_d(v2u64_r, v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.dpadd.u.d(
+  v8u16_r = __msa_dpadd_u_h(v8u16_r, v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.dpadd.u.h(
+  v4u32_r = __msa_dpadd_u_w(v4u32_r, v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.dpadd.u.w(
+  v2u64_r = __msa_dpadd_u_d(v2u64_r, v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.dpadd.u.d(
 
-  v8i16_r = __builtin_msa_dpsub_s_h(v8i16_r, v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.dpsub.s.h(
-  v4i32_r = __builtin_msa_dpsub_s_w(v4i32_r, v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.dpsub.s.w(
-  v2i64_r = __builtin_msa_dpsub_s_d(v2i64_r, v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.dpsub.s.d(
+  v8i16_r = __msa_dpsub_s_h(v8i16_r, v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.dpsub.s.h(
+  v4i32_r = __msa_dpsub_s_w(v4i32_r, v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.dpsub.s.w(
+  v2i64_r = __msa_dpsub_s_d(v2i64_r, v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.dpsub.s.d(
 
-  v8u16_r = __builtin_msa_dpsub_u_h(v8u16_r, v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.dpsub.u.h(
-  v4u32_r = __builtin_msa_dpsub_u_w(v4u32_r, v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.dpsub.u.w(
-  v2u64_r = __builtin_msa_dpsub_u_d(v2u64_r, v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.dpsub.u.d(
+  v8u16_r = __msa_dpsub_u_h(v8u16_r, v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.dpsub.u.h(
+  v4u32_r = __msa_dpsub_u_w(v4u32_r, v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.dpsub.u.w(
+  v2u64_r = __msa_dpsub_u_d(v2u64_r, v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.dpsub.u.d(
 
-  v4f32_r = __builtin_msa_fadd_w(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.mips.fadd.w(
-  v2f64_r = __builtin_msa_fadd_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fadd.d(
+  v4f32_r = __msa_fadd_w(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.mips.fadd.w(
+  v2f64_r = __msa_fadd_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fadd.d(
 
-  v4i32_r = __builtin_msa_fcaf_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcaf.w(
-  v2i64_r = __builtin_msa_fcaf_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcaf.d(
+  v4i32_r = __msa_fcaf_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcaf.w(
+  v2i64_r = __msa_fcaf_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcaf.d(
 
-  v4i32_r = __builtin_msa_fceq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fceq.w(
-  v2i64_r = __builtin_msa_fceq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fceq.d(
+  v4i32_r = __msa_fceq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fceq.w(
+  v2i64_r = __msa_fceq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fceq.d(
 
-  v4i32_r = __builtin_msa_fclass_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.fclass.w(
-  v2i64_r = __builtin_msa_fclass_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.fclass.d(
+  v4i32_r = __msa_fclass_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.fclass.w(
+  v2i64_r = __msa_fclass_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.fclass.d(
 
-  v4i32_r = __builtin_msa_fcle_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcle.w(
-  v2i64_r = __builtin_msa_fcle_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcle.d(
+  v4i32_r = __msa_fcle_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcle.w(
+  v2i64_r = __msa_fcle_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcle.d(
 
-  v4i32_r = __builtin_msa_fclt_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fclt.w(
-  v2i64_r = __builtin_msa_fclt_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fclt.d(
+  v4i32_r = __msa_fclt_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fclt.w(
+  v2i64_r = __msa_fclt_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fclt.d(
 
-  v4i32_r = __builtin_msa_fcne_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcne.w(
-  v2i64_r = __builtin_msa_fcne_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcne.d(
+  v4i32_r = __msa_fcne_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcne.w(
+  v2i64_r = __msa_fcne_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcne.d(
 
-  v4i32_r = __builtin_msa_fcor_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcor.w(
-  v2i64_r = __builtin_msa_fcor_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcor.d(
+  v4i32_r = __msa_fcor_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcor.w(
+  v2i64_r = __msa_fcor_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcor.d(
 
-  v4i32_r = __builtin_msa_fcueq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcueq.w(
-  v2i64_r = __builtin_msa_fcueq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcueq.d(
+  v4i32_r = __msa_fcueq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcueq.w(
+  v2i64_r = __msa_fcueq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcueq.d(
 
-  v4i32_r = __builtin_msa_fcule_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcule.w(
-  v2i64_r = __builtin_msa_fcule_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcule.d(
+  v4i32_r = __msa_fcule_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcule.w(
+  v2i64_r = __msa_fcule_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcule.d(
 
-  v4i32_r = __builtin_msa_fcult_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcult.w(
-  v2i64_r = __builtin_msa_fcult_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcult.d(
+  v4i32_r = __msa_fcult_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcult.w(
+  v2i64_r = __msa_fcult_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcult.d(
 
-  v4i32_r = __builtin_msa_fcun_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcun.w(
-  v2i64_r = __builtin_msa_fcun_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcun.d(
+  v4i32_r = __msa_fcun_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcun.w(
+  v2i64_r = __msa_fcun_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcun.d(
 
-  v4i32_r = __builtin_msa_fcune_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcune.w(
-  v2i64_r = __builtin_msa_fcune_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcune.d(
+  v4i32_r = __msa_fcune_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fcune.w(
+  v2i64_r = __msa_fcune_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fcune.d(
 
-  v4f32_r = __builtin_msa_fdiv_w(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.mips.fdiv.w(
-  v2f64_r = __builtin_msa_fdiv_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fdiv.d(
+  v4f32_r = __msa_fdiv_w(v4f32_a, v4f32_b); // CHECK: call <4 x float> @llvm.mips.fdiv.w(
+  v2f64_r = __msa_fdiv_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fdiv.d(
 
-  v8f16_r = __builtin_msa_fexdo_h(v4f32_a, v4f32_b); // CHECK: call <8 x half> @llvm.mips.fexdo.h(
-  v4f32_r = __builtin_msa_fexdo_w(v2f64_a, v2f64_b); // CHECK: call <4 x float> @llvm.mips.fexdo.w(
+  v8f16_r = __msa_fexdo_h(v4f32_a, v4f32_b); // CHECK: call <8 x half> @llvm.mips.fexdo.h(
+  v4f32_r = __msa_fexdo_w(v2f64_a, v2f64_b); // CHECK: call <4 x float> @llvm.mips.fexdo.w(
 
-  v4f32_r = __builtin_msa_fexp2_w(v4f32_a, v4i32_b); // CHECK: call <4 x float> @llvm.mips.fexp2.w(
-  v2f64_r = __builtin_msa_fexp2_d(v2f64_a, v2i64_b); // CHECK: call <2 x double> @llvm.mips.fexp2.d(
+  v4f32_r = __msa_fexp2_w(v4f32_a, v4i32_b); // CHECK: call <4 x float> @llvm.mips.fexp2.w(
+  v2f64_r = __msa_fexp2_d(v2f64_a, v2i64_b); // CHECK: call <2 x double> @llvm.mips.fexp2.d(
 
-  v4f32_r = __builtin_msa_fexupl_w(v8f16_a); // CHECK: call <4 x float> @llvm.mips.fexupl.w(
-  v2f64_r = __builtin_msa_fexupl_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.fexupl.d(
+  v4f32_r = __msa_fexupl_w(v8f16_a); // CHECK: call <4 x float> @llvm.mips.fexupl.w(
+  v2f64_r = __msa_fexupl_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.fexupl.d(
 
-  v4f32_r = __builtin_msa_fexupr_w(v8f16_a); // CHECK: call <4 x float> @llvm.mips.fexupr.w(
-  v2f64_r = __builtin_msa_fexupr_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.fexupr.d(
+  v4f32_r = __msa_fexupr_w(v8f16_a); // CHECK: call <4 x float> @llvm.mips.fexupr.w(
+  v2f64_r = __msa_fexupr_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.fexupr.d(
 
-  v4f32_r = __builtin_msa_ffint_s_w(v4i32_a); // CHECK: call <4 x float> @llvm.mips.ffint.s.w(
-  v2f64_r = __builtin_msa_ffint_s_d(v2i64_a); // CHECK: call <2 x double> @llvm.mips.ffint.s.d(
+  v4f32_r = __msa_ffint_s_w(v4i32_a); // CHECK: call <4 x float> @llvm.mips.ffint.s.w(
+  v2f64_r = __msa_ffint_s_d(v2i64_a); // CHECK: call <2 x double> @llvm.mips.ffint.s.d(
 
-  v4f32_r = __builtin_msa_ffint_u_w(v4i32_a); // CHECK: call <4 x float> @llvm.mips.ffint.u.w(
-  v2f64_r = __builtin_msa_ffint_u_d(v2i64_a); // CHECK: call <2 x double> @llvm.mips.ffint.u.d(
+  v4f32_r = __msa_ffint_u_w(v4i32_a); // CHECK: call <4 x float> @llvm.mips.ffint.u.w(
+  v2f64_r = __msa_ffint_u_d(v2i64_a); // CHECK: call <2 x double> @llvm.mips.ffint.u.d(
 
-  v4f32_r = __builtin_msa_ffql_w(v8i16_a); // CHECK: call <4 x float> @llvm.mips.ffql.w(
-  v2f64_r = __builtin_msa_ffql_d(v4i32_a); // CHECK: call <2 x double> @llvm.mips.ffql.d(
+  v4f32_r = __msa_ffql_w(v8i16_a); // CHECK: call <4 x float> @llvm.mips.ffql.w(
+  v2f64_r = __msa_ffql_d(v4i32_a); // CHECK: call <2 x double> @llvm.mips.ffql.d(
 
-  v4f32_r = __builtin_msa_ffqr_w(v8i16_a); // CHECK: call <4 x float> @llvm.mips.ffqr.w(
-  v2f64_r = __builtin_msa_ffqr_d(v4i32_a); // CHECK: call <2 x double> @llvm.mips.ffqr.d(
+  v4f32_r = __msa_ffqr_w(v8i16_a); // CHECK: call <4 x float> @llvm.mips.ffqr.w(
+  v2f64_r = __msa_ffqr_d(v4i32_a); // CHECK: call <2 x double> @llvm.mips.ffqr.d(
 
-  v16i8_r = __builtin_msa_fill_b(3); // CHECK: call <16 x i8>  @llvm.mips.fill.b(
-  v8i16_r = __builtin_msa_fill_h(3); // CHECK: call <8  x i16> @llvm.mips.fill.h(
-  v4i32_r = __builtin_msa_fill_w(3); // CHECK: call <4  x i32> @llvm.mips.fill.w(
-  v2i64_r = __builtin_msa_fill_d(3); // CHECK: call <2  x i64> @llvm.mips.fill.d(
+  v16i8_r = __msa_fill_b(3); // CHECK: call <16 x i8>  @llvm.mips.fill.b(
+  v8i16_r = __msa_fill_h(3); // CHECK: call <8  x i16> @llvm.mips.fill.h(
+  v4i32_r = __msa_fill_w(3); // CHECK: call <4  x i32> @llvm.mips.fill.w(
+  v2i64_r = __msa_fill_d(3); // CHECK: call <2  x i64> @llvm.mips.fill.d(
 
-  v4f32_r = __builtin_msa_flog2_w(v8f16_a); // CHECK: call <4 x float>  @llvm.mips.flog2.w(
-  v2f64_r = __builtin_msa_flog2_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.flog2.d(
+  v4f32_r = __msa_flog2_w(v4f32_a); // CHECK: call <4 x float>  @llvm.mips.flog2.w(
+  v2f64_r = __msa_flog2_d(v2f64_a); // CHECK: call <2 x double> @llvm.mips.flog2.d(
 
-  v4f32_r = __builtin_msa_fmadd_w(v8f16_r, v8f16_a, v8f16_b); // CHECK: call <4 x float>  @llvm.mips.fmadd.w(
-  v2f64_r = __builtin_msa_fmadd_d(v4f32_r, v4f32_a, v4f32_b); // CHECK: call <2 x double> @llvm.mips.fmadd.d(
+  v4f32_r = __msa_fmadd_w(v4f32_r, v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmadd.w(
+  v2f64_r = __msa_fmadd_d(v2f64_r, v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmadd.d(
 
-  v4f32_r = __builtin_msa_fmax_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmax.w(
-  v2f64_r = __builtin_msa_fmax_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmax.d(
+  v4f32_r = __msa_fmax_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmax.w(
+  v2f64_r = __msa_fmax_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmax.d(
 
-  v4f32_r = __builtin_msa_fmax_a_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmax.a.w(
-  v2f64_r = __builtin_msa_fmax_a_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmax.a.d(
+  v4f32_r = __msa_fmax_a_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmax.a.w(
+  v2f64_r = __msa_fmax_a_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmax.a.d(
 
-  v4f32_r = __builtin_msa_fmin_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmin.w(
-  v2f64_r = __builtin_msa_fmin_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmin.d(
+  v4f32_r = __msa_fmin_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmin.w(
+  v2f64_r = __msa_fmin_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmin.d(
 
-  v4f32_r = __builtin_msa_fmin_a_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmin.a.w(
-  v2f64_r = __builtin_msa_fmin_a_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmin.a.d(
+  v4f32_r = __msa_fmin_a_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmin.a.w(
+  v2f64_r = __msa_fmin_a_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmin.a.d(
 
-  v4f32_r = __builtin_msa_fmsub_w(v8f16_r, v8f16_a, v8f16_b); // CHECK: call <4 x float>  @llvm.mips.fmsub.w(
-  v2f64_r = __builtin_msa_fmsub_d(v4f32_r, v4f32_a, v4f32_b); // CHECK: call <2 x double> @llvm.mips.fmsub.d(
+  v4f32_r = __msa_fmsub_w(v4f32_r, v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmsub.w(
+  v2f64_r = __msa_fmsub_d(v2f64_r, v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmsub.d(
 
-  v4f32_r = __builtin_msa_fmul_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmul.w(
-  v2f64_r = __builtin_msa_fmul_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmul.d(
+  v4f32_r = __msa_fmul_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fmul.w(
+  v2f64_r = __msa_fmul_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fmul.d(
 
-  v4f32_r = __builtin_msa_frint_w(v8f16_a); // CHECK: call <4 x float>  @llvm.mips.frint.w(
-  v2f64_r = __builtin_msa_frint_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.frint.d(
+  v4f32_r = __msa_frint_w(v4f32_a); // CHECK: call <4 x float>  @llvm.mips.frint.w(
+  v2f64_r = __msa_frint_d(v2f64_a); // CHECK: call <2 x double> @llvm.mips.frint.d(
 
-  v4f32_r = __builtin_msa_frcp_w(v8f16_a); // CHECK: call <4 x float>  @llvm.mips.frcp.w(
-  v2f64_r = __builtin_msa_frcp_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.frcp.d(
+  v4f32_r = __msa_frcp_w(v4f32_a); // CHECK: call <4 x float>  @llvm.mips.frcp.w(
+  v2f64_r = __msa_frcp_d(v2f64_a); // CHECK: call <2 x double> @llvm.mips.frcp.d(
 
-  v4f32_r = __builtin_msa_frsqrt_w(v8f16_a); // CHECK: call <4 x float>  @llvm.mips.frsqrt.w(
-  v2f64_r = __builtin_msa_frsqrt_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.frsqrt.d(
+  v4f32_r = __msa_frsqrt_w(v4f32_a); // CHECK: call <4 x float>  @llvm.mips.frsqrt.w(
+  v2f64_r = __msa_frsqrt_d(v2f64_a); // CHECK: call <2 x double> @llvm.mips.frsqrt.d(
 
-  v4i32_r = __builtin_msa_fseq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fseq.w(
-  v2i64_r = __builtin_msa_fseq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fseq.d(
+  v4i32_r = __msa_fseq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fseq.w(
+  v2i64_r = __msa_fseq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fseq.d(
 
-  v4i32_r = __builtin_msa_fsaf_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsaf.w(
-  v2i64_r = __builtin_msa_fsaf_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsaf.d(
+  v4i32_r = __msa_fsaf_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsaf.w(
+  v2i64_r = __msa_fsaf_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsaf.d(
 
-  v4i32_r = __builtin_msa_fsle_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsle.w(
-  v2i64_r = __builtin_msa_fsle_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsle.d(
+  v4i32_r = __msa_fsle_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsle.w(
+  v2i64_r = __msa_fsle_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsle.d(
 
-  v4i32_r = __builtin_msa_fslt_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fslt.w(
-  v2i64_r = __builtin_msa_fslt_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fslt.d(
+  v4i32_r = __msa_fslt_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fslt.w(
+  v2i64_r = __msa_fslt_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fslt.d(
 
-  v4i32_r = __builtin_msa_fsne_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsne.w(
-  v2i64_r = __builtin_msa_fsne_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsne.d(
+  v4i32_r = __msa_fsne_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsne.w(
+  v2i64_r = __msa_fsne_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsne.d(
 
-  v4i32_r = __builtin_msa_fsor_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsor.w(
-  v2i64_r = __builtin_msa_fsor_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsor.d(
+  v4i32_r = __msa_fsor_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsor.w(
+  v2i64_r = __msa_fsor_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsor.d(
 
-  v4f32_r = __builtin_msa_fsqrt_w(v8f16_a); // CHECK: call <4 x float>  @llvm.mips.fsqrt.w(
-  v2f64_r = __builtin_msa_fsqrt_d(v4f32_a); // CHECK: call <2 x double> @llvm.mips.fsqrt.d(
+  v4f32_r = __msa_fsqrt_w(v4f32_a); // CHECK: call <4 x float>  @llvm.mips.fsqrt.w(
+  v2f64_r = __msa_fsqrt_d(v2f64_a); // CHECK: call <2 x double> @llvm.mips.fsqrt.d(
 
-  v4f32_r = __builtin_msa_fsub_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fsub.w(
-  v2f64_r = __builtin_msa_fsub_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fsub.d(
+  v4f32_r = __msa_fsub_w(v4f32_a, v4f32_b); // CHECK: call <4 x float>  @llvm.mips.fsub.w(
+  v2f64_r = __msa_fsub_d(v2f64_a, v2f64_b); // CHECK: call <2 x double> @llvm.mips.fsub.d(
 
-  v4i32_r = __builtin_msa_fsueq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsueq.w(
-  v2i64_r = __builtin_msa_fsueq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsueq.d(
+  v4i32_r = __msa_fsueq_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsueq.w(
+  v2i64_r = __msa_fsueq_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsueq.d(
 
-  v4i32_r = __builtin_msa_fsule_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsule.w(
-  v2i64_r = __builtin_msa_fsule_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsule.d(
+  v4i32_r = __msa_fsule_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsule.w(
+  v2i64_r = __msa_fsule_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsule.d(
 
-  v4i32_r = __builtin_msa_fsult_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsult.w(
-  v2i64_r = __builtin_msa_fsult_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsult.d(
+  v4i32_r = __msa_fsult_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsult.w(
+  v2i64_r = __msa_fsult_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsult.d(
 
-  v4i32_r = __builtin_msa_fsun_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsun.w(
-  v2i64_r = __builtin_msa_fsun_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsun.d(
+  v4i32_r = __msa_fsun_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsun.w(
+  v2i64_r = __msa_fsun_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsun.d(
 
-  v4i32_r = __builtin_msa_fsune_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsune.w(
-  v2i64_r = __builtin_msa_fsune_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsune.d(
+  v4i32_r = __msa_fsune_w(v4f32_a, v4f32_b); // CHECK: call <4 x i32> @llvm.mips.fsune.w(
+  v2i64_r = __msa_fsune_d(v2f64_a, v2f64_b); // CHECK: call <2 x i64> @llvm.mips.fsune.d(
 
-  v4i32_r = __builtin_msa_ftint_s_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftint.s.w(
-  v2i64_r = __builtin_msa_ftint_s_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftint.s.d(
+  v4i32_r = __msa_ftint_s_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftint.s.w(
+  v2i64_r = __msa_ftint_s_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftint.s.d(
 
-  v4i32_r = __builtin_msa_ftint_u_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftint.u.w(
-  v2i64_r = __builtin_msa_ftint_u_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftint.u.d(
+  v4i32_r = __msa_ftint_u_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftint.u.w(
+  v2i64_r = __msa_ftint_u_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftint.u.d(
 
-  v8i16_r = __builtin_msa_ftq_h(v4f32_a, v4f32_b); // CHECK: call <8 x i16> @llvm.mips.ftq.h(
-  v4i32_r = __builtin_msa_ftq_w(v2f64_a, v2f64_b); // CHECK: call <4 x i32> @llvm.mips.ftq.w(
+  v8i16_r = __msa_ftq_h(v4f32_a, v4f32_b); // CHECK: call <8 x i16> @llvm.mips.ftq.h(
+  v4i32_r = __msa_ftq_w(v2f64_a, v2f64_b); // CHECK: call <4 x i32> @llvm.mips.ftq.w(
 
-  v4i32_r = __builtin_msa_ftrunc_s_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftrunc.s.w(
-  v2i64_r = __builtin_msa_ftrunc_s_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftrunc.s.d(
+  v4i32_r = __msa_ftrunc_s_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftrunc.s.w(
+  v2i64_r = __msa_ftrunc_s_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftrunc.s.d(
 
-  v4i32_r = __builtin_msa_ftrunc_u_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftrunc.u.w(
-  v2i64_r = __builtin_msa_ftrunc_u_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftrunc.u.d(
+  v4i32_r = __msa_ftrunc_u_w(v4f32_a); // CHECK: call <4 x i32> @llvm.mips.ftrunc.u.w(
+  v2i64_r = __msa_ftrunc_u_d(v2f64_a); // CHECK: call <2 x i64> @llvm.mips.ftrunc.u.d(
 
-  v8i16_r = __builtin_msa_hadd_s_h(v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.hadd.s.h(
-  v4i32_r = __builtin_msa_hadd_s_w(v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.hadd.s.w(
-  v2i64_r = __builtin_msa_hadd_s_d(v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.hadd.s.d(
+  v8i16_r = __msa_hadd_s_h(v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.hadd.s.h(
+  v4i32_r = __msa_hadd_s_w(v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.hadd.s.w(
+  v2i64_r = __msa_hadd_s_d(v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.hadd.s.d(
 
-  v8u16_r = __builtin_msa_hadd_u_h(v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.hadd.u.h(
-  v4u32_r = __builtin_msa_hadd_u_w(v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.hadd.u.w(
-  v2u64_r = __builtin_msa_hadd_u_d(v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.hadd.u.d(
+  v8u16_r = __msa_hadd_u_h(v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.hadd.u.h(
+  v4u32_r = __msa_hadd_u_w(v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.hadd.u.w(
+  v2u64_r = __msa_hadd_u_d(v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.hadd.u.d(
 
-  v8i16_r = __builtin_msa_hsub_s_h(v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.hsub.s.h(
-  v4i32_r = __builtin_msa_hsub_s_w(v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.hsub.s.w(
-  v2i64_r = __builtin_msa_hsub_s_d(v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.hsub.s.d(
+  v8i16_r = __msa_hsub_s_h(v16i8_a, v16i8_b); // CHECK: call <8  x i16> @llvm.mips.hsub.s.h(
+  v4i32_r = __msa_hsub_s_w(v8i16_a, v8i16_b); // CHECK: call <4  x i32> @llvm.mips.hsub.s.w(
+  v2i64_r = __msa_hsub_s_d(v4i32_a, v4i32_b); // CHECK: call <2  x i64> @llvm.mips.hsub.s.d(
 
-  v8u16_r = __builtin_msa_hsub_u_h(v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.hsub.u.h(
-  v4u32_r = __builtin_msa_hsub_u_w(v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.hsub.u.w(
-  v2u64_r = __builtin_msa_hsub_u_d(v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.hsub.u.d(
+  v8u16_r = __msa_hsub_u_h(v16u8_a, v16u8_b); // CHECK: call <8  x i16> @llvm.mips.hsub.u.h(
+  v4u32_r = __msa_hsub_u_w(v8u16_a, v8u16_b); // CHECK: call <4  x i32> @llvm.mips.hsub.u.w(
+  v2u64_r = __msa_hsub_u_d(v4u32_a, v4u32_b); // CHECK: call <2  x i64> @llvm.mips.hsub.u.d(
 
-  v16i8_r = __builtin_msa_ilvev_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvev.b(
-  v8i16_r = __builtin_msa_ilvev_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvev.h(
-  v4i32_r = __builtin_msa_ilvev_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvev.w(
-  v2i64_r = __builtin_msa_ilvev_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvev.d(
+  v16i8_r = __msa_ilvev_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvev.b(
+  v8i16_r = __msa_ilvev_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvev.h(
+  v4i32_r = __msa_ilvev_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvev.w(
+  v2i64_r = __msa_ilvev_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvev.d(
 
-  v16i8_r = __builtin_msa_ilvl_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvl.b(
-  v8i16_r = __builtin_msa_ilvl_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvl.h(
-  v4i32_r = __builtin_msa_ilvl_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvl.w(
-  v2i64_r = __builtin_msa_ilvl_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvl.d(
+  v16i8_r = __msa_ilvl_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvl.b(
+  v8i16_r = __msa_ilvl_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvl.h(
+  v4i32_r = __msa_ilvl_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvl.w(
+  v2i64_r = __msa_ilvl_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvl.d(
 
-  v16i8_r = __builtin_msa_ilvod_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvod.b(
-  v8i16_r = __builtin_msa_ilvod_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvod.h(
-  v4i32_r = __builtin_msa_ilvod_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvod.w(
-  v2i64_r = __builtin_msa_ilvod_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvod.d(
+  v16i8_r = __msa_ilvod_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvod.b(
+  v8i16_r = __msa_ilvod_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvod.h(
+  v4i32_r = __msa_ilvod_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvod.w(
+  v2i64_r = __msa_ilvod_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvod.d(
 
-  v16i8_r = __builtin_msa_ilvr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvr.b(
-  v8i16_r = __builtin_msa_ilvr_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvr.h(
-  v4i32_r = __builtin_msa_ilvr_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvr.w(
-  v2i64_r = __builtin_msa_ilvr_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvr.d(
+  v16i8_r = __msa_ilvr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.ilvr.b(
+  v8i16_r = __msa_ilvr_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.ilvr.h(
+  v4i32_r = __msa_ilvr_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.ilvr.w(
+  v2i64_r = __msa_ilvr_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.ilvr.d(
 
-  v16i8_r = __builtin_msa_insert_b(v16i8_r, 1, 25); // CHECK: call <16 x i8>  @llvm.mips.insert.b(
-  v8i16_r = __builtin_msa_insert_h(v8i16_r, 1, 25); // CHECK: call <8  x i16> @llvm.mips.insert.h(
-  v4i32_r = __builtin_msa_insert_w(v4i32_r, 1, 25); // CHECK: call <4  x i32> @llvm.mips.insert.w(
-  v2i64_r = __builtin_msa_insert_d(v2i64_r, 1, 25); // CHECK: call <2  x i64> @llvm.mips.insert.d(
+  v16i8_r = __msa_insert_b(v16i8_r, 1, 25); // CHECK: call <16 x i8>  @llvm.mips.insert.b(
+  v8i16_r = __msa_insert_h(v8i16_r, 1, 25); // CHECK: call <8  x i16> @llvm.mips.insert.h(
+  v4i32_r = __msa_insert_w(v4i32_r, 1, 25); // CHECK: call <4  x i32> @llvm.mips.insert.w(
+  v2i64_r = __msa_insert_d(v2i64_r, 1, 25); // CHECK: call <2  x i64> @llvm.mips.insert.d(
 
-  v16i8_r = __builtin_msa_insve_b(v16i8_r, 1, v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.insve.b(
-  v8i16_r = __builtin_msa_insve_h(v8i16_r, 1, v8i16_a); // CHECK: call <8  x i16> @llvm.mips.insve.h(
-  v4i32_r = __builtin_msa_insve_w(v4i32_r, 1, v4i32_a); // CHECK: call <4  x i32> @llvm.mips.insve.w(
-  v2i64_r = __builtin_msa_insve_d(v2i64_r, 1, v2i64_a); // CHECK: call <2  x i64> @llvm.mips.insve.d(
+  v16i8_r = __msa_insve_b(v16i8_r, 1, v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.insve.b(
+  v8i16_r = __msa_insve_h(v8i16_r, 1, v8i16_a); // CHECK: call <8  x i16> @llvm.mips.insve.h(
+  v4i32_r = __msa_insve_w(v4i32_r, 1, v4i32_a); // CHECK: call <4  x i32> @llvm.mips.insve.w(
+  v2i64_r = __msa_insve_d(v2i64_r, 1, v2i64_a); // CHECK: call <2  x i64> @llvm.mips.insve.d(
 
-  v16i8_r = __builtin_msa_ld_b(&v16i8_a, 1); // CHECK: call <16 x i8>  @llvm.mips.ld.b(
-  v8i16_r = __builtin_msa_ld_h(&v8i16_a, 2); // CHECK: call <8  x i16> @llvm.mips.ld.h(
-  v4i32_r = __builtin_msa_ld_w(&v4i32_a, 4); // CHECK: call <4  x i32> @llvm.mips.ld.w(
-  v2i64_r = __builtin_msa_ld_d(&v2i64_a, 8); // CHECK: call <2  x i64> @llvm.mips.ld.d(
+  v16i8_r = __msa_ld_b(&v16i8_a, 16); // CHECK: call <16 x i8>  @llvm.mips.ld.b(
+  v8i16_r = __msa_ld_h(&v8i16_a, 32); // CHECK: call <8  x i16> @llvm.mips.ld.h(
+  v4i32_r = __msa_ld_w(&v4i32_a, 48); // CHECK: call <4  x i32> @llvm.mips.ld.w(
+  v2i64_r = __msa_ld_d(&v2i64_a, 96); // CHECK: call <2  x i64> @llvm.mips.ld.d(
 
-  v16i8_r = __builtin_msa_ldi_b(3); // CHECK: call <16 x i8>  @llvm.mips.ldi.b(
-  v8i16_r = __builtin_msa_ldi_h(3); // CHECK: call <8  x i16> @llvm.mips.ldi.h(
-  v4i32_r = __builtin_msa_ldi_w(3); // CHECK: call <4  x i32> @llvm.mips.ldi.w(
-  v2i64_r = __builtin_msa_ldi_d(3); // CHECK: call <2  x i64> @llvm.mips.ldi.d(
+  v16i8_r = __msa_ldi_b(3); // CHECK: call <16 x i8>  @llvm.mips.ldi.b(
+  v8i16_r = __msa_ldi_h(3); // CHECK: call <8  x i16> @llvm.mips.ldi.h(
+  v4i32_r = __msa_ldi_w(3); // CHECK: call <4  x i32> @llvm.mips.ldi.w(
+  v2i64_r = __msa_ldi_d(3); // CHECK: call <2  x i64> @llvm.mips.ldi.d(
 
-  v8i16_r = __builtin_msa_madd_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.madd.q.h(
-  v4i32_r = __builtin_msa_madd_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.madd.q.w(
-
-  v8i16_r = __builtin_msa_maddr_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.maddr.q.h(
-  v4i32_r = __builtin_msa_maddr_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.maddr.q.w(
-
-  v16i8_r = __builtin_msa_maddv_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.maddv.b(
-  v8i16_r = __builtin_msa_maddv_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.maddv.h(
-  v4i32_r = __builtin_msa_maddv_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.maddv.w(
-  v2i64_r = __builtin_msa_maddv_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.maddv.d(
-
-  v16i8_r = __builtin_msa_max_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.max.a.b(
-  v8i16_r = __builtin_msa_max_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.max.a.h(
-  v4i32_r = __builtin_msa_max_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.max.a.w(
-  v2i64_r = __builtin_msa_max_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.max.a.d(
-
-  v16i8_r = __builtin_msa_max_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.max.s.b(
-  v8i16_r = __builtin_msa_max_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.max.s.h(
-  v4i32_r = __builtin_msa_max_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.max.s.w(
-  v2i64_r = __builtin_msa_max_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.max.s.d(
-
-  v16u8_r = __builtin_msa_max_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.max.u.b(
-  v8u16_r = __builtin_msa_max_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.max.u.h(
-  v4u32_r = __builtin_msa_max_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.max.u.w(
-  v2u64_r = __builtin_msa_max_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.max.u.d(
-
-  v16i8_r = __builtin_msa_maxi_s_b(v16i8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.maxi.s.b(
-  v8i16_r = __builtin_msa_maxi_s_h(v8i16_a, 2); // CHECK: call <8  x i16> @llvm.mips.maxi.s.h(
-  v4i32_r = __builtin_msa_maxi_s_w(v4i32_a, 2); // CHECK: call <4  x i32> @llvm.mips.maxi.s.w(
-  v2i64_r = __builtin_msa_maxi_s_d(v2i64_a, 2); // CHECK: call <2  x i64> @llvm.mips.maxi.s.d(
-
-  v16u8_r = __builtin_msa_maxi_u_b(v16u8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.maxi.u.b(
-  v8u16_r = __builtin_msa_maxi_u_h(v8u16_a, 2); // CHECK: call <8  x i16> @llvm.mips.maxi.u.h(
-  v4u32_r = __builtin_msa_maxi_u_w(v4u32_a, 2); // CHECK: call <4  x i32> @llvm.mips.maxi.u.w(
-  v2u64_r = __builtin_msa_maxi_u_d(v2u64_a, 2); // CHECK: call <2  x i64> @llvm.mips.maxi.u.d(
-
-  v16i8_r = __builtin_msa_min_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.min.a.b(
-  v8i16_r = __builtin_msa_min_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.min.a.h(
-  v4i32_r = __builtin_msa_min_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.min.a.w(
-  v2i64_r = __builtin_msa_min_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.min.a.d(
-
-  v16i8_r = __builtin_msa_min_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.min.s.b(
-  v8i16_r = __builtin_msa_min_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.min.s.h(
-  v4i32_r = __builtin_msa_min_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.min.s.w(
-  v2i64_r = __builtin_msa_min_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.min.s.d(
-
-  v16u8_r = __builtin_msa_min_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.min.u.b(
-  v8u16_r = __builtin_msa_min_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.min.u.h(
-  v4u32_r = __builtin_msa_min_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.min.u.w(
-  v2u64_r = __builtin_msa_min_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.min.u.d(
-
-  v16i8_r = __builtin_msa_mini_s_b(v16i8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.mini.s.b(
-  v8i16_r = __builtin_msa_mini_s_h(v8i16_a, 2); // CHECK: call <8  x i16> @llvm.mips.mini.s.h(
-  v4i32_r = __builtin_msa_mini_s_w(v4i32_a, 2); // CHECK: call <4  x i32> @llvm.mips.mini.s.w(
-  v2i64_r = __builtin_msa_mini_s_d(v2i64_a, 2); // CHECK: call <2  x i64> @llvm.mips.mini.s.d(
-
-  v16u8_r = __builtin_msa_mini_u_b(v16u8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.mini.u.b(
-  v8u16_r = __builtin_msa_mini_u_h(v8u16_a, 2); // CHECK: call <8  x i16> @llvm.mips.mini.u.h(
-  v4u32_r = __builtin_msa_mini_u_w(v4u32_a, 2); // CHECK: call <4  x i32> @llvm.mips.mini.u.w(
-  v2u64_r = __builtin_msa_mini_u_d(v2u64_a, 2); // CHECK: call <2  x i64> @llvm.mips.mini.u.d(
-
-  v16i8_r = __builtin_msa_mod_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.mod.s.b(
-  v8i16_r = __builtin_msa_mod_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mod.s.h(
-  v4i32_r = __builtin_msa_mod_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mod.s.w(
-  v2i64_r = __builtin_msa_mod_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.mod.s.d(
-
-  v16u8_r = __builtin_msa_mod_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.mod.u.b(
-  v8u16_r = __builtin_msa_mod_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.mod.u.h(
-  v4u32_r = __builtin_msa_mod_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.mod.u.w(
-  v2u64_r = __builtin_msa_mod_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.mod.u.d(
-
-  v16i8_r = __builtin_msa_move_v(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.move.v(
-
-  v8i16_r = __builtin_msa_msub_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.msub.q.h(
-  v4i32_r = __builtin_msa_msub_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.msub.q.w(
-
-  v8i16_r = __builtin_msa_msubr_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.msubr.q.h(
-  v4i32_r = __builtin_msa_msubr_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.msubr.q.w(
-
-  v16i8_r = __builtin_msa_msubv_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.msubv.b(
-  v8i16_r = __builtin_msa_msubv_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.msubv.h(
-  v4i32_r = __builtin_msa_msubv_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.msubv.w(
-  v2i64_r = __builtin_msa_msubv_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.msubv.d(
-
-  v8i16_r = __builtin_msa_mul_q_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mul.q.h(
-  v4i32_r = __builtin_msa_mul_q_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mul.q.w(
-
-  v8i16_r = __builtin_msa_mulr_q_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mulr.q.h(
-  v4i32_r = __builtin_msa_mulr_q_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mulr.q.w(
-
-  v16i8_r = __builtin_msa_mulv_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.mulv.b(
-  v8i16_r = __builtin_msa_mulv_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mulv.h(
-  v4i32_r = __builtin_msa_mulv_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mulv.w(
-  v2i64_r = __builtin_msa_mulv_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.mulv.d(
-
-  v16i8_r = __builtin_msa_nloc_b(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.nloc.b(
-  v8i16_r = __builtin_msa_nloc_h(v8i16_a); // CHECK: call <8  x i16> @llvm.mips.nloc.h(
-  v4i32_r = __builtin_msa_nloc_w(v4i32_a); // CHECK: call <4  x i32> @llvm.mips.nloc.w(
-  v2i64_r = __builtin_msa_nloc_d(v2i64_a); // CHECK: call <2  x i64> @llvm.mips.nloc.d(
-
-  v16i8_r = __builtin_msa_nlzc_b(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.nlzc.b(
-  v8i16_r = __builtin_msa_nlzc_h(v8i16_a); // CHECK: call <8  x i16> @llvm.mips.nlzc.h(
-  v4i32_r = __builtin_msa_nlzc_w(v4i32_a); // CHECK: call <4  x i32> @llvm.mips.nlzc.w(
-  v2i64_r = __builtin_msa_nlzc_d(v2i64_a); // CHECK: call <2  x i64> @llvm.mips.nlzc.d(
-
-  v16i8_r = __builtin_msa_nor_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
-  v8i16_r = __builtin_msa_nor_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
-  v4i32_r = __builtin_msa_nor_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
-  v2i64_r = __builtin_msa_nor_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
-
-  v16i8_r = __builtin_msa_nori_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-  v8i16_r = __builtin_msa_nori_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-  v4i32_r = __builtin_msa_nori_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-  v2i64_r = __builtin_msa_nori_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-
-  v16u8_r = __builtin_msa_nori_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-  v8u16_r = __builtin_msa_nori_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-  v4u32_r = __builtin_msa_nori_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-  v2u64_r = __builtin_msa_nori_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
-
-  v16i8_r = __builtin_msa_or_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
-  v8i16_r = __builtin_msa_or_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
-  v4i32_r = __builtin_msa_or_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
-  v2i64_r = __builtin_msa_or_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
-
-  v16i8_r = __builtin_msa_ori_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-  v8i16_r = __builtin_msa_ori_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-  v4i32_r = __builtin_msa_ori_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-  v2i64_r = __builtin_msa_ori_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-
-  v16u8_r = __builtin_msa_ori_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-  v8u16_r = __builtin_msa_ori_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-  v4u32_r = __builtin_msa_ori_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-  v2u64_r = __builtin_msa_ori_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
-
-  v16i8_r = __builtin_msa_pckev_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.pckev.b(
-  v8i16_r = __builtin_msa_pckev_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.pckev.h(
-  v4i32_r = __builtin_msa_pckev_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.pckev.w(
-  v2i64_r = __builtin_msa_pckev_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.pckev.d(
-
-  v16i8_r = __builtin_msa_pckod_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.pckod.b(
-  v8i16_r = __builtin_msa_pckod_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.pckod.h(
-  v4i32_r = __builtin_msa_pckod_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.pckod.w(
-  v2i64_r = __builtin_msa_pckod_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.pckod.d(
-
-  v16i8_r = __builtin_msa_pcnt_b(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.pcnt.b(
-  v8i16_r = __builtin_msa_pcnt_h(v8i16_a); // CHECK: call <8  x i16> @llvm.mips.pcnt.h(
-  v4i32_r = __builtin_msa_pcnt_w(v4i32_a); // CHECK: call <4  x i32> @llvm.mips.pcnt.w(
-  v2i64_r = __builtin_msa_pcnt_d(v2i64_a); // CHECK: call <2  x i64> @llvm.mips.pcnt.d(
-
-  v16i8_r = __builtin_msa_sat_s_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.sat.s.b(
-  v8i16_r = __builtin_msa_sat_s_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.sat.s.h(
-  v4i32_r = __builtin_msa_sat_s_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.sat.s.w(
-  v2i64_r = __builtin_msa_sat_s_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.sat.s.d(
-
-  v16i8_r = __builtin_msa_sat_u_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.sat.u.b(
-  v8i16_r = __builtin_msa_sat_u_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.sat.u.h(
-  v4i32_r = __builtin_msa_sat_u_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.sat.u.w(
-  v2i64_r = __builtin_msa_sat_u_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.sat.u.d(
-
-  v16i8_r = __builtin_msa_shf_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.shf.b(
-  v8i16_r = __builtin_msa_shf_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.shf.h(
-  v4i32_r = __builtin_msa_shf_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.shf.w(
-
-  v16i8_r = __builtin_msa_sld_b(v16i8_r, v16i8_a, 10); // CHECK: call <16 x i8>  @llvm.mips.sld.b(
-  v8i16_r = __builtin_msa_sld_h(v8i16_r, v8i16_a, 10); // CHECK: call <8  x i16> @llvm.mips.sld.h(
-  v4i32_r = __builtin_msa_sld_w(v4i32_r, v4i32_a, 10); // CHECK: call <4  x i32> @llvm.mips.sld.w(
-  v2i64_r = __builtin_msa_sld_d(v2i64_r, v2i64_a, 10); // CHECK: call <2  x i64> @llvm.mips.sld.d(
-
-  v16i8_r = __builtin_msa_sldi_b(v16i8_r, v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.sldi.b(
-  v8i16_r = __builtin_msa_sldi_h(v8i16_r, v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.sldi.h(
-  v4i32_r = __builtin_msa_sldi_w(v4i32_r, v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.sldi.w(
-  v2i64_r = __builtin_msa_sldi_d(v2i64_r, v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.sldi.d(
-
-  v16i8_r = __builtin_msa_sll_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.sll.b(
-  v8i16_r = __builtin_msa_sll_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.sll.h(
-  v4i32_r = __builtin_msa_sll_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.sll.w(
-  v2i64_r = __builtin_msa_sll_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.sll.d(
-
-  v16i8_r = __builtin_msa_slli_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.slli.b(
-  v8i16_r = __builtin_msa_slli_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.slli.h(
-  v4i32_r = __builtin_msa_slli_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.slli.w(
-  v2i64_r = __builtin_msa_slli_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.slli.d(
-
-  v16i8_r = __builtin_msa_splat_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.splat.b(
-  v8i16_r = __builtin_msa_splat_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.splat.h(
-  v4i32_r = __builtin_msa_splat_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.splat.w(
-  v2i64_r = __builtin_msa_splat_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.splat.d(
-
-  v16i8_r = __builtin_msa_splati_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.splati.b(
-  v8i16_r = __builtin_msa_splati_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.splati.h(
-  v4i32_r = __builtin_msa_splati_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.splati.w(
-  v2i64_r = __builtin_msa_splati_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.splati.d(
-
-  v16i8_r = __builtin_msa_sra_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.sra.b(
-  v8i16_r = __builtin_msa_sra_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.sra.h(
-  v4i32_r = __builtin_msa_sra_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.sra.w(
-  v2i64_r = __builtin_msa_sra_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.sra.d(
-
-  v16i8_r = __builtin_msa_srai_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srai.b(
-  v8i16_r = __builtin_msa_srai_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srai.h(
-  v4i32_r = __builtin_msa_srai_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srai.w(
-  v2i64_r = __builtin_msa_srai_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srai.d(
-
-  v16i8_r = __builtin_msa_srar_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.srar.b(
-  v8i16_r = __builtin_msa_srar_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.srar.h(
-  v4i32_r = __builtin_msa_srar_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.srar.w(
-  v2i64_r = __builtin_msa_srar_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.srar.d(
-
-  v16i8_r = __builtin_msa_srari_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srari.b(
-  v8i16_r = __builtin_msa_srari_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srari.h(
-  v4i32_r = __builtin_msa_srari_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srari.w(
-  v2i64_r = __builtin_msa_srari_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srari.d(
-
-  v16i8_r = __builtin_msa_srl_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.srl.b(
-  v8i16_r = __builtin_msa_srl_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.srl.h(
-  v4i32_r = __builtin_msa_srl_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.srl.w(
-  v2i64_r = __builtin_msa_srl_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.srl.d(
-
-  v16i8_r = __builtin_msa_srli_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srli.b(
-  v8i16_r = __builtin_msa_srli_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srli.h(
-  v4i32_r = __builtin_msa_srli_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srli.w(
-  v2i64_r = __builtin_msa_srli_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srli.d(
-
-  v16i8_r = __builtin_msa_srlr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.srlr.b(
-  v8i16_r = __builtin_msa_srlr_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.srlr.h(
-  v4i32_r = __builtin_msa_srlr_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.srlr.w(
-  v2i64_r = __builtin_msa_srlr_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.srlr.d(
-
-  v16i8_r = __builtin_msa_srlri_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srlri.b(
-  v8i16_r = __builtin_msa_srlri_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srlri.h(
-  v4i32_r = __builtin_msa_srlri_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srlri.w(
-  v2i64_r = __builtin_msa_srlri_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srlri.d(
-
-  __builtin_msa_st_b(v16i8_b, &v16i8_a, 1); // CHECK: call void @llvm.mips.st.b(
-  __builtin_msa_st_h(v8i16_b, &v8i16_a, 2); // CHECK: call void @llvm.mips.st.h(
-  __builtin_msa_st_w(v4i32_b, &v4i32_a, 4); // CHECK: call void @llvm.mips.st.w(
-  __builtin_msa_st_d(v2i64_b, &v2i64_a, 8); // CHECK: call void @llvm.mips.st.d(
-
-  v16i8_r = __builtin_msa_subs_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.subs.s.b(
-  v8i16_r = __builtin_msa_subs_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.subs.s.h(
-  v4i32_r = __builtin_msa_subs_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.subs.s.w(
-  v2i64_r = __builtin_msa_subs_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.subs.s.d(
-
-  v16u8_r = __builtin_msa_subs_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.subs.u.b(
-  v8u16_r = __builtin_msa_subs_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.subs.u.h(
-  v4u32_r = __builtin_msa_subs_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.subs.u.w(
-  v2u64_r = __builtin_msa_subs_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.subs.u.d(
-
-  v16u8_r = __builtin_msa_subsus_u_b(v16u8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.subsus.u.b(
-  v8u16_r = __builtin_msa_subsus_u_h(v8u16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.subsus.u.h(
-  v4u32_r = __builtin_msa_subsus_u_w(v4u32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.subsus.u.w(
-  v2u64_r = __builtin_msa_subsus_u_d(v2u64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.subsus.u.d(
-
-  v16i8_r = __builtin_msa_subsuu_s_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.subsuu.s.b(
-  v8i16_r = __builtin_msa_subsuu_s_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.subsuu.s.h(
-  v4i32_r = __builtin_msa_subsuu_s_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.subsuu.s.w(
-  v2i64_r = __builtin_msa_subsuu_s_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.subsuu.s.d(
-
-  v16i8_r = __builtin_msa_subv_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.subv.b(
-  v8i16_r = __builtin_msa_subv_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.subv.h(
-  v4i32_r = __builtin_msa_subv_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.subv.w(
-  v2i64_r = __builtin_msa_subv_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.subv.d(
-
-  v16i8_r = __builtin_msa_subvi_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.subvi.b(
-  v8i16_r = __builtin_msa_subvi_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.subvi.h(
-  v4i32_r = __builtin_msa_subvi_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.subvi.w(
-  v2i64_r = __builtin_msa_subvi_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.subvi.d(
-
-  v16i8_r = __builtin_msa_vshf_b(v16i8_a, v16i8_b, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.vshf.b(
-  v8i16_r = __builtin_msa_vshf_h(v8i16_a, v8i16_b, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.vshf.h(
-  v4i32_r = __builtin_msa_vshf_w(v4i32_a, v4i32_b, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.vshf.w(
-  v2i64_r = __builtin_msa_vshf_d(v2i64_a, v2i64_b, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.vshf.d(
-
-  v16i8_r = __builtin_msa_xor_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
-  v8i16_r = __builtin_msa_xor_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
-  v4i32_r = __builtin_msa_xor_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
-  v2i64_r = __builtin_msa_xor_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
-
-  v16i8_r = __builtin_msa_xori_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
-  v8i16_r = __builtin_msa_xori_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
-  v4i32_r = __builtin_msa_xori_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
-  v2i64_r = __builtin_msa_xori_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
-
-  v16u8_r = __builtin_msa_xori_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
-  v8u16_r = __builtin_msa_xori_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
-  v4u32_r = __builtin_msa_xori_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
-  v2u64_r = __builtin_msa_xori_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+  v8i16_r = __msa_madd_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.madd.q.h(
+  v4i32_r = __msa_madd_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.madd.q.w(
+
+  v8i16_r = __msa_maddr_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.maddr.q.h(
+  v4i32_r = __msa_maddr_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.maddr.q.w(
+
+  v16i8_r = __msa_maddv_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.maddv.b(
+  v8i16_r = __msa_maddv_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.maddv.h(
+  v4i32_r = __msa_maddv_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.maddv.w(
+  v2i64_r = __msa_maddv_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.maddv.d(
+
+  v16i8_r = __msa_max_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.max.a.b(
+  v8i16_r = __msa_max_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.max.a.h(
+  v4i32_r = __msa_max_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.max.a.w(
+  v2i64_r = __msa_max_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.max.a.d(
+
+  v16i8_r = __msa_max_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.max.s.b(
+  v8i16_r = __msa_max_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.max.s.h(
+  v4i32_r = __msa_max_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.max.s.w(
+  v2i64_r = __msa_max_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.max.s.d(
+
+  v16u8_r = __msa_max_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.max.u.b(
+  v8u16_r = __msa_max_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.max.u.h(
+  v4u32_r = __msa_max_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.max.u.w(
+  v2u64_r = __msa_max_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.max.u.d(
+
+  v16i8_r = __msa_maxi_s_b(v16i8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.maxi.s.b(
+  v8i16_r = __msa_maxi_s_h(v8i16_a, 2); // CHECK: call <8  x i16> @llvm.mips.maxi.s.h(
+  v4i32_r = __msa_maxi_s_w(v4i32_a, 2); // CHECK: call <4  x i32> @llvm.mips.maxi.s.w(
+  v2i64_r = __msa_maxi_s_d(v2i64_a, 2); // CHECK: call <2  x i64> @llvm.mips.maxi.s.d(
+
+  v16u8_r = __msa_maxi_u_b(v16u8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.maxi.u.b(
+  v8u16_r = __msa_maxi_u_h(v8u16_a, 2); // CHECK: call <8  x i16> @llvm.mips.maxi.u.h(
+  v4u32_r = __msa_maxi_u_w(v4u32_a, 2); // CHECK: call <4  x i32> @llvm.mips.maxi.u.w(
+  v2u64_r = __msa_maxi_u_d(v2u64_a, 2); // CHECK: call <2  x i64> @llvm.mips.maxi.u.d(
+
+  v16i8_r = __msa_min_a_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.min.a.b(
+  v8i16_r = __msa_min_a_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.min.a.h(
+  v4i32_r = __msa_min_a_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.min.a.w(
+  v2i64_r = __msa_min_a_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.min.a.d(
+
+  v16i8_r = __msa_min_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.min.s.b(
+  v8i16_r = __msa_min_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.min.s.h(
+  v4i32_r = __msa_min_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.min.s.w(
+  v2i64_r = __msa_min_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.min.s.d(
+
+  v16u8_r = __msa_min_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.min.u.b(
+  v8u16_r = __msa_min_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.min.u.h(
+  v4u32_r = __msa_min_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.min.u.w(
+  v2u64_r = __msa_min_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.min.u.d(
+
+  v16i8_r = __msa_mini_s_b(v16i8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.mini.s.b(
+  v8i16_r = __msa_mini_s_h(v8i16_a, 2); // CHECK: call <8  x i16> @llvm.mips.mini.s.h(
+  v4i32_r = __msa_mini_s_w(v4i32_a, 2); // CHECK: call <4  x i32> @llvm.mips.mini.s.w(
+  v2i64_r = __msa_mini_s_d(v2i64_a, 2); // CHECK: call <2  x i64> @llvm.mips.mini.s.d(
+
+  v16u8_r = __msa_mini_u_b(v16u8_a, 2); // CHECK: call <16 x i8>  @llvm.mips.mini.u.b(
+  v8u16_r = __msa_mini_u_h(v8u16_a, 2); // CHECK: call <8  x i16> @llvm.mips.mini.u.h(
+  v4u32_r = __msa_mini_u_w(v4u32_a, 2); // CHECK: call <4  x i32> @llvm.mips.mini.u.w(
+  v2u64_r = __msa_mini_u_d(v2u64_a, 2); // CHECK: call <2  x i64> @llvm.mips.mini.u.d(
+
+  v16i8_r = __msa_mod_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.mod.s.b(
+  v8i16_r = __msa_mod_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mod.s.h(
+  v4i32_r = __msa_mod_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mod.s.w(
+  v2i64_r = __msa_mod_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.mod.s.d(
+
+  v16u8_r = __msa_mod_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.mod.u.b(
+  v8u16_r = __msa_mod_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.mod.u.h(
+  v4u32_r = __msa_mod_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.mod.u.w(
+  v2u64_r = __msa_mod_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.mod.u.d(
+
+  v16i8_r = __msa_move_v(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.move.v(
+
+  v8i16_r = __msa_msub_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.msub.q.h(
+  v4i32_r = __msa_msub_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.msub.q.w(
+
+  v8i16_r = __msa_msubr_q_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.msubr.q.h(
+  v4i32_r = __msa_msubr_q_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.msubr.q.w(
+
+  v16i8_r = __msa_msubv_b(v16i8_r, v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.msubv.b(
+  v8i16_r = __msa_msubv_h(v8i16_r, v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.msubv.h(
+  v4i32_r = __msa_msubv_w(v4i32_r, v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.msubv.w(
+  v2i64_r = __msa_msubv_d(v2i64_r, v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.msubv.d(
+
+  v8i16_r = __msa_mul_q_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mul.q.h(
+  v4i32_r = __msa_mul_q_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mul.q.w(
+
+  v8i16_r = __msa_mulr_q_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mulr.q.h(
+  v4i32_r = __msa_mulr_q_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mulr.q.w(
+
+  v16i8_r = __msa_mulv_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.mulv.b(
+  v8i16_r = __msa_mulv_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.mulv.h(
+  v4i32_r = __msa_mulv_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.mulv.w(
+  v2i64_r = __msa_mulv_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.mulv.d(
+
+  v16i8_r = __msa_nloc_b(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.nloc.b(
+  v8i16_r = __msa_nloc_h(v8i16_a); // CHECK: call <8  x i16> @llvm.mips.nloc.h(
+  v4i32_r = __msa_nloc_w(v4i32_a); // CHECK: call <4  x i32> @llvm.mips.nloc.w(
+  v2i64_r = __msa_nloc_d(v2i64_a); // CHECK: call <2  x i64> @llvm.mips.nloc.d(
+
+  v16i8_r = __msa_nlzc_b(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.nlzc.b(
+  v8i16_r = __msa_nlzc_h(v8i16_a); // CHECK: call <8  x i16> @llvm.mips.nlzc.h(
+  v4i32_r = __msa_nlzc_w(v4i32_a); // CHECK: call <4  x i32> @llvm.mips.nlzc.w(
+  v2i64_r = __msa_nlzc_d(v2i64_a); // CHECK: call <2  x i64> @llvm.mips.nlzc.d(
+
+  v16i8_r = __msa_nor_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
+  v8i16_r = __msa_nor_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
+  v4i32_r = __msa_nor_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
+  v2i64_r = __msa_nor_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.nor.v(
+
+  v16i8_r = __msa_nori_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+  v8i16_r = __msa_nori_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+  v4i32_r = __msa_nori_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+  v2i64_r = __msa_nori_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+
+  v16u8_r = __msa_nori_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+  v8u16_r = __msa_nori_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+  v4u32_r = __msa_nori_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+  v2u64_r = __msa_nori_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.nori.b(
+
+  v16i8_r = __msa_or_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
+  v8i16_r = __msa_or_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
+  v4i32_r = __msa_or_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
+  v2i64_r = __msa_or_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.or.v(
+
+  v16i8_r = __msa_ori_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+  v8i16_r = __msa_ori_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+  v4i32_r = __msa_ori_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+  v2i64_r = __msa_ori_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+
+  v16u8_r = __msa_ori_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+  v8u16_r = __msa_ori_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+  v4u32_r = __msa_ori_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+  v2u64_r = __msa_ori_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.ori.b(
+
+  v16i8_r = __msa_pckev_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.pckev.b(
+  v8i16_r = __msa_pckev_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.pckev.h(
+  v4i32_r = __msa_pckev_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.pckev.w(
+  v2i64_r = __msa_pckev_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.pckev.d(
+
+  v16i8_r = __msa_pckod_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.pckod.b(
+  v8i16_r = __msa_pckod_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.pckod.h(
+  v4i32_r = __msa_pckod_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.pckod.w(
+  v2i64_r = __msa_pckod_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.pckod.d(
+
+  v16i8_r = __msa_pcnt_b(v16i8_a); // CHECK: call <16 x i8>  @llvm.mips.pcnt.b(
+  v8i16_r = __msa_pcnt_h(v8i16_a); // CHECK: call <8  x i16> @llvm.mips.pcnt.h(
+  v4i32_r = __msa_pcnt_w(v4i32_a); // CHECK: call <4  x i32> @llvm.mips.pcnt.w(
+  v2i64_r = __msa_pcnt_d(v2i64_a); // CHECK: call <2  x i64> @llvm.mips.pcnt.d(
+
+  v16i8_r = __msa_sat_s_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.sat.s.b(
+  v8i16_r = __msa_sat_s_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.sat.s.h(
+  v4i32_r = __msa_sat_s_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.sat.s.w(
+  v2i64_r = __msa_sat_s_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.sat.s.d(
+
+  v16i8_r = __msa_sat_u_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.sat.u.b(
+  v8i16_r = __msa_sat_u_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.sat.u.h(
+  v4i32_r = __msa_sat_u_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.sat.u.w(
+  v2i64_r = __msa_sat_u_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.sat.u.d(
+
+  v16i8_r = __msa_shf_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.shf.b(
+  v8i16_r = __msa_shf_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.shf.h(
+  v4i32_r = __msa_shf_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.shf.w(
+
+  v16i8_r = __msa_sld_b(v16i8_r, v16i8_a, 7); // CHECK: call <16 x i8>  @llvm.mips.sld.b(
+  v8i16_r = __msa_sld_h(v8i16_r, v8i16_a, 5); // CHECK: call <8  x i16> @llvm.mips.sld.h(
+  v4i32_r = __msa_sld_w(v4i32_r, v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.sld.w(
+  v2i64_r = __msa_sld_d(v2i64_r, v2i64_a, 1); // CHECK: call <2  x i64> @llvm.mips.sld.d(
+
+  v16i8_r = __msa_sldi_b(v16i8_r, v16i8_a, 7); // CHECK: call <16 x i8>  @llvm.mips.sldi.b(
+  v8i16_r = __msa_sldi_h(v8i16_r, v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.sldi.h(
+  v4i32_r = __msa_sldi_w(v4i32_r, v4i32_a, 2); // CHECK: call <4  x i32> @llvm.mips.sldi.w(
+  v2i64_r = __msa_sldi_d(v2i64_r, v2i64_a, 1); // CHECK: call <2  x i64> @llvm.mips.sldi.d(
+
+  v16i8_r = __msa_sll_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.sll.b(
+  v8i16_r = __msa_sll_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.sll.h(
+  v4i32_r = __msa_sll_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.sll.w(
+  v2i64_r = __msa_sll_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.sll.d(
+
+  v16i8_r = __msa_slli_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.slli.b(
+  v8i16_r = __msa_slli_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.slli.h(
+  v4i32_r = __msa_slli_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.slli.w(
+  v2i64_r = __msa_slli_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.slli.d(
+
+  v16i8_r = __msa_splat_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.splat.b(
+  v8i16_r = __msa_splat_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.splat.h(
+  v4i32_r = __msa_splat_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.splat.w(
+  v2i64_r = __msa_splat_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.splat.d(
+
+  v16i8_r = __msa_splati_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.splati.b(
+  v8i16_r = __msa_splati_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.splati.h(
+  v4i32_r = __msa_splati_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.splati.w(
+  v2i64_r = __msa_splati_d(v2i64_a, 1); // CHECK: call <2  x i64> @llvm.mips.splati.d(
+
+  v16i8_r = __msa_sra_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.sra.b(
+  v8i16_r = __msa_sra_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.sra.h(
+  v4i32_r = __msa_sra_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.sra.w(
+  v2i64_r = __msa_sra_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.sra.d(
+
+  v16i8_r = __msa_srai_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srai.b(
+  v8i16_r = __msa_srai_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srai.h(
+  v4i32_r = __msa_srai_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srai.w(
+  v2i64_r = __msa_srai_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srai.d(
+
+  v16i8_r = __msa_srar_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.srar.b(
+  v8i16_r = __msa_srar_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.srar.h(
+  v4i32_r = __msa_srar_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.srar.w(
+  v2i64_r = __msa_srar_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.srar.d(
+
+  v16i8_r = __msa_srari_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srari.b(
+  v8i16_r = __msa_srari_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srari.h(
+  v4i32_r = __msa_srari_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srari.w(
+  v2i64_r = __msa_srari_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srari.d(
+
+  v16i8_r = __msa_srl_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.srl.b(
+  v8i16_r = __msa_srl_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.srl.h(
+  v4i32_r = __msa_srl_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.srl.w(
+  v2i64_r = __msa_srl_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.srl.d(
+
+  v16i8_r = __msa_srli_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srli.b(
+  v8i16_r = __msa_srli_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srli.h(
+  v4i32_r = __msa_srli_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srli.w(
+  v2i64_r = __msa_srli_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srli.d(
+
+  v16i8_r = __msa_srlr_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.srlr.b(
+  v8i16_r = __msa_srlr_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.srlr.h(
+  v4i32_r = __msa_srlr_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.srlr.w(
+  v2i64_r = __msa_srlr_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.srlr.d(
+
+  v16i8_r = __msa_srlri_b(v16i8_a, 3); // CHECK: call <16 x i8>  @llvm.mips.srlri.b(
+  v8i16_r = __msa_srlri_h(v8i16_a, 3); // CHECK: call <8  x i16> @llvm.mips.srlri.h(
+  v4i32_r = __msa_srlri_w(v4i32_a, 3); // CHECK: call <4  x i32> @llvm.mips.srlri.w(
+  v2i64_r = __msa_srlri_d(v2i64_a, 3); // CHECK: call <2  x i64> @llvm.mips.srlri.d(
+
+  __msa_st_b(v16i8_b, &v16i8_a, 16); // CHECK: call void @llvm.mips.st.b(
+  __msa_st_h(v8i16_b, &v8i16_a, 32); // CHECK: call void @llvm.mips.st.h(
+  __msa_st_w(v4i32_b, &v4i32_a, 48); // CHECK: call void @llvm.mips.st.w(
+  __msa_st_d(v2i64_b, &v2i64_a, 96); // CHECK: call void @llvm.mips.st.d(
+
+  v16i8_r = __msa_subs_s_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.subs.s.b(
+  v8i16_r = __msa_subs_s_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.subs.s.h(
+  v4i32_r = __msa_subs_s_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.subs.s.w(
+  v2i64_r = __msa_subs_s_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.subs.s.d(
+
+  v16u8_r = __msa_subs_u_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.subs.u.b(
+  v8u16_r = __msa_subs_u_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.subs.u.h(
+  v4u32_r = __msa_subs_u_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.subs.u.w(
+  v2u64_r = __msa_subs_u_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.subs.u.d(
+
+  v16u8_r = __msa_subsus_u_b(v16u8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.subsus.u.b(
+  v8u16_r = __msa_subsus_u_h(v8u16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.subsus.u.h(
+  v4u32_r = __msa_subsus_u_w(v4u32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.subsus.u.w(
+  v2u64_r = __msa_subsus_u_d(v2u64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.subsus.u.d(
+
+  v16i8_r = __msa_subsuu_s_b(v16u8_a, v16u8_b); // CHECK: call <16 x i8>  @llvm.mips.subsuu.s.b(
+  v8i16_r = __msa_subsuu_s_h(v8u16_a, v8u16_b); // CHECK: call <8  x i16> @llvm.mips.subsuu.s.h(
+  v4i32_r = __msa_subsuu_s_w(v4u32_a, v4u32_b); // CHECK: call <4  x i32> @llvm.mips.subsuu.s.w(
+  v2i64_r = __msa_subsuu_s_d(v2u64_a, v2u64_b); // CHECK: call <2  x i64> @llvm.mips.subsuu.s.d(
+
+  v16i8_r = __msa_subv_b(v16i8_a, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.subv.b(
+  v8i16_r = __msa_subv_h(v8i16_a, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.subv.h(
+  v4i32_r = __msa_subv_w(v4i32_a, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.subv.w(
+  v2i64_r = __msa_subv_d(v2i64_a, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.subv.d(
+
+  v16i8_r = __msa_subvi_b(v16i8_a, 25); // CHECK: call <16 x i8>  @llvm.mips.subvi.b(
+  v8i16_r = __msa_subvi_h(v8i16_a, 25); // CHECK: call <8  x i16> @llvm.mips.subvi.h(
+  v4i32_r = __msa_subvi_w(v4i32_a, 25); // CHECK: call <4  x i32> @llvm.mips.subvi.w(
+  v2i64_r = __msa_subvi_d(v2i64_a, 25); // CHECK: call <2  x i64> @llvm.mips.subvi.d(
+
+  v16i8_r = __msa_vshf_b(v16i8_a, v16i8_b, v16i8_b); // CHECK: call <16 x i8>  @llvm.mips.vshf.b(
+  v8i16_r = __msa_vshf_h(v8i16_a, v8i16_b, v8i16_b); // CHECK: call <8  x i16> @llvm.mips.vshf.h(
+  v4i32_r = __msa_vshf_w(v4i32_a, v4i32_b, v4i32_b); // CHECK: call <4  x i32> @llvm.mips.vshf.w(
+  v2i64_r = __msa_vshf_d(v2i64_a, v2i64_b, v2i64_b); // CHECK: call <2  x i64> @llvm.mips.vshf.d(
+
+  v16i8_r = __msa_xor_v(v16i8_a, v16i8_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
+  v8i16_r = __msa_xor_v(v8i16_a, v8i16_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
+  v4i32_r = __msa_xor_v(v4i32_a, v4i32_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
+  v2i64_r = __msa_xor_v(v2i64_a, v2i64_b); // CHECK: call <16 x i8> @llvm.mips.xor.v(
+
+  v16i8_r = __msa_xori_b(v16i8_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+  v8i16_r = __msa_xori_b(v8i16_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+  v4i32_r = __msa_xori_b(v4i32_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+  v2i64_r = __msa_xori_b(v2i64_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+
+  v16u8_r = __msa_xori_b(v16u8_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+  v8u16_r = __msa_xori_b(v8u16_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+  v4u32_r = __msa_xori_b(v4u32_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
+  v2u64_r = __msa_xori_b(v2u64_a, 25); // CHECK: call <16 x i8> @llvm.mips.xori.b(
 
 }
diff --git a/test/CodeGenCXX/PR28523.cpp b/test/CodeGenCXX/PR28523.cpp
new file mode 100644
index 000000000000..4c3a81c8b854
--- /dev/null
+++ b/test/CodeGenCXX/PR28523.cpp
@@ -0,0 +1,19 @@
+// RUN: %clang_cc1 -std=c++14 -verify -triple %itanium_abi_triple -emit-llvm %s -o - | FileCheck %s
+// expected-no-diagnostics
+
+template <class F> void parallel_loop(F &&f) { f(0); }
+
+//CHECK-LABEL: @main
+int main() {
+// CHECK: [[X_ADDR:%.+]] = alloca i32,
+  int x;
+// CHECK: getelementptr inbounds
+// CHECK: store i32* [[X_ADDR]], i32** %
+// CHECK: call
+  parallel_loop([&](auto y) {
+#pragma clang __debug captured
+    {
+      x = y;
+    };
+  });
+}
diff --git a/test/CodeGenCXX/captured-statements.cpp b/test/CodeGenCXX/captured-statements.cpp
index fdda24fcf301..4b95503ad7b3 100644
--- a/test/CodeGenCXX/captured-statements.cpp
+++ b/test/CodeGenCXX/captured-statements.cpp
@@ -78,6 +78,7 @@ void test3(int x) {
   {
     x = [=]() { return x + 1; } ();
   }
+  x = [=]() { return x + 1; }();
 
   // CHECK-3: %[[Capture:struct\.anon[\.0-9]*]] = type { i32* }
 
diff --git a/test/CodeGenCXX/switch-case-folding-2.cpp b/test/CodeGenCXX/switch-case-folding-2.cpp
index 558ca3c87d9b..cfb8447ee326 100644
--- a/test/CodeGenCXX/switch-case-folding-2.cpp
+++ b/test/CodeGenCXX/switch-case-folding-2.cpp
@@ -1,12 +1,14 @@
 // RUN: %clang_cc1 -triple i386-unknown-unknown -emit-llvm %s -o - | FileCheck %s
-// CHECK that we don't crash.
 
 extern int printf(const char*, ...);
+
+// CHECK-LABEL: @_Z4testi(
 int test(int val){
  switch (val) {
  case 4:
    do {
      switch (6) {
+       // CHECK: call i32 (i8*, ...) @_Z6printfPKcz
        case 6: do { case 5: printf("bad\n"); } while (0);
      };
    } while (0);
@@ -18,6 +20,7 @@ int main(void) {
  return test(5);
 }
 
+// CHECK-LABEL: @_Z10other_testv(
 void other_test() {
   switch(0) {
   case 0:
@@ -27,4 +30,79 @@ void other_test() {
   }
 }
 
-// CHECK: call i32 (i8*, ...) @_Z6printfPKcz
+struct X { X(); ~X(); };
+
+void dont_call();
+void foo();
+
+// CHECK-LABEL: @_Z13nested_scopesv(
+void nested_scopes() {
+  switch (1) {
+  case 0:
+    // CHECK-NOT: @_Z9dont_callv(
+    dont_call();
+    break;
+
+  default:
+    // CHECK: call {{.*}} @_ZN1XC1Ev(
+    // CHECK: call {{.*}} @_Z3foov(
+    // CHECK-NOT: call {{.*}} @_Z3foov(
+    // CHECK: call {{.*}} @_ZN1XD1Ev(
+    { X x; foo(); }
+
+    // CHECK: call {{.*}} @_ZN1XC1Ev(
+    // CHECK: call {{.*}} @_Z3foov(
+    // CHECK: call {{.*}} @_ZN1XD1Ev(
+    { X x; foo(); }
+
+    // CHECK: call {{.*}} @_ZN1XC1Ev(
+    // CHECK: call {{.*}} @_Z3foov(
+    // CHECK: call {{.*}} @_ZN1XD1Ev(
+    { X x; foo(); }
+    break;
+  }
+}
+
+// CHECK-LABEL: @_Z17scope_fallthroughv(
+void scope_fallthrough() {
+  switch (1) {
+    // CHECK: call {{.*}} @_ZN1XC1Ev(
+    // CHECK-NOT: call {{.*}} @_Z3foov(
+    // CHECK: call {{.*}} @_ZN1XD1Ev(
+    { default: X x; }
+    // CHECK: call {{.*}} @_Z3foov(
+    foo();
+    break;
+  }
+}
+
+// CHECK-LABEL: @_Z12hidden_breakb(
+void hidden_break(bool b) {
+  switch (1) {
+  default:
+    // CHECK: br
+    if (b)
+      break;
+    // CHECK: call {{.*}} @_Z3foov(
+    foo();
+    break;
+  }
+}
+
+// CHECK-LABEL: @_Z10hidden_varv(
+int hidden_var() {
+  switch (1) {
+  // CHECK: %[[N:.*]] = alloca i32
+  case 0: int n;
+  // CHECK: store i32 0, i32* %[[N]]
+  // CHECK: load i32, i32* %[[N]]
+  // CHECK: ret
+  default: n = 0; return n;
+  }
+}
+
+// CHECK-LABEL: @_Z13case_in_labelv(
+void case_in_label() {
+  // CHECK: br label %
+  switch (1) case 1: foo: case 0: goto foo;
+}
diff --git a/test/CodeGenOpenCL/kernel-arg-info-single-as.cl b/test/CodeGenOpenCL/kernel-arg-info-single-as.cl
new file mode 100644
index 000000000000..595c97464e3a
--- /dev/null
+++ b/test/CodeGenOpenCL/kernel-arg-info-single-as.cl
@@ -0,0 +1,9 @@
+// Test that the kernel argument info always refers to SPIR address spaces,
+// even if the target has only one address space like x86_64 does.
+// RUN: %clang_cc1 %s -cl-std=CL2.0 -emit-llvm -o - -triple x86_64-unknown-unknown -cl-kernel-arg-info | FileCheck %s
+
+kernel void foo(__global int * G, __constant int *C, __local int *L) {
+  *G = *C + *L;
+}
+// CHECK: !kernel_arg_addr_space ![[MD123:[0-9]+]]
+// CHECK: ![[MD123]] = !{i32 1, i32 2, i32 3}
diff --git a/test/Driver/darwin-ld-lto.c b/test/Driver/darwin-ld-lto.c
index 23e006a01850..6b9b79bd011a 100644
--- a/test/Driver/darwin-ld-lto.c
+++ b/test/Driver/darwin-ld-lto.c
@@ -1,25 +1,16 @@
 // REQUIRES: system-darwin
 
-// Check that ld gets "-lto_library" and warnings about libLTO.dylib path.
+// Check that ld gets "-lto_library".
 
 // RUN: %clang -target x86_64-apple-darwin10 -### %s \
-// RUN:   -mlinker-version=133 -flto 2> %t.log
-// RUN: cat %t.log
-// RUN: FileCheck -check-prefix=LINK_LTOLIB_PATH %s < %t.log
+// RUN:   -ccc-install-dir %T/bin -mlinker-version=133 2> %t.log
+// RUN: FileCheck -check-prefix=LINK_LTOLIB_PATH %s -input-file %t.log
 //
 // LINK_LTOLIB_PATH: {{ld(.exe)?"}}
 // LINK_LTOLIB_PATH: "-lto_library"
 
+// Also pass -lto_library even if the file doesn't exist; if it's needed at
+// link time, ld will complain instead.
 // RUN: %clang -target x86_64-apple-darwin10 -### %s \
-// RUN:   -ccc-install-dir %S/dummytestdir -mlinker-version=133 -flto 2> %t.log
-// RUN: cat %t.log
-// RUN: FileCheck -check-prefix=LINK_LTOLIB_PATH_WRN %s < %t.log
-//
-// LINK_LTOLIB_PATH_WRN: warning: libLTO.dylib relative to clang installed dir not found; using 'ld' default search path instead
-
-// RUN: %clang -target x86_64-apple-darwin10 -### %s \
-// RUN:   -ccc-install-dir %S/dummytestdir -mlinker-version=133 -Wno-liblto -flto 2> %t.log
-// RUN: cat %t.log
-// RUN: FileCheck -check-prefix=LINK_LTOLIB_PATH_NOWRN %s < %t.log
-//
-// LINK_LTOLIB_PATH_NOWRN-NOT: warning: libLTO.dylib relative to clang installed dir not found; using 'ld' default search path instead
+// RUN:   -ccc-install-dir %S/dummytestdir -mlinker-version=133 2> %t.log
+// RUN: FileCheck -check-prefix=LINK_LTOLIB_PATH %s -input-file %t.log
diff --git a/test/Frontend/darwin-version.c b/test/Frontend/darwin-version.c
index e7bc41117e3f..eb05a48cfd36 100644
--- a/test/Frontend/darwin-version.c
+++ b/test/Frontend/darwin-version.c
@@ -10,6 +10,8 @@
 // RUN: %clang_cc1 -triple armv6-apple-ios2.3.1 -dM -E -o %t %s
 // RUN: grep '__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__' %t | grep '20301' | count 1
 // RUN: not grep '__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__' %t
+// RUN: %clang_cc1 -triple armv7-apple-ios10.1.2 -dM -E -o %t %s
+// RUN: grep '__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__' %t | grep '100102' | count 1
 // RUN: %clang_cc1 -triple i386-apple-macosx10.4.0 -dM -E -o %t %s
 // RUN: grep '__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__' %t | grep '1040' | count 1
 // RUN: not grep '__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__' %t
@@ -32,6 +34,8 @@
 // RUN: grep '__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__' %t | grep '80300' | count 1
 // RUN: not grep '__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__' %t
 // RUN: not grep '__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__' %t
+// RUN: %clang_cc1 -triple arm64-apple-tvos10.2.3 -dM -E -o %t %s
+// RUN: grep '__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__' %t | grep '100203' | count 1
 
 // RUN: %clang_cc1 -triple x86_64-apple-tvos8.3 -dM -E -o %t %s
 // RUN: grep '__ENVIRONMENT_TV_OS_VERSION_MIN_REQUIRED__' %t | grep '80300' | count 1
diff --git a/test/Modules/Inputs/static_assert/a.h b/test/Modules/Inputs/static_assert/a.h
new file mode 100644
index 000000000000..e876df06d066
--- /dev/null
+++ b/test/Modules/Inputs/static_assert/a.h
@@ -0,0 +1,3 @@
+class S {
+  static_assert(4 == 4);
+};
diff --git a/test/Modules/Inputs/static_assert/module.modulemap b/test/Modules/Inputs/static_assert/module.modulemap
new file mode 100644
index 000000000000..bb00c840ce39
--- /dev/null
+++ b/test/Modules/Inputs/static_assert/module.modulemap
@@ -0,0 +1 @@
+module a { header "a.h" }
diff --git a/test/Modules/static_assert.cpp b/test/Modules/static_assert.cpp
new file mode 100644
index 000000000000..d1b759073cd5
--- /dev/null
+++ b/test/Modules/static_assert.cpp
@@ -0,0 +1,8 @@
+// RUN: rm -rf %t
+// RUN: %clang_cc1 -fmodules -fmodules-cache-path=%t -fimplicit-module-maps \
+// RUN:            -I%S/Inputs/static_assert -std=c++1z -verify %s
+// expected-no-diagnostics
+
+#include "a.h"
+
+S s;
diff --git a/test/OpenMP/atomic_write_codegen.c b/test/OpenMP/atomic_write_codegen.c
index 66172af07e8f..050d7a510566 100644
--- a/test/OpenMP/atomic_write_codegen.c
+++ b/test/OpenMP/atomic_write_codegen.c
@@ -78,6 +78,9 @@ float2 float2x;
 register int rix __asm__("esp");
 
 int main() {
+// CHECK: store atomic i32 1, i32* getelementptr inbounds ({ i32, i32 }, { i32, i32 }* @civ, i32 0, i32 1) monotonic,
+#pragma omp atomic write
+ __imag(civ) = 1;
 // CHECK: load i8, i8*
 // CHECK: store atomic i8
 #pragma omp atomic write
diff --git a/test/OpenMP/cancel_codegen.cpp b/test/OpenMP/cancel_codegen.cpp
index 8234193e8f95..a09214c9319c 100644
--- a/test/OpenMP/cancel_codegen.cpp
+++ b/test/OpenMP/cancel_codegen.cpp
@@ -91,9 +91,11 @@ for (int i = 0; i < argc; ++i) {
   }
 }
 // CHECK: call void (%ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(
-#pragma omp parallel for
+int r = 0;
+#pragma omp parallel for reduction(+: r)
 for (int i = 0; i < argc; ++i) {
 #pragma omp cancel for
+  r += i;
 }
 // CHECK: call void (%ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(
   return argc;
@@ -164,6 +166,9 @@ for (int i = 0; i < argc; ++i) {
 // CHECK: [[CONTINUE]]
 // CHECK: br label
 // CHECK: call void @__kmpc_for_static_fini(
+// CHECK: call i32 @__kmpc_reduce_nowait(
+// CHECK: call void @__kmpc_end_reduce_nowait(
+// CHECK: call void @__kmpc_for_static_fini(
 // CHECK: ret void
 
 #endif
diff --git a/test/OpenMP/debug-info-openmp-array.cpp b/test/OpenMP/debug-info-openmp-array.cpp
new file mode 100644
index 000000000000..19bf2a284ad6
--- /dev/null
+++ b/test/OpenMP/debug-info-openmp-array.cpp
@@ -0,0 +1,13 @@
+// RUN: %clang_cc1 -fopenmp -x c++ %s -verify -debug-info-kind=limited -emit-llvm -o - | FileCheck %s
+// expected-no-diagnostics
+
+void f(int m) {
+  int i;
+  int cen[m];
+#pragma omp parallel for
+  for (i = 0; i < m; ++i) {
+    cen[i] = i;
+  }
+}
+
+// CHECK: !DILocalVariable(name: "cen", arg: 6
diff --git a/test/OpenMP/distribute_parallel_for_reduction_messages.cpp b/test/OpenMP/distribute_parallel_for_reduction_messages.cpp
index f23a25e28c02..95654a9e5018 100644
--- a/test/OpenMP/distribute_parallel_for_reduction_messages.cpp
+++ b/test/OpenMP/distribute_parallel_for_reduction_messages.cpp
@@ -9,6 +9,14 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp target
+#pragma omp teams
+#pragma omp distribute parallel for reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/distribute_parallel_for_simd_reduction_messages.cpp b/test/OpenMP/distribute_parallel_for_simd_reduction_messages.cpp
index 7b7e9ea53ccb..6ad41d72bf78 100644
--- a/test/OpenMP/distribute_parallel_for_simd_reduction_messages.cpp
+++ b/test/OpenMP/distribute_parallel_for_simd_reduction_messages.cpp
@@ -9,6 +9,14 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp target
+#pragma omp teams
+#pragma omp distribute parallel for simd reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/distribute_simd_reduction_messages.cpp b/test/OpenMP/distribute_simd_reduction_messages.cpp
index e03b85292808..fca3e85a7104 100644
--- a/test/OpenMP/distribute_simd_reduction_messages.cpp
+++ b/test/OpenMP/distribute_simd_reduction_messages.cpp
@@ -9,6 +9,14 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp target
+#pragma omp teams
+#pragma omp distribute simd reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/for_lastprivate_codegen.cpp b/test/OpenMP/for_lastprivate_codegen.cpp
index 2b1d6c3cf97f..75f59f7d406d 100644
--- a/test/OpenMP/for_lastprivate_codegen.cpp
+++ b/test/OpenMP/for_lastprivate_codegen.cpp
@@ -41,7 +41,7 @@ struct SS {
     for (a = 0; a < 2; ++a)
 #ifdef LAMBDA
       [&]() {
-        ++this->a, --b, (this)->c /= 1;
+        --this->a, ++b, (this)->c *= 2;
 #pragma omp parallel
 #pragma omp for lastprivate(b)
         for (b = 0; b < 2; ++b)
@@ -190,7 +190,7 @@ int main() {
     // LAMBDA: call void (%{{.+}}*, i{{[0-9]+}}, void (i{{[0-9]+}}*, i{{[0-9]+}}*, ...)*, ...) @__kmpc_fork_call(%{{.+}}* @{{.+}}, i{{[0-9]+}} 1, void (i{{[0-9]+}}*, i{{[0-9]+}}*, ...)* bitcast (void (i{{[0-9]+}}*, i{{[0-9]+}}*, [[SS_TY]]*)* [[SS_MICROTASK:@.+]] to void
     // LAMBDA: call void @__kmpc_for_static_init_4(
     // LAMBDA-NOT: getelementptr inbounds [[SS_TY]], [[SS_TY]]* %{{.+}}, i32 0, i32 0
-    // LAMBDA: call void {{.+}} [[SS_LAMBDA:@[^ ]+]]
+    // LAMBDA: call{{.*}} void [[SS_LAMBDA1:@[^ ]+]]
     // LAMBDA: call void @__kmpc_for_static_fini(%
     // LAMBDA: ret
 
@@ -200,7 +200,7 @@ int main() {
     // LAMBDA: getelementptr {{.*}}[[SS_TY]], [[SS_TY]]* %{{.*}}, i32 0, i32 2
     // LAMBDA: call void @__kmpc_for_static_init_4(
     // LAMBDA-NOT: getelementptr {{.*}}[[SS_TY]], [[SS_TY]]*
-    // LAMBDA: call{{.*}} void
+    // LAMBDA: call{{.*}} void [[SS_LAMBDA:@[^ ]+]]
     // LAMBDA: call void @__kmpc_for_static_fini(
     // LAMBDA: br i1
     // LAMBDA: [[B_REF:%.+]] = getelementptr {{.*}}[[SS_TY]], [[SS_TY]]* %{{.*}}, i32 0, i32 1
@@ -236,6 +236,9 @@ int main() {
     // LAMBDA: br label
     // LAMBDA: ret void
 
+    // LAMBDA: define internal void @{{.+}}(i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, [[SS_TY]]* %{{.+}})
+    // LAMBDA: ret void
+
     // LAMBDA: define{{.*}} internal{{.*}} void [[OMP_REGION]](i32* noalias %{{.+}}, i32* noalias %{{.+}}, i32* dereferenceable(4) [[SIVAR:%.+]])
     // LAMBDA: alloca i{{[0-9]+}},
     // LAMBDA: alloca i{{[0-9]+}},
diff --git a/test/OpenMP/for_reduction_codegen.cpp b/test/OpenMP/for_reduction_codegen.cpp
index 6997d8138842..5dcc4b2622b2 100644
--- a/test/OpenMP/for_reduction_codegen.cpp
+++ b/test/OpenMP/for_reduction_codegen.cpp
@@ -492,7 +492,7 @@ int main() {
 // CHECK: store float [[UP]], float* [[T_VAR1_LHS]],
 // CHECK: ret void
 
-// CHECK: define internal void [[MAIN_MICROTASK1]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* nonnull %{{.+}}, [2 x i32]* dereferenceable(8) %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(160) %{{.+}})
+// CHECK: define internal void [[MAIN_MICROTASK1]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* %{{.+}}, [2 x i32]* dereferenceable(8) %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(160) %{{.+}})
 
 // Reduction list for runtime.
 // CHECK: [[RED_LIST:%.+]] = alloca [4 x i8*],
@@ -696,7 +696,7 @@ int main() {
 
 // CHECK: ret void
 
-// CHECK: define internal void [[MAIN_MICROTASK2]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* nonnull %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(160) %{{.+}})
+// CHECK: define internal void [[MAIN_MICROTASK2]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(160) %{{.+}})
 
 // CHECK: [[ARRS_PRIV:%.+]] = alloca [10 x [4 x [[S_FLOAT_TY]]]],
 
diff --git a/test/OpenMP/for_reduction_codegen_UDR.cpp b/test/OpenMP/for_reduction_codegen_UDR.cpp
index a30df368663d..da384294e238 100644
--- a/test/OpenMP/for_reduction_codegen_UDR.cpp
+++ b/test/OpenMP/for_reduction_codegen_UDR.cpp
@@ -301,7 +301,7 @@ int main() {
 // CHECK: fadd float 5.550000e+02, %
 // CHECK: ret void
 
-// CHECK: define internal void [[MAIN_MICROTASK1]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* nonnull %{{.+}}, [2 x i32]* dereferenceable(8) %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(480) %{{.+}})
+// CHECK: define internal void [[MAIN_MICROTASK1]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* %{{.+}}, [2 x i32]* dereferenceable(8) %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(480) %{{.+}})
 
 // Reduction list for runtime.
 // CHECK: [[RED_LIST:%.+]] = alloca [4 x i8*],
@@ -500,7 +500,7 @@ int main() {
 
 // CHECK: ret void
 
-// CHECK: define internal void [[MAIN_MICROTASK2]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* nonnull %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(480) %{{.+}})
+// CHECK: define internal void [[MAIN_MICROTASK2]](i{{[0-9]+}}* noalias [[GTID_ADDR:%.+]], i{{[0-9]+}}* noalias %{{.+}}, i64 %{{.+}}, i64 %{{.+}}, i32* %{{.+}}, [10 x [4 x [[S_FLOAT_TY]]]]* dereferenceable(480) %{{.+}})
 
 // CHECK: [[ARRS_PRIV:%.+]] = alloca [10 x [4 x [[S_FLOAT_TY]]]],
 
diff --git a/test/OpenMP/for_reduction_messages.cpp b/test/OpenMP/for_reduction_messages.cpp
index 45a4681440fc..bb70ecc2b23e 100644
--- a/test/OpenMP/for_reduction_messages.cpp
+++ b/test/OpenMP/for_reduction_messages.cpp
@@ -9,6 +9,13 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp parallel
+#pragma omp for reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/for_simd_reduction_messages.cpp b/test/OpenMP/for_simd_reduction_messages.cpp
index 2935cec602dd..485070e758ae 100644
--- a/test/OpenMP/for_simd_reduction_messages.cpp
+++ b/test/OpenMP/for_simd_reduction_messages.cpp
@@ -9,6 +9,13 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp parallel
+#pragma omp for simd reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/parallel_codegen.cpp b/test/OpenMP/parallel_codegen.cpp
index 04c5c2c85ae2..41a050100bd0 100644
--- a/test/OpenMP/parallel_codegen.cpp
+++ b/test/OpenMP/parallel_codegen.cpp
@@ -11,7 +11,7 @@
 // CHECK-DEBUG-DAG: %ident_t = type { i32, i32, i32, i32, i8* }
 // CHECK-DEBUG-DAG: [[STR:@.+]] = private unnamed_addr constant [23 x i8] c";unknown;unknown;0;0;;\00"
 // CHECK-DEBUG-DAG: [[DEF_LOC_2:@.+]] = private unnamed_addr constant %ident_t { i32 0, i32 2, i32 0, i32 0, i8* getelementptr inbounds ([23 x i8], [23 x i8]* [[STR]], i32 0, i32 0) }
-// CHECK-DEBUG-DAG: [[LOC1:@.+]] = private unnamed_addr constant [{{.+}} x i8] c";{{.*}}parallel_codegen.cpp;main;[[@LINE+14]];9;;\00"
+// CHECK-DEBUG-DAG: [[LOC1:@.+]] = private unnamed_addr constant [{{.+}} x i8] c";{{.*}}parallel_codegen.cpp;main;[[@LINE+15]];9;;\00"
 // CHECK-DEBUG-DAG: [[LOC2:@.+]] = private unnamed_addr constant [{{.+}} x i8] c";{{.*}}parallel_codegen.cpp;tmain;[[@LINE+7]];9;;\00"
 
 template <class T>
@@ -25,17 +25,19 @@ int tmain(T argc) {
 }
 
 int main (int argc, char **argv) {
+  int a[argc];
 #pragma omp parallel
-  foo(argc);
+  foo(a[1]);
   return tmain(argv);
 }
 
 // CHECK-LABEL: define {{[a-z\_\b]*[ ]?i32}} @main({{i32[ ]?[a-z]*}} %argc, i8** %argv)
 // CHECK: store i32 %argc, i32* [[ARGC_ADDR:%.+]],
-// CHECK:  call {{.*}}void (%ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%ident_t* [[DEF_LOC_2]], i32 1, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i32*)* [[OMP_OUTLINED:@.+]] to void (i32*, i32*, ...)*), i32* [[ARGC_ADDR]])
+// CHECK: [[VLA:%.+]] = alloca i32, i{{[0-9]+}} [[VLA_SIZE:%[^,]+]],
+// CHECK:  call {{.*}}void (%ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%ident_t* [[DEF_LOC_2]], i32 2, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i{{[0-9]+}}, i32*)* [[OMP_OUTLINED:@.+]] to void (i32*, i32*, ...)*), i{{[0-9]+}} [[VLA_SIZE]], i32* [[VLA]])
 // CHECK-NEXT:  [[ARGV:%.+]] = load i8**, i8*** {{%[a-z0-9.]+}}
 // CHECK-NEXT:  [[RET:%.+]] = call {{[a-z\_\b]*[ ]?i32}} [[TMAIN:@.+tmain.+]](i8** [[ARGV]])
-// CHECK-NEXT:  ret i32 [[RET]]
+// CHECK:       ret i32
 // CHECK-NEXT:  }
 // CHECK-DEBUG-LABEL: define i32 @main(i32 %argc, i8** %argv)
 // CHECK-DEBUG:       [[LOC_2_ADDR:%.+]] = alloca %ident_t
@@ -43,30 +45,33 @@ int main (int argc, char **argv) {
 // CHECK-DEBUG-NEXT:  [[KMPC_DEFAULT_LOC_VOIDPTR:%.+]] = bitcast %ident_t* [[DEF_LOC_2]] to i8*
 // CHECK-DEBUG-NEXT:   call void @llvm.memcpy.p0i8.p0i8.i64(i8* [[KMPC_LOC_VOIDPTR]], i8* [[KMPC_DEFAULT_LOC_VOIDPTR]], i64 24, i32 8, i1 false)
 // CHECK-DEBUG:       store i32 %argc, i32* [[ARGC_ADDR:%.+]],
+// CHECK-DEBUG:       [[VLA:%.+]] = alloca i32, i64 [[VLA_SIZE:%[^,]+]],
 // CHECK-DEBUG:       [[KMPC_LOC_PSOURCE_REF:%.+]] = getelementptr inbounds %ident_t, %ident_t* [[LOC_2_ADDR]], i32 0, i32 4
 // CHECK-DEBUG-NEXT:  store i8* getelementptr inbounds ([{{.+}} x i8], [{{.+}} x i8]* [[LOC1]], i32 0, i32 0), i8** [[KMPC_LOC_PSOURCE_REF]]
-// CHECK-DEBUG:  call {{.*}}void (%ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%ident_t* [[LOC_2_ADDR]], i32 1, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i32*)* [[OMP_OUTLINED:@.+]] to void (i32*, i32*, ...)*), i32* [[ARGC_ADDR]])
+// CHECK-DEBUG:       call {{.*}}void (%ident_t*, i32, void (i32*, i32*, ...)*, ...) @__kmpc_fork_call(%ident_t* [[LOC_2_ADDR]], i32 2, void (i32*, i32*, ...)* bitcast (void (i32*, i32*, i64, i32*)* [[OMP_OUTLINED:@.+]] to void (i32*, i32*, ...)*), i64 [[VLA_SIZE]], i32* [[VLA]])
 // CHECK-DEBUG-NEXT:  [[ARGV:%.+]] = load i8**, i8*** {{%[a-z0-9.]+}}
 // CHECK-DEBUG-NEXT:  [[RET:%.+]] = call i32 [[TMAIN:@.+tmain.+]](i8** [[ARGV]])
-// CHECK-DEBUG-NEXT:  ret i32 [[RET]]
+// CHECK-DEBUG:       ret i32
 // CHECK-DEBUG-NEXT:  }
 
-// CHECK:       define internal {{.*}}void [[OMP_OUTLINED]](i32* noalias %.global_tid., i32* noalias %.bound_tid., i32* dereferenceable(4) [[ARGC_ADDR:%[^)]+]])
+// CHECK:       define internal {{.*}}void [[OMP_OUTLINED]](i32* noalias %.global_tid., i32* noalias %.bound_tid., i{{[0-9]+}}{{.*}} [[VLA_SIZE:%.+]], i32* [[VLA_ADDR:%[^)]+]])
 // CHECK-SAME:       #[[FN_ATTRS:[0-9]+]]
-// CHECK:       store i32* [[ARGC_ADDR]], i32** [[ARGC_PTR_ADDR:%.+]],
-// CHECK:       [[ARGC_REF:%.+]] = load i32*, i32** [[ARGC_PTR_ADDR]]
-// CHECK-NEXT:  [[ARGC:%.+]] = load i32, i32* [[ARGC_REF]]
-// CHECK-NEXT:  invoke {{.*}}void [[FOO:@.+foo.+]](i32{{[ ]?[a-z]*}} [[ARGC]])
+// CHECK:       store i32* [[VLA_ADDR]], i32** [[VLA_PTR_ADDR:%.+]],
+// CHECK:       [[VLA_REF:%.+]] = load i32*, i32** [[VLA_PTR_ADDR]]
+// CHECK:       [[VLA_ELEM_REF:%.+]] = getelementptr inbounds i32, i32* [[VLA_REF]], i{{[0-9]+}} 1
+// CHECK-NEXT:  [[VLA_ELEM:%.+]] = load i32, i32* [[VLA_ELEM_REF]]
+// CHECK-NEXT:  invoke {{.*}}void [[FOO:@.+foo.+]](i32{{[ ]?[a-z]*}} [[VLA_ELEM]])
 // CHECK:       ret void
 // CHECK:       call {{.*}}void @{{.+terminate.*|abort}}(
 // CHECK-NEXT:  unreachable
 // CHECK-NEXT:  }
-// CHECK-DEBUG:       define internal void [[OMP_OUTLINED]](i32* noalias %.global_tid., i32* noalias %.bound_tid., i32* dereferenceable(4) [[ARGC_ADDR:%[^)]+]])
+// CHECK-DEBUG:       define internal void [[OMP_OUTLINED]](i32* noalias %.global_tid., i32* noalias %.bound_tid., i64 [[VLA_SIZE:%.+]], i32* [[VLA_ADDR:%[^)]+]])
 // CHECK-DEBUG-SAME:  #[[FN_ATTRS:[0-9]+]]
-// CHECK-DEBUG:       store i32* [[ARGC_ADDR]], i32** [[ARGC_PTR_ADDR:%.+]],
-// CHECK-DEBUG:  [[ARGC_REF:%.+]] = load i32*, i32** [[ARGC_PTR_ADDR]]
-// CHECK-DEBUG-NEXT:  [[ARGC:%.+]] = load i32, i32* [[ARGC_REF]]
-// CHECK-DEBUG-NEXT:  invoke void [[FOO:@.+foo.+]](i32 [[ARGC]])
+// CHECK-DEBUG:       store i32* [[VLA_ADDR]], i32** [[VLA_PTR_ADDR:%.+]],
+// CHECK-DEBUG:       [[VLA_REF:%.+]] = load i32*, i32** [[VLA_PTR_ADDR]]
+// CHECK-DEBUG:       [[VLA_ELEM_REF:%.+]] = getelementptr inbounds i32, i32* [[VLA_REF]], i64 1
+// CHECK-DEBUG-NEXT:  [[VLA_ELEM:%.+]] = load i32, i32* [[VLA_ELEM_REF]]
+// CHECK-DEBUG-NEXT:  invoke void [[FOO:@.+foo.+]](i32 [[VLA_ELEM]])
 // CHECK-DEBUG:       ret void
 // CHECK-DEBUG:       call void @{{.+terminate.*|abort}}(
 // CHECK-DEBUG-NEXT:  unreachable
diff --git a/test/OpenMP/parallel_for_reduction_messages.cpp b/test/OpenMP/parallel_for_reduction_messages.cpp
index 4d5a143bae3a..57ab1fac9c11 100644
--- a/test/OpenMP/parallel_for_reduction_messages.cpp
+++ b/test/OpenMP/parallel_for_reduction_messages.cpp
@@ -9,6 +9,12 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp parallel for reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/parallel_for_simd_reduction_messages.cpp b/test/OpenMP/parallel_for_simd_reduction_messages.cpp
index afb0b367c419..60a947dd5f84 100644
--- a/test/OpenMP/parallel_for_simd_reduction_messages.cpp
+++ b/test/OpenMP/parallel_for_simd_reduction_messages.cpp
@@ -9,6 +9,12 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp parallel for simd reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/parallel_reduction_messages.cpp b/test/OpenMP/parallel_reduction_messages.cpp
index af1f5ed7bc0d..947353fc2dd4 100644
--- a/test/OpenMP/parallel_reduction_messages.cpp
+++ b/test/OpenMP/parallel_reduction_messages.cpp
@@ -9,6 +9,11 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp parallel reduction(+:ref)
+  foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/parallel_sections_reduction_messages.cpp b/test/OpenMP/parallel_sections_reduction_messages.cpp
index 52d4cb9cdcb6..05cc7f0f6c5f 100644
--- a/test/OpenMP/parallel_sections_reduction_messages.cpp
+++ b/test/OpenMP/parallel_sections_reduction_messages.cpp
@@ -9,6 +9,13 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp parallel sections reduction(+:ref)
+  {
+    foo();
+  }
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/sections_reduction_messages.cpp b/test/OpenMP/sections_reduction_messages.cpp
index 134bf619c911..f13c5b3f2862 100644
--- a/test/OpenMP/sections_reduction_messages.cpp
+++ b/test/OpenMP/sections_reduction_messages.cpp
@@ -9,6 +9,14 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp parallel   
+#pragma omp sections reduction(+:ref)
+  {
+    foo();
+  }
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/simd_reduction_messages.cpp b/test/OpenMP/simd_reduction_messages.cpp
index c47d53eb9185..1e1a233ec499 100644
--- a/test/OpenMP/simd_reduction_messages.cpp
+++ b/test/OpenMP/simd_reduction_messages.cpp
@@ -9,6 +9,12 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp simd reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/target_firstprivate_codegen.cpp b/test/OpenMP/target_firstprivate_codegen.cpp
index ca459e02a425..a9af2d02f237 100644
--- a/test/OpenMP/target_firstprivate_codegen.cpp
+++ b/test/OpenMP/target_firstprivate_codegen.cpp
@@ -222,7 +222,7 @@ int foo(int n, double *ptr) {
   
   // make sure that firstprivate variables are generated in all cases and that we use those instances for operations inside the
   // target region
-  // TCHECK:  define void @__omp_offloading_{{.+}}(i{{[0-9]+}} [[A2_IN:%.+]], [10 x float]* {{.+}} [[B_IN:%.+]], i{{[0-9]+}} [[BN_SZ:%.+]], float* {{.+}} [[BN_IN:%.+]], [5 x [10 x double]]* {{.+}} [[C_IN:%.+]], i{{[0-9]+}} [[CN_SZ1:%.+]], i{{[0-9]+}} [[CN_SZ2:%.+]], double* {{.+}} [[CN_IN:%.+]], [[TT]]* {{.+}} [[D_IN:%.+]])
+  // TCHECK:  define {{.*}}void @__omp_offloading_{{.+}}(i{{[0-9]+}} [[A2_IN:%.+]], [10 x float]* {{.+}} [[B_IN:%.+]], i{{[0-9]+}} [[BN_SZ:%.+]], float* [[BN_IN:%.+]], [5 x [10 x double]]* {{.+}} [[C_IN:%.+]], i{{[0-9]+}} [[CN_SZ1:%.+]], i{{[0-9]+}} [[CN_SZ2:%.+]], double* [[CN_IN:%.+]], [[TT]]* {{.+}} [[D_IN:%.+]])
   // TCHECK:  [[A2_ADDR:%.+]] = alloca i{{[0-9]+}},
   // TCHECK:  [[B_ADDR:%.+]] = alloca [10 x float]*,
   // TCHECK:  [[VLA_ADDR:%.+]] = alloca i{{[0-9]+}},
diff --git a/test/OpenMP/target_map_codegen.cpp b/test/OpenMP/target_map_codegen.cpp
index 626f68d5a2a9..678e774566c1 100644
--- a/test/OpenMP/target_map_codegen.cpp
+++ b/test/OpenMP/target_map_codegen.cpp
@@ -645,7 +645,7 @@ void implicit_maps_variable_length_array (int a){
   }
 }
 
-// CK13: define internal void [[KERNEL]](i[[sz]] [[VLA0:%.+]], i[[sz]] [[VLA1:%.+]], double* {{.+}}[[ARG:%.+]])
+// CK13: define internal void [[KERNEL]](i[[sz]] [[VLA0:%.+]], i[[sz]] [[VLA1:%.+]], double* {{.*}}[[ARG:%.+]])
 // CK13: [[ADDR0:%.+]] = alloca i[[sz]],
 // CK13: [[ADDR1:%.+]] = alloca i[[sz]],
 // CK13: [[ADDR2:%.+]] = alloca double*,
@@ -4074,7 +4074,9 @@ int explicit_maps_with_inner_lambda(int a){
 // CK25: [[VAL1:%.+]] = load i32*, i32** [[VALADDR]],
 // CK25: [[VALADDR1:%.+]] = getelementptr inbounds [[CA01]], [[CA01]]* [[CA:%[^,]+]], i32 0, i32 0
 // CK25: store i32* [[VAL1]], i32** [[VALADDR1]],
-// CK25: call void {{.*}}[[LAMBDA]]{{.*}}([[CA01]]* [[CA]])
+// CK25: call void {{.*}}[[LAMBDA2:@.+]]{{.*}}([[CA01]]* [[CA]])
+
+// CK25: define {{.+}}[[LAMBDA2]]
 #endif
 ///==========================================================================///
 // RUN: %clang_cc1 -DCK26 -std=c++11 -verify -fopenmp -fopenmp-targets=powerpc64le-ibm-linux-gnu -x c++ -triple powerpc64le-unknown-unknown -emit-llvm %s -o - | FileCheck %s --check-prefix CK26 --check-prefix CK26-64
diff --git a/test/OpenMP/target_map_messages.cpp b/test/OpenMP/target_map_messages.cpp
index 543f47f8216a..f9bb9410c492 100644
--- a/test/OpenMP/target_map_messages.cpp
+++ b/test/OpenMP/target_map_messages.cpp
@@ -284,6 +284,11 @@ void SAclient(int arg) {
     {}
   }
   }
+  #pragma omp target data map(marr[:][:][:])
+  {
+    #pragma omp target data map(marr)
+    {}
+  }
 
   #pragma omp target data map(to: t)
   {
@@ -419,10 +424,10 @@ T tmain(T argc) {
 #pragma omp target data map(j)
 #pragma omp target map(l) map(l[:5]) // expected-error 2 {{variable already marked as mapped in current construct}} expected-note 2 {{used here}}
   foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 4 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
 #pragma omp target data map(k) // expected-error 2 {{pointer cannot be mapped along with a section derived from itself}}
 #pragma omp target data map(j)
-#pragma omp target map(l) // expected-error 2 {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target map(l)
   foo();
 
 #pragma omp target data map(always, tofrom: x)
@@ -488,10 +493,10 @@ int main(int argc, char **argv) {
 #pragma omp target data map(j)
 #pragma omp target map(l) map(l[:5]) // expected-error {{variable already marked as mapped in current construct}} expected-note {{used here}}
   foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note {{used here}}
 #pragma omp target data map(k) // expected-error {{pointer cannot be mapped along with a section derived from itself}}
 #pragma omp target data map(j)
-#pragma omp target map(l) // expected-error {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target map(l)
   foo();
 
 #pragma omp target data map(always, tofrom: x)
diff --git a/test/OpenMP/target_parallel_for_map_messages.cpp b/test/OpenMP/target_parallel_for_map_messages.cpp
index 5223a2cc78ea..6b5d2e7f8204 100644
--- a/test/OpenMP/target_parallel_for_map_messages.cpp
+++ b/test/OpenMP/target_parallel_for_map_messages.cpp
@@ -143,13 +143,13 @@ T tmain(T argc) {
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for map(l) map(l[:5]) // expected-error 2 {{variable already marked as mapped in current construct}} expected-note 2 {{used here}}
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 4 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
 {
 #pragma omp target parallel for map(k) // expected-error 2 {{pointer cannot be mapped along with a section derived from itself}}
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for map(j)
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target parallel for map(l) // expected-error 2 {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target parallel for map(l)
   for (i = 0; i < argc; ++i) foo();
 }
 
@@ -247,13 +247,13 @@ int main(int argc, char **argv) {
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for map(l) map(l[:5]) // expected-error 1 {{variable already marked as mapped in current construct}} expected-note 1 {{used here}}
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note {{used here}}
 {
 #pragma omp target parallel for map(k) // expected-error {{pointer cannot be mapped along with a section derived from itself}}
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for map(j)
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target parallel for map(l) // expected-error {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target parallel for map(l)
   for (i = 0; i < argc; ++i) foo();
 }
 
diff --git a/test/OpenMP/target_parallel_for_reduction_messages.cpp b/test/OpenMP/target_parallel_for_reduction_messages.cpp
index 16697a98733b..234b71aec218 100644
--- a/test/OpenMP/target_parallel_for_reduction_messages.cpp
+++ b/test/OpenMP/target_parallel_for_reduction_messages.cpp
@@ -9,6 +9,12 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp target parallel for reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/target_parallel_for_simd_map_messages.cpp b/test/OpenMP/target_parallel_for_simd_map_messages.cpp
index f44639e86cab..93f0be81e31a 100644
--- a/test/OpenMP/target_parallel_for_simd_map_messages.cpp
+++ b/test/OpenMP/target_parallel_for_simd_map_messages.cpp
@@ -143,13 +143,13 @@ T tmain(T argc) {
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for simd map(l) map(l[:5]) // expected-error 2 {{variable already marked as mapped in current construct}} expected-note 2 {{used here}}
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 4 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
 {
 #pragma omp target parallel for simd map(k) // expected-error 2 {{pointer cannot be mapped along with a section derived from itself}}
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for simd map(j)
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target parallel for simd map(l) // expected-error 2 {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target parallel for simd map(l)
   for (i = 0; i < argc; ++i) foo();
 }
 
@@ -247,13 +247,13 @@ int main(int argc, char **argv) {
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for simd map(l) map(l[:5]) // expected-error 1 {{variable already marked as mapped in current construct}} expected-note 1 {{used here}}
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note {{used here}}
 {
 #pragma omp target parallel for simd map(k) // expected-error {{pointer cannot be mapped along with a section derived from itself}}
   for (i = 0; i < argc; ++i) foo();
 #pragma omp target parallel for simd map(j)
   for (i = 0; i < argc; ++i) foo();
-#pragma omp target parallel for simd map(l) // expected-error {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target parallel for simd map(l)
   for (i = 0; i < argc; ++i) foo();
 }
 
diff --git a/test/OpenMP/target_parallel_for_simd_reduction_messages.cpp b/test/OpenMP/target_parallel_for_simd_reduction_messages.cpp
index 3999d3816284..289b5b2641b6 100644
--- a/test/OpenMP/target_parallel_for_simd_reduction_messages.cpp
+++ b/test/OpenMP/target_parallel_for_simd_reduction_messages.cpp
@@ -9,6 +9,12 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp target parallel for simd reduction(+:ref)
+  for (int i = 0; i < 10; ++i)
+    foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/target_parallel_map_messages.cpp b/test/OpenMP/target_parallel_map_messages.cpp
index ff20567185bf..e16801665428 100644
--- a/test/OpenMP/target_parallel_map_messages.cpp
+++ b/test/OpenMP/target_parallel_map_messages.cpp
@@ -143,13 +143,13 @@ T tmain(T argc) {
   foo();
 #pragma omp target parallel map(l) map(l[:5]) // expected-error 2 {{variable already marked as mapped in current construct}} expected-note 2 {{used here}}
   foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 4 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
 {
 #pragma omp target parallel map(k) // expected-error 2 {{pointer cannot be mapped along with a section derived from itself}}
   foo();
 #pragma omp target parallel map(j)
   foo();
-#pragma omp target parallel map(l) // expected-error 2 {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target parallel map(l)
   foo();
 }
 
@@ -246,13 +246,13 @@ int main(int argc, char **argv) {
   foo();
 #pragma omp target parallel map(l) map(l[:5]) // expected-error 1 {{variable already marked as mapped in current construct}} expected-note 1 {{used here}}
   foo();
-#pragma omp target data map(k[:4], j, l[:5]) // expected-note 2 {{used here}}
+#pragma omp target data map(k[:4], j, l[:5]) // expected-note 1 {{used here}}
 {
 #pragma omp target parallel map(k) // expected-error {{pointer cannot be mapped along with a section derived from itself}}
   foo();
 #pragma omp target parallel map(j)
   foo();
-#pragma omp target parallel map(l) // expected-error {{original storage of expression in data environment is shared but data environment do not fully contain mapped expression storage}}
+#pragma omp target parallel map(l)
   foo();
 }
 
diff --git a/test/OpenMP/target_parallel_reduction_messages.cpp b/test/OpenMP/target_parallel_reduction_messages.cpp
index c9434e76245b..52338ee71cbe 100644
--- a/test/OpenMP/target_parallel_reduction_messages.cpp
+++ b/test/OpenMP/target_parallel_reduction_messages.cpp
@@ -9,6 +9,11 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp target parallel reduction(+:ref)
+  foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/OpenMP/teams_reduction_messages.cpp b/test/OpenMP/teams_reduction_messages.cpp
index 0420b010bb65..9974c147b72c 100644
--- a/test/OpenMP/teams_reduction_messages.cpp
+++ b/test/OpenMP/teams_reduction_messages.cpp
@@ -9,6 +9,12 @@ bool foobool(int argc) {
   return argc;
 }
 
+void foobar(int &ref) {
+#pragma omp target
+#pragma omp teams reduction(+:ref)
+  foo();
+}
+
 struct S1; // expected-note {{declared here}} expected-note 4 {{forward declaration of 'S1'}}
 extern S1 a;
 class S2 {
diff --git a/test/Preprocessor/init.c b/test/Preprocessor/init.c
index f7c320b7226b..adf5c778767e 100644
--- a/test/Preprocessor/init.c
+++ b/test/Preprocessor/init.c
@@ -1975,6 +1975,11 @@
 // ARMEABIHARDFP:#define __arm 1
 // ARMEABIHARDFP:#define __arm__ 1
 
+// RUN: %clang_cc1 -E -dM -ffreestanding -triple=armv6-unknown-cloudabi-eabihf < /dev/null | FileCheck -match-full-lines -check-prefix ARMV6-CLOUDABI %s
+//
+// ARMV6-CLOUDABI:#define __CloudABI__ 1
+// ARMV6-CLOUDABI:#define __arm__ 1
+
 // RUN: %clang_cc1 -E -dM -ffreestanding -triple=arm-netbsd-eabi < /dev/null | FileCheck -match-full-lines -check-prefix ARM-NETBSD %s
 //
 // ARM-NETBSD-NOT:#define _LP64
diff --git a/test/SemaCXX/cxx0x-defaulted-functions.cpp b/test/SemaCXX/cxx0x-defaulted-functions.cpp
index 16e20ff4964d..7ec9726095cb 100644
--- a/test/SemaCXX/cxx0x-defaulted-functions.cpp
+++ b/test/SemaCXX/cxx0x-defaulted-functions.cpp
@@ -208,3 +208,38 @@ int fn() {
   t = true;
 }
 }
+
+namespace dependent_classes {
+template <bool B, typename X, typename Y>
+struct conditional;
+
+template <typename X, typename Y>
+struct conditional<true, X, Y> { typedef X type; };
+
+template <typename X, typename Y>
+struct conditional<false, X, Y> { typedef Y type; };
+
+template<bool B> struct X {
+  X();
+
+  // B == false triggers error for = default.
+  using T = typename conditional<B, const X &, int>::type;
+  X(T) = default;  // expected-error {{only special member functions}}
+
+  // Either value of B creates a constructor that can be default
+  using U = typename conditional<B, X&&, const X&>::type;
+  X(U) = default;
+};
+
+X<true> x1;
+X<false> x2; // expected-note {{in instantiation}}
+
+template <typename Type>
+class E {
+  explicit E(const int &) = default;
+};
+
+template <typename Type>
+E<Type>::E(const int&) {}  // expected-error {{definition of explicitly defaulted function}}
+
+}
diff --git a/test/SemaCXX/cxx11-crashes.cpp b/test/SemaCXX/cxx11-crashes.cpp
index 97c959454c35..7c455eecd5fa 100644
--- a/test/SemaCXX/cxx11-crashes.cpp
+++ b/test/SemaCXX/cxx11-crashes.cpp
@@ -91,3 +91,15 @@ void test(int some_number) {  // expected-note {{'some_number' declared here}}
   Foo(lambda);
 }
 }
+
+namespace pr29091 {
+  struct X{ X(const X &x); };
+  struct Y: X { using X::X; };
+  bool foo() { return __has_nothrow_constructor(Y); }
+  bool bar() { return __has_nothrow_copy(Y); }
+
+  struct A { template <typename T> A(); };
+  struct B : A { using A::A; };
+  bool baz() { return __has_nothrow_constructor(B); }
+  bool qux() { return __has_nothrow_copy(B); }
+}
diff --git a/test/SemaCXX/nested-name-spec.cpp b/test/SemaCXX/nested-name-spec.cpp
index 0fbdedc70a69..f445725c7429 100644
--- a/test/SemaCXX/nested-name-spec.cpp
+++ b/test/SemaCXX/nested-name-spec.cpp
@@ -435,3 +435,21 @@ namespace PR16951 {
                              // expected-error{{no member named 'X2' in 'PR16951::enumerator_2'}}
 
 }
+
+namespace PR30619 {
+c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d;
+// expected-error@-1 16{{unknown type name 'c'}}
+c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d;
+// expected-error@-1 16{{unknown type name 'c'}}
+c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d;
+// expected-error@-1 16{{unknown type name 'c'}}
+c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d; c d;
+// expected-error@-1 16{{unknown type name 'c'}}
+namespace A {
+class B {
+  typedef C D; // expected-error{{unknown type name 'C'}}
+  A::D::F;
+  // expected-error@-1{{'D' (aka 'int') is not a class, namespace, or enumeration}}
+};
+}
+}
diff --git a/test/SemaTemplate/instantiate-self.cpp b/test/SemaTemplate/instantiate-self.cpp
index cfe902509f7f..916a01e63f11 100644
--- a/test/SemaTemplate/instantiate-self.cpp
+++ b/test/SemaTemplate/instantiate-self.cpp
@@ -1,4 +1,4 @@
-// RUN: %clang_cc1 -std=c++11 -verify %s
+// RUN: %clang_cc1 -std=c++1z -verify -pedantic-errors %s
 
 // Check that we deal with cases where the instantiation of a class template
 // recursively requires the instantiation of the same template.
@@ -47,9 +47,8 @@ namespace test4 {
   A<int> a; // expected-note {{in instantiation of}}
 }
 
-// FIXME: PR12298: Recursive constexpr function template instantiation leads to
+// PR12298: Recursive constexpr function template instantiation leads to
 // stack overflow.
-#if 0
 namespace test5 {
   template<typename T> struct A {
     constexpr T f(T k) { return g(k); }
@@ -57,22 +56,20 @@ namespace test5 {
       return k ? f(k-1)+1 : 0;
     }
   };
-  // This should be accepted.
-  constexpr int x = A<int>().f(5);
+  constexpr int x = A<int>().f(5); // ok
 }
 
 namespace test6 {
   template<typename T> constexpr T f(T);
   template<typename T> constexpr T g(T t) {
-    typedef int arr[f(T())];
+    typedef int arr[f(T())]; // expected-error {{variable length array}}
     return t;
   }
   template<typename T> constexpr T f(T t) {
-    typedef int arr[g(T())];
+    typedef int arr[g(T())]; // expected-error {{zero size array}} expected-note {{instantiation of}}
     return t;
   }
-  // This should be ill-formed.
-  int n = f(0);
+  int n = f(0); // expected-note 2{{instantiation of}}
 }
 
 namespace test7 {
@@ -80,10 +77,94 @@ namespace test7 {
     return t;
   }
   template<typename T> constexpr T f(T t) {
-    typedef int arr[g(T())];
+    typedef int arr[g(T() + 1)];
     return t;
   }
-  // This should be accepted.
   int n = f(0);
 }
+
+namespace test8 {
+  template<typename T> struct A {
+    int n = A{}.n; // expected-error {{default member initializer for 'n' uses itself}} expected-note {{instantiation of default member init}}
+  };
+  A<int> ai = {}; // expected-note {{instantiation of default member init}}
+}
+
+namespace test9 {
+  template<typename T> struct A { enum class B; };
+  // FIXME: It'd be nice to give the "it has not yet been instantiated" diagnostic here.
+  template<typename T> enum class A<T>::B { k = A<T>::B::k2, k2 = k }; // expected-error {{no member named 'k2'}}
+  auto k = A<int>::B::k; // expected-note {{in instantiation of}}
+}
+
+namespace test10 {
+  template<typename T> struct A {
+    void f() noexcept(noexcept(f())); // expected-error {{exception specification of 'f' uses itself}} expected-note {{instantiation of}}
+  };
+  bool b = noexcept(A<int>().f()); // expected-note {{instantiation of}}
+}
+
+namespace test11 {
+  template<typename T> const int var = var<T>;
+  int k = var<int>;
+
+  template<typename T> struct X {
+    static const int k = X<T>::k;
+  };
+  template<typename T> const int X<T>::k;
+  int q = X<int>::k;
+
+  template<typename T> struct Y {
+    static const int k;
+  };
+  template<typename T> const int Y<T>::k = Y<T>::k;
+  int r = Y<int>::k;
+}
+
+namespace test12 {
+  template<typename T> int f(T t, int = f(T())) {} // expected-error {{recursive evaluation of default argument}} expected-note {{instantiation of}}
+  struct X {};
+  int q = f(X()); // expected-note {{instantiation of}}
+}
+
+namespace test13 {
+  struct A {
+    // Cycle via type of non-type template parameter.
+    template<typename T, typename T::template W<T>::type U = 0> struct W { using type = int; };
+    // Cycle via default template argument.
+    template<typename T, typename U = typename T::template X<T>> struct X {};
+    template<typename T, int U = T::template Y<T>::value> struct Y { static const int value = 0; };
+    template<typename T, template<typename> typename U = T::template Z<T>::template nested> struct Z { template<typename> struct nested; };
+  };
+  template<typename T> struct Wrap {
+    template<typename U> struct W : A::W<T> {};
+    template<typename U> struct X : A::X<T> {};
+    template<typename U> struct Y : A::Y<T> {};
+    template<typename U> struct Z : A::Z<T> {};
+  };
+  struct B {
+    template<typename U> struct W { using type = int; };
+    template<typename U> struct X {};
+    template<typename U> struct Y { static const int value = 0; };
+    template<typename U> struct Z { template<typename> struct nested; };
+  };
+
+  A::W<B> awb;
+  A::X<B> axb;
+  A::Y<B> ayb;
+  A::Z<B> azb;
+
+  A::W<Wrap<Wrap<B>>> awwwb;
+  A::X<Wrap<Wrap<B>>> axwwb;
+  A::Y<Wrap<Wrap<B>>> aywwb;
+  A::Z<Wrap<Wrap<B>>> azwwb;
+
+  // FIXME: These tests cause us to use too much stack and crash on a self-hosted debug build.
+  // FIXME: Check for recursion here and give a better diagnostic.
+#if 0
+  A::W<A> awa;
+  A::X<A> axa;
+  A::Y<A> aya;
+  A::Z<A> aza;
 #endif
+}
diff --git a/test/SemaTemplate/instantiation-depth-exception-spec.cpp b/test/SemaTemplate/instantiation-depth-exception-spec.cpp
index 6caa4a60e6f8..3f64811bd108 100644
--- a/test/SemaTemplate/instantiation-depth-exception-spec.cpp
+++ b/test/SemaTemplate/instantiation-depth-exception-spec.cpp
@@ -1,11 +1,14 @@
 // RUN: %clang_cc1 -fsyntax-only -verify -std=c++11 -ftemplate-depth 16 -fcxx-exceptions -fexceptions %s
 
-template<typename T> T go(T a) noexcept(noexcept(go(a))); // \
-// expected-error 16{{call to function 'go' that is neither visible}} \
-// expected-note 16{{'go' should be declared prior to the call site}} \
-// expected-error {{recursive template instantiation exceeded maximum depth of 16}}
+template<int N> struct X {
+  static int go(int a) noexcept(noexcept(X<N+1>::go(a))); // \
+// expected-error {{recursive template instantiation exceeded maximum depth of 16}} \
+// expected-note 9{{in instantiation of exception specification}} \
+// expected-note {{skipping 7 context}} \
+// expected-note {{use -ftemplate-depth}}
+};
 
 void f() {
-  int k = go(0); // \
-  // expected-note {{in instantiation of exception specification for 'go<int>' requested here}}
+  int k = X<0>::go(0); // \
+  // expected-note {{in instantiation of exception specification for 'go' requested here}}
 }
diff --git a/test/SemaTemplate/instantiation-depth.cpp b/test/SemaTemplate/instantiation-depth.cpp
index c0b8bb2a1245..17f84c170c32 100644
--- a/test/SemaTemplate/instantiation-depth.cpp
+++ b/test/SemaTemplate/instantiation-depth.cpp
@@ -19,13 +19,12 @@ void test() {
 // RUN: %clang_cc1 -fsyntax-only -verify -ftemplate-depth 5 -ftemplate-backtrace-limit 4 -std=c++11 -DNOEXCEPT %s
 
 template<typename T> struct S {
-  S() noexcept(noexcept(T()));
-};
-struct T : S<T> {}; \
+  S() noexcept(noexcept(S<S>())); \
 // expected-error{{recursive template instantiation exceeded maximum depth of 5}} \
-// expected-note 4 {{in instantiation of exception spec}} \
+// expected-note 3 {{in instantiation of exception spec}} \
 // expected-note {{skipping 2 contexts in backtrace}} \
 // expected-note {{use -ftemplate-depth=N to increase recursive template instantiation depth}}
-T t; // expected-note {{implicit default constructor for 'T' first required here}}
+};
+S<void> t; // expected-note {{in instantiation of exception spec}}
 
 #endif
diff --git a/tools/libclang/CIndex.cpp b/tools/libclang/CIndex.cpp
index 027bf95b660b..deb4cc551b8a 100644
--- a/tools/libclang/CIndex.cpp
+++ b/tools/libclang/CIndex.cpp
@@ -1243,8 +1243,9 @@ bool CursorVisitor::VisitUnresolvedUsingTypenameDecl(
 bool CursorVisitor::VisitStaticAssertDecl(StaticAssertDecl *D) {
   if (Visit(MakeCXCursor(D->getAssertExpr(), StmtParent, TU, RegionOfInterest)))
     return true;
-  if (Visit(MakeCXCursor(D->getMessage(), StmtParent, TU, RegionOfInterest)))
-    return true;
+  if (StringLiteral *Message = D->getMessage())
+    if (Visit(MakeCXCursor(Message, StmtParent, TU, RegionOfInterest)))
+      return true;
   return false;
 }
 
-- 
2.7.4

